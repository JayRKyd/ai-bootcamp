{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad1e673c-0408-4987-a437-5e9d392c4159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d1e777-8fde-4e4b-836b-89744cbe3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b2ae3fa-1108-432c-9438-1fe4ed40ad0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0611ae65-4a7b-46bf-baf4-f7b45ecf6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ab7798-2664-4eff-bb97-e52035952ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import Iterable, Callable\n",
    "import zipfile\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RawRepositoryFile:\n",
    "    filename: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class GithubRepositoryDataReader:\n",
    "    \"\"\"\n",
    "    Downloads and parses markdown and code files from a GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                repo_owner: str,\n",
    "                repo_name: str,\n",
    "                allowed_extensions: Iterable[str] | None = None,\n",
    "                filename_filter: Callable[[str], bool] | None = None\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the GitHub repository data reader.\n",
    "        \n",
    "        Args:\n",
    "            repo_owner: The owner/organization of the GitHub repository\n",
    "            repo_name: The name of the GitHub repository\n",
    "            allowed_extensions: Optional set of file extensions to include\n",
    "                    (e.g., {\"md\", \"py\"}). If not provided, all file types are included\n",
    "            filename_filter: Optional callable to filter files by their path\n",
    "        \"\"\"\n",
    "        prefix = \"https://codeload.github.com\"\n",
    "        self.url = (\n",
    "            f\"{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main\"\n",
    "        )\n",
    "\n",
    "        if allowed_extensions is not None:\n",
    "            self.allowed_extensions = {ext.lower() for ext in allowed_extensions}\n",
    "\n",
    "        if filename_filter is None:\n",
    "            self.filename_filter = lambda filepath: True\n",
    "        else:\n",
    "            self.filename_filter = filename_filter\n",
    "\n",
    "    def read(self) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Download and extract files from the GitHub repository.\n",
    "        \n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If the repository download fails\n",
    "        \"\"\"\n",
    "        resp = requests.get(self.url)\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "        zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "        repository_data = self._extract_files(zf)\n",
    "        zf.close()\n",
    "\n",
    "        return repository_data\n",
    "\n",
    "    def _extract_files(self, zf: zipfile.ZipFile) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Extract and process files from the zip archive.\n",
    "        \n",
    "        Args:\n",
    "            zf: ZipFile object containing the repository data\n",
    "\n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "        \"\"\"\n",
    "        data = []\n",
    "\n",
    "        for file_info in zf.infolist():\n",
    "            filepath = self._normalize_filepath(file_info.filename)\n",
    "\n",
    "            if self._should_skip_file(filepath):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                    if content is not None:\n",
    "                        content = content.strip()\n",
    "\n",
    "                    file = RawRepositoryFile(\n",
    "                        filename=filepath,\n",
    "                        content=content\n",
    "                    )\n",
    "                    data.append(file)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_info.filename}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _should_skip_file(self, filepath: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determine whether a file should be skipped during processing.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to check\n",
    "            \n",
    "        Returns:\n",
    "            True if the file should be skipped, False otherwise\n",
    "        \"\"\"\n",
    "        filepath = filepath.lower()\n",
    "\n",
    "        # directory\n",
    "        if filepath.endswith(\"/\"):\n",
    "            return True\n",
    "\n",
    "        # hidden file\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        if filename.startswith(\".\"):\n",
    "            return True\n",
    "\n",
    "        if self.allowed_extensions:\n",
    "            ext = self._get_extension(filepath)\n",
    "            if ext not in self.allowed_extensions:\n",
    "                return True\n",
    "\n",
    "        if not self.filename_filter(filepath):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _get_extension(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the file extension from a filepath.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to extract extension from\n",
    "            \n",
    "        Returns:\n",
    "            The file extension (without dot) or empty string if no extension\n",
    "        \"\"\"\n",
    "        filename = filepath.lower().split(\"/\")[-1]\n",
    "        if \".\" in filename:\n",
    "            return filename.rsplit(\".\", maxsplit=1)[-1]\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _normalize_filepath(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes the top-level directory from the file path inside the zip archive.\n",
    "        'repo-main/path/to/file.py' -> 'path/to/file.py'\n",
    "        \n",
    "        Args:\n",
    "            filepath: The original filepath from the zip archive\n",
    "            \n",
    "        Returns:\n",
    "            The normalized filepath with top-level directory removed\n",
    "        \"\"\"\n",
    "        parts = filepath.split(\"/\", maxsplit=1)\n",
    "        if len(parts) > 1:\n",
    "            return parts[1]\n",
    "        else:\n",
    "            return parts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba54a340-1eb2-4abd-8aca-0e867d2b9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_github_data():\n",
    "\n",
    "    repo_owner = 'datatalksclub'\n",
    "    repo_name = 'datatalksclub.github.io'\n",
    "    \n",
    "\n",
    "    allowed_extensions = {\"md\", \"mdx\"}\n",
    "\n",
    "    def podcast_filter(filename: str) -> bool:\n",
    "        return '_podcast' in filename\n",
    "    reader = GithubRepositoryDataReader(\n",
    "        repo_owner,\n",
    "        repo_name,\n",
    "        allowed_extensions=allowed_extensions,\n",
    "        filename_filter=podcast_filter\n",
    "    )\n",
    "    \n",
    "    return reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b253b83-76b0-4852-b7f7-116aa8d6c972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m153 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m133 packages\u001b[0m \u001b[2min 91ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add python-frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b54df0c-b00a-4ac4-acf5-e5813969291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_data = read_github_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "635a975d-f204-4221-a745-c31c7a0979c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RawRepositoryFile(filename='_podcast/s05e02-data-engineering-acronyms.md', content='---\\ntitle: \"Making Sense of Data Engineering Acronyms and Buzzwords\"\\nshort: \"Making Sense of Data Engineering Acronyms and Buzzwords\"\\nguests: [nataliekwong]\\n\\nimage: images/podcast/s05e02-data-engineering-acronyms.jpg\\n\\nseason: 5\\nepisode: 2\\n\\nids:\\n  youtube: t9Z1S3OYnJU\\n  anchor: Making-Sense-of-Data-Engineering-Acronyms-and-Buzzwords---Natalie-Kwong-e177303\\n\\nlinks:\\n  youtube: https://www.youtube.com/watch?v=t9Z1S3OYnJU\\n  anchor: https://anchor.fm/datatalksclub/episodes/Making-Sense-of-Data-Engineering-Acronyms-and-Buzzwords---Natalie-Kwong-e177303\\n  spotify: https://open.spotify.com/episode/1AvtwdcAXGGjdJ7fl0Hsuw\\n  apple: https://podcasts.apple.com/us/podcast/making-sense-of-data-engineering-acronyms-and/id1541710331?i=1000534990760\\n\\ntranscript:\\n- line: This week we\\'ll try to make sense of common engineering acronyms and buzzwords\\n    with the help of our special guest today, Natalie. Natalie works at Airbyte, focusing\\n    on building user experience and overseeing analytics. Your expertise includes\\n    scaling analytics teams and systems from the ground up. Welcome, Natalie.\\n  sec: 94\\n  time: \\'1:34\\'\\n  who: Alexey\\n- line: Thank you. Happy to be here.\\n  sec: 116\\n  time: \\'1:56\\'\\n  who: Natalie\\n- header: \"Natalie\\\\u2019s background\"\\n- line: Before we go into our main topic of understanding these acronyms and buzzwords,\\n    let\\'s start with your background. Can you tell us about your career journey so\\n    far?\\n  sec: 118\\n  time: \\'1:58\\'\\n  who: Alexey\\n- line: \"Yeah, sure. I\\'ve been in startup tech for my entire career. I actually started\\\\\\n    \\\\ out in the Bay Area at Vox, doing marketing operations. Then I moved into marketing\\\\\\n    \\\\ analytics at a company called Admiral. I really went deeper into analytics there,\\\\\\n    \\\\ doing R, SQL, a little bit of Python, and really ended up becoming an acquisition\\\\\\n    \\\\ analyst. This involves looking at both marketing and sales and how they interact\\\\\\n    \\\\ \\\\u2013 so that would be building out multi-touch attribution models and things\\\\\\n    \\\\ like that.\"\\n  sec: 128\\n  time: \\'2:08\\'\\n  who: Natalie\\n- line: \"After that, I moved a little bit more into operations at AppDynamics, which\\\\\\n    \\\\ has been acquired by Cisco, and then moved to actually manage my own team at\\\\\\n    \\\\ a company called Keep Truckin\\\\u2019, which is focused on more on the IoT space,\\\\\\n    \\\\ filling out dashcams and ELDs for the trucking industry. There, I built out\\\\\\n    \\\\ a team of about 11 analysts, already from marketing and sales to customer success\\\\\\n    \\\\ and product. Then I moved on to Harness doing a customer sales ops role. So\\\\\\n    \\\\ I really kind of straddled that analytics and operation space. Now I\\'m in Airbyte,\\\\\\n    \\\\ doing growth and analytics.\"\\n  who: Natalie\\n- header: Airbyte\\n- line: What does Airbyte do?\\n  sec: 199\\n  time: \\'3:19\\'\\n  who: Alexey\\n- line: \"Airbyte is an extract/load or \\\\u201CELT\\\\u201D platform \\\\u2013 with Transform\\\\\\n    \\\\ being the T \\\\u2013 that essentially allows you to ingest a lot of different\\\\\\n    \\\\ data from different sources, maybe APIs like AdWords or Facebook ads, weave\\\\\\n    \\\\ it in data warehouses like Snowflake, and bring them into your data warehouse.\"\\n  sec: 202\\n  time: \\'3:22\\'\\n  who: Natalie\\n- header: What is ETL?\\n- line: \"You mentioned a few things \\\\u2013 transform, ingest, and ELT. We wanted to\\\\\\n    \\\\ talk about this today. Actually, this is a question I get sometimes. Not super\\\\\\n    \\\\ often, but it pops up: \\\\u201CWhat\\'s the difference between ELT/ETL, all these\\\\\\n    \\\\ acronyms \\\\u2013 what do they actually mean?\\\\u201D That\\'s why we\\\\u2019re having\\\\\\n    \\\\ a conversation today \\\\u2013 to finally figure that out and help everyone else\\\\\\n    \\\\ with that. Let\\'s start with ETL, which is probably the oldest concept from data\\\\\\n    \\\\ engineering. I think it was used even before the term \\\\u2018data engineering\\\\u2019\\\\\\n    \\\\ even existed. I think it\\'s pretty old, coming from business intelligence times\\\\\\n    \\\\ or even older. I don\\'t know. So what is ETL?\"\\n  sec: 226\\n  time: \\'3:46\\'\\n  who: Alexey\\n- line: \"ETL stands for \\\\u2013 E is the extract, T is the transform, and L is the\\\\\\n    \\\\ load. When we think about ETL, we\\'re really thinking about extracting the source-specific\\\\\\n    \\\\ routines where you pull selected data out of an external system. The transform\\\\\\n    \\\\ layer is kind of your specific business logic. Your organization is going to\\\\\\n    \\\\ have some sort of logic that really defines how you pull the data or certain\\\\\\n    \\\\ use cases that you have that are operational. Then the loading piece is where\\\\\\n    \\\\ you have destination-specific routines to push data into the place where it\\'s\\\\\\n    \\\\ going to be consumed. So that\\'s kind of the traditional way to think about it.\"\\n  sec: 270\\n  time: \\'4:30\\'\\n  who: Natalie\\n- line: \"Can you think of an example? Let\\'s say there are some sources, right? We\\\\\\n    \\\\ extract data from these sources, transform the data somehow, and then put it\\\\\\n    \\\\ in a data warehouse. Can you think of an example for this \\\\u2013 you mentioned\\\\\\n    \\\\ something like Facebook ads or something like this?\"\\n  sec: 313\\n  time: \\'5:13\\'\\n  who: Alexey\\n- line: \"Generally, you might see \\\\u2013 if you\\'re working in the marketing space,\\\\\\n    \\\\ for example \\\\u2013 your data is stored in Google AdWords, because you\\'re running\\\\\\n    \\\\ data or you\\'re running ads on Google. Or maybe the same thing but with Facebook.\\\\\\n    \\\\ If you\\'re working in sales, your data might be stored in Salesforce, your CRM.\\\\\\n    \\\\ If you\\'re working in finance, it might be stored in NetSuite, maybe. So all\\\\\\n    \\\\ of these different kinds of API sources all house some data that your business\\\\\\n    \\\\ needs to build some picture of how the business is doing. Those sources would\\\\\\n    \\\\ be the places that we would extract from.\"\\n  sec: 332\\n  time: \\'5:32\\'\\n  who: Natalie\\n- line: The example is that your background is more in marketing, as I understood.\\n    You would want to extract some things from Google AdWords and from Facebook, right?\\n    There is something interesting in this data that you want, and then you do some\\n    transformation on it. You go to Google AdWords, it returns you some data, and\\n    then you want to transform this data. Is that right?\\n  sec: 369\\n  time: \\'6:09\\'\\n  who: Alexey\\n- line: \"Yeah, exactly. One really good use case that we could speak to here \\\\u2013\\\\\\n    \\\\ just to be a little bit more concrete \\\\u2013\\\\u201CWhat is your cost to acquire\\\\\\n    \\\\ a customer?\\\\u201D You need an accurate CAC, in other words. In order to get\\\\\\n    \\\\ that, you need to know how many customers that you\\'ve specifically acquired\\\\\\n    \\\\ from, let\\'s say, Google AdWords. You also need to know how much is being spent\\\\\\n    \\\\ to acquire those customers. The only way to really concretely bridge those things\\\\\\n    \\\\ is to pull out data from your CRM, which stores all of your revenue information\\\\\\n    \\\\ and where the customers came from. Then you also pull up the spend data from\\\\\\n    \\\\ a more upper-funnel source. Then you merge those together using the transform\\\\\\n    \\\\ capability.\"\\n  sec: 397\\n  time: \\'6:37\\'\\n  who: Natalie\\n- line: And then everything eventually goes to the data warehouse, which you use for\\n    building reports. Then you see how much money was spent? Is that right?\\n  sec: 438\\n  time: \\'7:18\\'\\n  who: Alexey\\n- line: \"Exactly. Yep. The way that you finish out the process is \\\\u2013 once it\\'s\\\\\\n    \\\\ loaded in the data warehouse, in this traditional ETL model, you\\'d essentially\\\\\\n    \\\\ have a data mart that specifically says, \\\\u201CHey, this is the data mart that\\\\\\n    \\\\ answers that question.\\\\u201D Then you would have a visualization tool like Looker\\\\\\n    \\\\ or Superset in order to show that from a visualization perspective \\\\u2013 bring\\\\\\n    \\\\ it out to the business so that they can actually consume the insights.\"\\n  sec: 447\\n  time: \\'7:27\\'\\n  who: Natalie\\n- header: Why ELT instead of ETL?\\n- line: What is ELT, then? Why do we want to switch to this tool?\\n  sec: 477\\n  time: \\'7:57\\'\\n  who: Alexey\\n- line: \"I think the traditional way to think about this is \\\\u2013 ETL is just a little\\\\\\n    \\\\ bit more inflexible. The business logic changes a lot of the time and you\\'re\\\\\\n    \\\\ going to receive friction whenever you need to change part of this pipeline.\\\\\\n    \\\\ So because you\\'re transforming it before you load it into a data warehouse,\\\\\\n    \\\\ it\\'s difficult to actually bring in new data. Let\\'s say, there\\'s a new table\\\\\\n    \\\\ or a new field that gets added to Salesforce, the new data that you\\'re collecting\\\\\\n    \\\\ or the new data source, it\\'s fairly inflexible to just go ahead and add those\\\\\\n    \\\\ things. It often will force data to be completely re-extracted, which takes\\\\\\n    \\\\ much more computation and much more time than is really necessary for small\\\\\\n    \\\\ changes like this.\"\\n  sec: 483\\n  time: \\'8:03\\'\\n  who: Natalie\\n- line: \"You also have this lack of autonomy. What we\\'ve generally seen is that these\\\\\\n    \\\\ ELT tools are actually managed by engineering teams. When analysts \\\\u2013 who\\\\\\n    \\\\ are working more with the business end \\\\u2013 have these needs, they actually\\\\\\n    \\\\ have a dependency on an external team to go and compute those. This, of course,\\\\\\n    \\\\ creates more cycles and takes more time to make these changes. Really the crux\\\\\\n    \\\\ of it is \\\\u2013 it requires engineering to be actually involved.\"\\n  who: Natalie\\n- line: So ELT is really generalizing to the ETL. Instead of having the transform\\n    be in the middle, in ELT, the T is at the end. Thus, instead of having a tool\\n    to actually manage the transformation for you, you\\'re actually bifurcating the\\n    E-L and the T. Everything is loaded into your data warehouse and then the transformation\\n    happens after the data is loaded. The transformation actually happens within the\\n    data warehouse itself, the destination.\\n  who: Natalie\\n- header: Transformations\\n- line: \"Yeah, thanks. We already have a comment about transformations. The question\\\\\\n    \\\\ is, \\\\u201CWhen you say transform, can you elaborate so that we can understand\\\\\\n    \\\\ what\\\\u2019s happening here?\\\\u201D Like, \\\\u201CWhat kind of transformations do\\\\\\n    \\\\ we run?\\\\u201D\"\\n  sec: 600\\n  time: \\'10:00\\'\\n  who: Alexey\\n- line: \"Yeah, we definitely can be more specific about that. It can go from the very\\\\\\n    \\\\ basic \\\\u2013 the simplest transformation I can think of is something like changing\\\\\\n    \\\\ a column type from a numeric to a character one. That\\'s a very basic transformation.\\\\\\n    \\\\ It\\'s almost like it\\'s a casting of a column to a different data type. The more\\\\\\n    \\\\ complex transformations generally will join across different data sources. So\\\\\\n    \\\\ you\\'ll say \\\\u201CI want to grab AdWords data and Salesforce data, join them\\\\\\n    \\\\ using some kind of unique identifier, and then figure out how to show these\\\\\\n    \\\\ data sources alongside each other in some sort of finalized data model.\\\\u201D\\\\\\n    \\\\ Generally, we think of these as a kind of transformations that you\\'re running\\\\\\n    \\\\ in SQL. These can be very simple SQL statements or pretty complex ones. But\\\\\\n    \\\\ those are the two ways that I see transformation being done.\"\\n  sec: 622\\n  time: \\'10:22\\'\\n  who: Natalie\\n- line: \"When you swap the T and L, so that the T comes at the end, you said the reason\\\\\\n    \\\\ for this is \\\\u2013 when T is in the middle (ETL) it\\'s not flexible because the\\\\\\n    \\\\ business logic can change. Then you also depend on engineering teams. I also\\\\\\n    \\\\ imagine that, let\\'s say the data we extract from the source, we don\\'t need the\\\\\\n    \\\\ entire response from the ads \\\\u2013 if we\\\\u2019re talking about marketing. So\\\\\\n    \\\\ this service gives us some response. Let\\'s say we keep only one part of this\\\\\\n    \\\\ response, if we\\'re interested in how much money we spent, for example.\"\\n  sec: 679\\n  time: \\'11:19\\'\\n  who: Alexey\\n- line: \"So we keep only this data, we transform it, and we load it to our data warehouse,\\\\\\n    \\\\ only the specific part. Later somebody comes and says, \\\\u201CHey, what about\\\\\\n    \\\\ some other thing from Google AdWords?\\\\u201D and you reply \\\\u201COkay, sorry,\\\\\\n    \\\\ because our T was only keeping this part and we don\\'t have the rest of the data.\\\\u201D\\\\\\n    \\\\ Thus, by keeping the entire thing and then doing the transformation later, if\\\\\\n    \\\\ somebody comes to us and asks for something extra, then the data is there. We\\\\\\n    \\\\ just write another transformation on top of the data that we already extracted.\\\\\\n    \\\\ Is that right?\"\\n  who: Alexey\\n- line: Exactly. Yep.\\n  sec: 757\\n  time: \\'12:37\\'\\n  who: Natalie\\n- header: How does ELT help analysts be more independent?\\n- line: \"Yeah, and this part about depending on engineering teams \\\\u2013 I\\'m curious.\\\\\\n    \\\\ How does it help analysts to be more independent now? Why do they not depend\\\\\\n    \\\\ on engineers now?\"\\n  sec: 759\\n  time: \\'12:39\\'\\n  who: Alexey\\n- line: \"Generally, analytics teams operate within the data warehouse itself. I know\\\\\\n    \\\\ you recently had an interview with Victoria, an analytics engineer. There\\'s\\\\\\n    \\\\ sort of a rise of this analytics engineer role, which is a role that is generally\\\\\\n    \\\\ found on the analytics team. Essentially, it\\\\u2019s managing the process from\\\\\\n    \\\\ the pipelines to the data warehouse and building out that transformation later.\\\\\\n    \\\\ Instead of business analysts or product analysts going to the engineering team,\\\\\\n    \\\\ which are generally more focused on the data platform or data infrastructure,\\\\\\n    \\\\ we can actually see this rise of the analytics engineer role. This role allows\\\\\\n    \\\\ there to be autonomy within the analytics team itself. That allows them to not\\\\\\n    \\\\ only understand the business, its needs, and impact, but also to be able to\\\\\\n    \\\\ make their changes very quickly.\"\\n  sec: 772\\n  time: \\'12:52\\'\\n  who: Natalie\\n- line: Basically, analysts were not necessarily strong engineers, so we have an analytic\\n    engineer role that can help them if something is more complex than just writing\\n    the usual SQL query, right?\\n  sec: 825\\n  time: \\'13:45\\'\\n  who: Alexey\\n- line: \"Yeah. I think a lot of these transformations can honestly be done using SQL.\\\\\\n    \\\\ That\\'s just very ubiquitous \\\\u2013 it\\'s a very well understood, very common\\\\\\n    \\\\ language. The level of access or the level of ability to access it and build\\\\\\n    \\\\ your own transformations \\\\u2013 that barrier is much lower. Even if the team\\\\\\n    \\\\ is so small that you don\\'t have an analytics engineer, you\\'re essentially empowering\\\\\\n    \\\\ your analysts to be much more full-stack and say, \\\\u201CI know that the data\\\\\\n    \\\\ is in the data warehouse, all I have to do is write SQL using something like\\\\\\n    \\\\ DBT. Then I can service any requests or generate any insights autonomously.\\\\\\n    \\\\ That reduces my time to be able to make positive relationships with my stakeholders.\\\\u201D\"\\n  sec: 840\\n  time: \\'14:00\\'\\n  who: Natalie\\n- line: Also, if the data is already extracted, I guess you have fewer steps to run.\\n  sec: 894\\n  time: \\'14:54\\'\\n  who: Alexey\\n- line: Yeah. Honestly, a big part of it is also speed. Because it\\'s already there\\n    and because these data warehouses have really scaled out how much time it takes\\n    to compute. The cost of storage is also way down and has produced a ton over time.\\n    The amount of speed it takes to actually even do these computing calculations\\n    is much lower.\\n  sec: 903\\n  time: \\'15:03\\'\\n  who: Natalie\\n- header: Data marts and Data warehouses\\n- line: \"You also mentioned one thing when talking about ETL \\\\u2013 this thing called\\\\\\n    \\\\ data mart. We also talked about the data warehouse. What are those? What is\\\\\\n    \\\\ a data mart? What is a data warehouse? What is the difference between them?\"\\n  sec: 930\\n  time: \\'15:30\\'\\n  who: Alexey\\n- line: \"=Data marts are very specific. Maybe we can use marketing again as a use\\\\\\n    \\\\ case. In this case, you could say \\\\u201CI\\'m going to build a data mart to serve\\\\\\n    \\\\ a dashboard that I\\'m going to build in Superset or Looker.\\\\u201D That data mart\\\\\\n    \\\\ specifically contains the average spend \\\\u2013 the Facebook spending \\\\u2013\\\\\\n    \\\\ aggregates. You put them together, build out how many leads came in from those\\\\\\n    \\\\ sources, how many customers actually converted from those sources, and actually\\\\\\n    \\\\ serve a marketing use case. On the same level, you can produce data marts for\\\\\\n    \\\\ sales, finance, or product. But they each serve a certain use case for the business.\"\\n  sec: 945\\n  time: \\'15:45\\'\\n  who: \"I think of data warehouses sort of as places to store data marts. When I think\\\\\\n    \\\\ about data warehouses, there\\'s an ingestion layer. Some users of ours, they\\'ll\\\\\\n    \\\\ call it an ingestion DB. Maybe within your data warehouse, you have multiple\\\\\\n    \\\\ databases. That first layer is almost like the rawest form that comes from Airbyte.\\\\\\n    \\\\ You hook your data warehouse up to, let\\'s say, Snowflake, and you have a database\\\\\\n    \\\\ called \\\\u201Cingestion DB\\\\u201D. That\\'s essentially it \\\\u2013 you don\\'t touch\\\\\\n    \\\\ it \\\\u2013 but that is where your next layer comes from. This could be maybe\\\\\\n    \\\\ a common layer, which is something that maybe several teams can draw from in\\\\\\n    \\\\ order to build out the data marts.\"\\n- line: So a data mart is basically a bunch of tables within a database, right? If\\n    I understood that correctly.\\n  sec: 1039\\n  time: \\'17:19\\'\\n  who: Alexey\\n- line: \"Yeah, it\\\\u2019s post-transformation. I think you can have a lot of different\\\\\\n    \\\\ types of data tables. But the ones that I would consider a data mart is like\\\\\\n    \\\\ a finalized table \\\\u2013 it\\'s almost production-ready. A business user can take\\\\\\n    \\\\ this and there are enough guardrails in place so that when they do pull metrics\\\\\\n    \\\\ out of it, they\\'re sanitized. They\\'re ready to use and the business user can\\\\\\n    \\\\ trust the data that comes out of there.\"\\n  sec: 1045\\n  time: \\'17:25\\'\\n  who: Natalie\\n- header: Ingestion DB\\n- line: \"So ingestion databases are everything that comes before data marts, right?\\\\\\n    \\\\ This is where the data that is maybe dirty or not cleaned or that is not aggregated\\\\\\n    \\\\ \\\\u2013 this is not something that business users can use. Right?\"\\n  sec: 1075\\n  time: \\'17:55\\'\\n  who: Alexey\\n- line: \"Exactly. It\\'s the rawest form. We generally wouldn\\'t want business users\\\\\\n    \\\\ to be pulling off the raw forms of data, because they\\'ll probably have to do\\\\\\n    \\\\ some transformation. That transformation might not be consistent across different\\\\\\n    \\\\ users in the business. So in order to reduce the potential mistakes or different\\\\\\n    \\\\ interpretations of the data down the line, that\\'s why that transformation layer\\\\\\n    \\\\ exists \\\\u2013 to separate and bifurcate the ingestion from the actual business\\\\\\n    \\\\ users and the data marts that they use.\"\\n  sec: 1091\\n  time: \\'18:11\\'\\n  who: Natalie\\n- header: ETL vs ELT\\n- line: \"So previously, in ETL, we would extract some data, we would immediately do\\\\\\n    \\\\ the transformation, apply it perhaps without saving it, and then put it into\\\\\\n    \\\\ a data warehouse or data mart. Now the data that we extract, we first put it\\\\\\n    \\\\ to the ingestion database, where we keep it, and then we run transformation\\\\\\n    \\\\ on top of this. Then we pull it again to create some tables that we call data\\\\\\n    \\\\ marts. This is where the data that is used by the final users \\\\u2013 the business\\\\\\n    \\\\ users \\\\u2013 is where we keep it. Is that right?\"\\n  sec: 1127\\n  time: \\'18:47\\'\\n  who: Alexey\\n- line: \"Yeah, exactly. Going back to ELT vs ETL \\\\u2013 previously, these transformations\\\\\\n    \\\\ might have been done outside the data warehouse, and now we\\'re bringing it into\\\\\\n    \\\\ the data warehouse. That\\'s the biggest difference here. That transform layer\\\\\\n    \\\\ is essentially operating within the destination and then does the transformation,\\\\\\n    \\\\ creating new tables within the exact same destination.\"\\n  sec: 1166\\n  time: \\'19:26\\'\\n  who: Natalie\\n- header: Data lakes\\n- line: And what is a data lake?\\n  sec: 1190\\n  time: \\'19:50\\'\\n  who: Alexey\\n- line: \"Yeah, it\\'s interesting because a data lake has some similarities to data\\\\\\n    \\\\ warehouses. But a data lake is much more unstructured. When we think about data\\\\\\n    \\\\ warehouses, they\\'re all relational tables \\\\u2013 they all have set schemas.\\\\\\n    \\\\ You can very easily pull from them using SQL. When we think about data lakes,\\\\\\n    \\\\ they\\'re a little bit more unstructured. I\\'d say the place that I\\'ve seen it\\\\\\n    \\\\ become very useful is when I was at Keep Truckin\\\\u2019. We were in the IoT business,\\\\\\n    \\\\ so we had a bigger warehouse and we had Snowflake. But the data that we had\\\\\\n    \\\\ on all our customers weren\\\\u2019t always in the table format. We would sometimes\\\\\\n    \\\\ be collecting videos using our hardware, and those are files. Those files are\\\\\\n    \\\\ not things that data warehouses can store and read. That\\'s something that really\\\\\\n    \\\\ belongs in a data lake, which is a lot more unstructured and can support these\\\\\\n    \\\\ different file types.\"\\n  sec: 1192\\n  time: \\'19:52\\'\\n  who: Natalie\\n- line: Basically, we just dump everything into a lake and then later figure out how\\n    to actually make it cleaner, more organized, and more structured. Is that right?\\n  sec: 1259\\n  time: \\'20:59\\'\\n  who: Alexey\\n- line: \"Yes, it\\'s definitely interesting at a very raw level. I know there are certain\\\\\\n    \\\\ other terms like \\\\u2018data swamp\\\\u2019 or things that were, you know\\\\u2026\"\\n  sec: 1271\\n  time: \\'21:11\\'\\n  who: Natalie\\n- header: Data swamps\\n- line: \"We actually have a question about this, \\\\u201CWhat is a data swamp? How can\\\\\\n    \\\\ a lake become a swamp?\\\\u201D\"\\n  sec: 1282\\n  time: \\'21:22\\'\\n  who: Alexey\\n- line: \"Yeah, I think when I\\'ve heard that term it\\'s generally because there\\'s maybe\\\\\\n    \\\\ low quality or maybe very unrefined data. I\\'ve also heard this term refer to\\\\\\n    \\\\ places or data lakes that have essentially become large places of just unused\\\\\\n    \\\\ data. You put so much in there, and there\\'s so little organization that it\\\\u2019\\\\\\n    s very difficult to actually be able to utilize what\\\\u2019s in there. Maybe over\\\\\\n    \\\\ time, especially as new people come in or people leave the team, it becomes\\\\\\n    \\\\ harder and harder to manage what is there and what is usable. Yeah, I heard\\\\\\n    \\\\ that term being used as a generic term to refer to data lakes that essentially\\\\\\n    \\\\ have low quality data \\\\u2013 data that people can\\'t trust.\"\\n  sec: 1289\\n  time: \\'21:29\\'\\n  who: Natalie\\n- header: Data governance\\n- line: \"Yeah, there is another buzzword, \\\\u201Cdata governance\\\\u201D \\\\u2013 I guess\\\\\\n    \\\\ this refers to making sure that your data lake doesn\\'t become a swamp. When\\\\\\n    \\\\ you make sure that the data is clean, what kind of data is there, everything\\\\\\n    \\\\ is accounted for. So you just keep it more organized, I guess.\"\\n  sec: 1341\\n  time: \\'22:21\\'\\n  who: Alexey\\n- line: \"The data governance term also definitely applies to data warehouses. I have\\\\\\n    \\\\ one company I worked at, we had this schema called \\\\u201Cad hoc.\\\\u201D Of course,\\\\\\n    \\\\ people are going to throw things into \\\\u201Cad hoc\\\\u201D whenever they want\\\\\\n    \\\\ \\\\u2013 there are no rules around it. So part of the data governance that we\\\\\\n    \\\\ did was, \\\\u201CHow do we ensure that in certain databases or schemas, it\\'s always\\\\\\n    \\\\ clear what they\\'re used for. It\\\\u2019s always clear how long things will stay\\\\\\n    \\\\ there.\\\\u201D Because I\\'ve kind of married into the definition of \\\\u201CHow is\\\\\\n    \\\\ this useful?\\\\u201D Of course, there\\'s always this continual inspection of what\\\\\\n    \\\\ is there, in order to ensure that it is still relevant or still will be used.\\\\\\n    \\\\ Rather than having almost a trash bin that never gets empty. You want to make\\\\\\n    \\\\ sure that your data warehouse or your data lake has that level of quality and\\\\\\n    \\\\ relevance.\"\\n  sec: 1368\\n  time: \\'22:48\\'\\n  who: Natalie\\n- line: \"Maybe not a trash bin, I\\'m thinking about my basement, which has all the\\\\\\n    \\\\ things that I don\\'t need right now. I don\\'t know what to do with them. I don\\'t\\\\\\n    \\\\ want to throw them away yet. So what to do with them? I\\'ll just put them in\\\\\\n    \\\\ my basement and figure out what to do with them later. You can do the same with\\\\\\n    \\\\ data, right? \\\\u201CDo I need to track this data? Maybe I do. Let\\'s track it.\\\\\\n    \\\\ Let\\'s keep this data.\\\\u201D Then one year later, you have this huge data source\\\\\\n    \\\\ that nobody uses. So it becomes a swamp.\"\\n  sec: 1427\\n  time: \\'23:47\\'\\n  who: Alexey\\n- line: Yeah, exactly.\\n  sec: 1463\\n  time: \\'24:23\\'\\n  who: Natalie\\n- header: Ingestion layer vs Data lake\\n- line: \"We also talked about the ingestion layer and the ingestion database. We talked\\\\\\n    \\\\ about the data lake. I\\\\u2019m wondering \\\\u2013 to me, they look similar. First\\\\\\n    \\\\ of all, are they similar? Are they the same? Or are those different things?\"\\n  sec: 1464\\n  time: \\'24:24\\'\\n  who: Alexey\\n- line: \"Yeah, I think Evo actually came up with a good article on this too. Maybe\\\\\\n    \\\\ we can put it in the links. She wrote about the difference and how they might\\\\\\n    \\\\ be converging in some ways. I\\'d say there\\'s still relevance for both. Data lakes\\\\\\n    \\\\ are obviously going to be more flexible \\\\u2013 they\\'re going to be able to support\\\\\\n    \\\\ a lot more different file types and structures. That\\'s the thing that data warehouses\\\\\\n    \\\\ don\\'t do. So there\\'s a purpose for both. From what I\\'ve noticed, data warehouses\\\\\\n    \\\\ are generally very helpful for smaller or intermediate-sized teams. As your\\\\\\n    \\\\ needs grow and become more complex \\\\u2013 maybe your organization gets larger\\\\\\n    \\\\ \\\\u2013 you may need to move to the data lake structure, which offers flexibility.\\\\\\n    \\\\ As your team organization grows, it might be something that you have to weigh\\\\\\n    \\\\ the pros and cons of, whether to even add a data lake as an addition, or potentially\\\\\\n    \\\\ migrating fully to it. But a lot of the functionalities of the industry are\\\\\\n    \\\\ allowing for the flexibility to choose between a data lake and a data warehouse.\"\\n  sec: 1485\\n  time: \\'24:45\\'\\n  who: Natalie\\n- line: \"Basically, the ingestion database is a part of a data warehouse, right? Maybe\\\\\\n    \\\\ this is one of the tables in the data warehouse. Let\\'s say we\\\\u2019re talking\\\\\\n    \\\\ about Snowflake \\\\u2013 this can be one of the tables that are already in Snowflake.\\\\\\n    \\\\ It\\'s just that the end users, the business users or analysts, don\\'t use this\\\\\\n    \\\\ particular table, but it\\'s still part of the warehouse. Is that right?\"\\n  sec: 1556\\n  time: \\'25:56\\'\\n  who: Alexey\\n- line: \"We were talking about the ingestion database. This is where we keep intermediate\\\\\\n    \\\\ results. To me, a data lake also seems like a place where we keep intermediate\\\\\\n    \\\\ results. So I was wondering \\\\u2013 are the ingestion layers part of the data\\\\\\n    \\\\ warehouse or not?\"\\n  who: Alexey\\n- line: \"I think in the analytics team framework, it generally is ingesting into a\\\\\\n    \\\\ data warehouse, not a data lake. Because they\\'re generally dealing with different\\\\\\n    \\\\ APIs, different sources, and then doing that transformation there and, of course,\\\\\\n    \\\\ doing the visualization on top. From an analytics team perspective, I think\\\\\\n    \\\\ the data warehouse is the most relevant. Where it may not be as relevant is\\\\\\n    \\\\ maybe for engineering teams, who need data lakes to power parts of their application,\\\\\\n    \\\\ or maybe data science teams who need to parse through lots of data that isn\\'t\\\\\\n    \\\\ necessarily in a structured format in order to do their analysis. I think it\\\\\\n    \\\\ depends on your business use case, what kind of team you\\'re on, and what is\\\\\\n    \\\\ helpful for you. You have to make that call \\\\u2013 what are the capabilities\\\\\\n    \\\\ that you really need to get your work done? Essentially, you choose the solution\\\\\\n    \\\\ from there.\"\\n  sec: 1607\\n  time: \\'26:47\\'\\n  who: Natalie\\n- header: Do you need both a Data warehouse and a Data lake?\\n- line: \"We have a question \\\\u2013 \\\\u201CDo we need to have both a data lake and a\\\\\\n    \\\\ data warehouse?\\\\u201D I think, from what I understood, the answer was \\\\u201C\\\\\\n    Yes.\\\\u201D Right? We have the raw data in the lake. We have prepared data in a\\\\\\n    \\\\ data mart in a data warehouse. Then if somebody such as data scientists, like\\\\\\n    \\\\ you said in your example \\\\u2013 if they need to parse through raw data, they\\\\\\n    \\\\ can just go ahead and do it.\"\\n  sec: 1659\\n  time: \\'27:39\\'\\n  who: Alexey\\n- line: \"I don\\\\u2019t think you need to have both. We don\\'t necessarily need it in\\\\\\n    \\\\ our business to have both. It really depends on the complexity of your business.\\\\\\n    \\\\ From an analytics perspective, generally, if I\\'m in the analytics team, I probably\\\\\\n    \\\\ will never touch a data lake. I\\\\u2019ll probably operate within the data warehouse.\\\\\\n    \\\\ But I know that there are teams within the organization that might rely on more\\\\\\n    \\\\ of a data lake structure instead. I think it really depends on the complexity\\\\\\n    \\\\ of the business and what different teams need.\"\\n  sec: 1687\\n  time: \\'28:07\\'\\n  who: Natalie\\n- line: Yeah. I prepared a question, but I think you already answered it. Let me ask\\n    the question and maybe I can answer it and then you tell me if I\\'m right.\\n  sec: 1718\\n  time: \\'28:38\\'\\n  who: Alexey\\n- line: Sure.\\n  sec: 1729\\n  time: \\'28:49\\'\\n  who: Natalie\\n- line: \"Let\\'s say we have an ecommerce online shop. We want to track some events\\\\\\n    \\\\ there \\\\u2013 so clicks. Every time a user comes to our online shop and selects\\\\\\n    \\\\ a product, clicks on this product, we track this event. These events \\\\u2013\\\\\\n    \\\\ these clicks \\\\u2013 they end up in the data lake where we keep the clicks. I\\\\\\n    \\\\ have a bunch of SQL queries to transform these clicks into something else \\\\u2013\\\\\\n    \\\\ so aggregate, calculate some statistics. I\\'m a data scientist, so what I do\\\\\\n    \\\\ is run some machine learning on top of these clicks. For example, I have a model\\\\\\n    \\\\ that wants to predict how many clicks there will be for each product. So I need\\\\\\n    \\\\ to use this information about the clicks. I write some SQL queries, extract\\\\\\n    \\\\ these clicks, and I build the model for that. Maybe instead of building a model,\\\\\\n    \\\\ I just put the clicks into a dashboard. Then the top management sees \\\\u201C\\\\\\n    Okay, in this category, we have that many clicks. In that category, we have that\\\\\\n    \\\\ many clicks.\\\\u201D Then to orchestrate everything, in our company at least,\\\\\\n    \\\\ we typically use Airflow for all these things.\"\\n  sec: 1730\\n  time: \\'28:50\\'\\n  who: Alexey\\n- line: \"So the question is, \\\\u201CIs this ETL, or ELT?\\\\u201D I think \\\\u2013 let me\\\\\\n    \\\\ answer this and you correct me \\\\u2013 I think this is ELT. Because first, we\\\\\\n    \\\\ dump everything into a data lake \\\\u2013 we don\\'t change the raw events. We leave\\\\\\n    \\\\ them be in the data lake. Then there are other jobs \\\\u2013 other transformation\\\\\\n    \\\\ jobs \\\\u2013 that take the raw data, transform, and then eventually put this\\\\\\n    \\\\ in a model or in a dashboard. Right?\"\\n  who: Alexey\\n- line: Exactly. Yeah. You\\'re not using a tool to do that transformation. You yourself\\n    are taking all the data that has been loaded into your area, and then doing something\\n    with it. Exactly.\\n  sec: 1844\\n  time: \\'30:44\\'\\n  who: Natalie\\n- line: \"Yeah. All this time I thought that Airflow was an ETL tool, but it\\\\u2019\\\\\\n    s actually an ELT tool, right?\"\\n  sec: 1859\\n  time: \\'30:59\\'\\n  who: Alexey\\n- line: \"Airflow? Yeah, I think it\\'s very much like an orchestrator. It also helps\\\\\\n    \\\\ to just schedule. But ultimately, yeah. Everybody has a very good integration\\\\\\n    \\\\ with Airflow that essentially runs your Airbyte jobs, using Airflow. So yeah\\\\\\n    \\\\ \\\\u2013 we also use Airflow here.\"\\n  sec: 1872\\n  time: \\'31:12\\'\\n  who: Natalie\\n- header: Airbyte and ELT\\n- line: \"I think you mentioned at the beginning what Airbyte does \\\\u2013 it\\'s about\\\\\\n    \\\\ transformation, right? It\\'s about ingesting and then putting it into a data\\\\\\n    \\\\ warehouse. Maybe now we can try to make sense from all these buzzwords. We know\\\\\\n    \\\\ what the transformation means. This is taking the data and changing it a little\\\\\\n    \\\\ bit. Then ingestion is about putting something into a data warehouse. Then a\\\\\\n    \\\\ data warehouse is basically the database that we use for all these analytical\\\\\\n    \\\\ purposes. So yeah, maybe you can tell us now what Airbyte does?\"\\n  sec: 1891\\n  time: \\'31:31\\'\\n  who: Alexey\\n- line: \"Yeah, so everybody tackles the E-L part. That\\'s our main goal \\\\u2013 to ensure\\\\\\n    \\\\ that the E-L is as seamless and reliable as any other product on the market\\\\\\n    \\\\ and that you have a great understanding and expectation of what the output in\\\\\\n    \\\\ your data warehouse is going to be. We also integrate really well with DBT,\\\\\\n    \\\\ right within the product. So we\\'re not handling the transformation ourselves,\\\\\\n    \\\\ per se, but we\\'re relying on DBT as a part of our product to ensure that analysts\\\\\\n    \\\\ can use DBT to do those SQL transformations once the data is there. We\\'re not\\\\\\n    \\\\ like a transform product necessarily, but we just integrate really well with\\\\\\n    \\\\ that and have embedded that into our product.\"\\n  sec: 1931\\n  time: \\'32:11\\'\\n  who: Natalie\\n- line: \"One thing I didn\\'t actually mention earlier is that Airbyte is also open\\\\\\n    \\\\ source. We are really focused on building our community, enabling users \\\\u2013\\\\\\n    \\\\ people out there who are excited to contribute back to our project \\\\u2013 to\\\\\\n    \\\\ enable those people to actually build out potentially new connectors or maybe\\\\\\n    \\\\ even amend existing ones, and contribute back to our project.\"\\n  who: Natalie\\n- line: DBT is also open source, right?\\n  sec: 2011\\n  time: \\'33:31\\'\\n  who: Alexey\\n- line: Yes, exactly. DBT is also open source. It\\'s part of that modern data stack,\\n    you could say, for the evolution towards more open source tools. They also have\\n    a cloud product.\\n  sec: 2013\\n  time: \\'33:33\\'\\n  who: Natalie\\n- header: Modern data stack\\n- line: \"Yeah. So speaking of this modern stack, I\\'ve heard this term many times and\\\\\\n    \\\\ actually we have a talk about this quite soon. It\\'s about this modern stack\\\\\\n    \\\\ for analytics. Actually the talk we have is \\\\u201Cmodern data stack for analytics\\\\\\n    \\\\ engineering.\\\\u201D I don\\'t know if there are different stacks for analytics\\\\\\n    \\\\ and for analytics engineering \\\\u2013 probably they\\\\u2019re the same. So, what\\\\\\n    \\\\ is it? Can you tell us a bit about it? Which tools are a part of this stack?\\\\\\n    \\\\ Why do we even talk about it? Why is it a thing?\"\\n  sec: 2025\\n  time: \\'33:45\\'\\n  who: Alexey\\n- line: \"So why it\\'s a thing \\\\u2013 because essentially, you are now able to choose\\\\\\n    \\\\ each piece of the stack individually instead of having a platform approach where\\\\\\n    \\\\ \\\\u201Cone fits all\\\\u201D \\\\u2013 where you have a lot of vendor lock-in. You\\\\\\n    \\\\ now get to choose the best of breed for each of the pieces of the data puzzle.\\\\\\n    \\\\ For extract and load obviously, there\\'s Airbyte. There are also incomes like\\\\\\n    \\\\ Fivetran that have been around for quite a bit longer. From a data warehousing\\\\\\n    \\\\ perspective, you have Snowflake, you have Databricks, BigQuery, Amazon Redshift.\\\\\\n    \\\\ Then for transformation, you have DBT. Outside of DBT and all the features it\\\\\\n    \\\\ provides, you could just write SQL and that would also work as well. Then from\\\\\\n    \\\\ a visualization perspective, we see new tools like Superset being adopted fairly\\\\\\n    \\\\ well. Then obviously, incumbents like Looker, or even Tableau. The idea of the\\\\\\n    \\\\ modern data stack is that instead of having one solution that tries to do it\\\\\\n    \\\\ all, you\\'re essentially picking and choosing the one that really fits with what\\\\\\n    \\\\ you need the best.\"\\n  sec: 2063\\n  time: \\'34:23\\'\\n  who: Natalie\\n- line: So basically, it\\'s a bunch of tools that work really well together.\\n  sec: 2138\\n  time: \\'35:38\\'\\n  who: Alexey\\n- header: Reverse ETL\\n- line: Yeah, and of course, we can\\'t forget Airflow, which does a lot of the orchestration.\\n    Then there\\'s also this emerging space of reverse ETL, where you\\'ll have tools\\n    like Hightouch or Census, and even Airbyte is thinking about going into this space\\n    as well.\\n  sec: 2142\\n  time: \\'35:42\\'\\n  who: Natalie\\n- line: \"Yeah, so can you tell us a bit more about this \\\\u201Creverse ETL\\\\u201D? Or\\\\\\n    \\\\ should it be reverse ETL or reverse ELT? Or what is that anyways? Why is it\\\\\\n    \\\\ reverse? Why would you want to reverse it?\"\\n  sec: 2162\\n  time: \\'36:02\\'\\n  who: Alexey\\n- line: \"=In the past, what I\\'ve seen data teams use is maybe a Python wrapper to\\\\\\n    \\\\ push data back into Salesforce. These \\\\u201Creverse ETL\\\\u201D tools are enabling\\\\\\n    \\\\ really low-code solutions for salespeople or marketers to actually come and\\\\\\n    \\\\ just kind of \\\\u201Cpoint and click\\\\u201D and say, \\\\u201CI want to copy this\\\\\\n    \\\\ table and the output of this table in this data warehouse and bring it back\\\\\\n    \\\\ into my source system to be able to action on it.\\\\u201D You don\\'t have to be\\\\\\n    \\\\ technical \\\\u2013 it\\'s pretty low-code or no code. That\\'s really something that\\'s\\\\\\n    \\\\ very powerful, because it essentially allows analytics to be a function within\\\\\\n    \\\\ the organization itself. It allows analysts to really be very aligned with what\\\\\\n    \\\\ the business needs.\"\\n  sec: 2174\\n  time: \\'36:14\\'\\n  who: \"Reverse ETL is definitely something that a lot of data teams are trying to\\\\\\n    \\\\ already solve today using custom scripts that bring a lot of that analysis that\\\\\\n    \\\\ analytics teams do. It also brings that back into the operational systems that\\\\\\n    \\\\ business users actually need that data in. One good example is \\\\u2013 let\\'s\\\\\\n    \\\\ say that an analytics team is working on a lead scoring model. Essentially,\\\\\\n    \\\\ it says, \\\\u201CI have 100 leads. I rank them using behavioral data, demographic\\\\\\n    \\\\ data. I take this information and I rate these leads from 1 to 100 on what the\\\\\\n    \\\\ priority is \\\\u2013 who you should reach out to.\\\\u201D Traditionally, that data\\\\\\n    \\\\ would just live in a data warehouse and maybe in a visualization tool too. If\\\\\\n    \\\\ I\\'m a salesperson, I need that data in the system that I\\'m using to actually\\\\\\n    \\\\ action on it.\"\\n- line: \"Basically, before, engineers would need to write a bunch of scripts for doing\\\\\\n    \\\\ this. This is emphasized in the healthcare APIs that allow them to push the\\\\\\n    \\\\ data there. But I guess it\\'s not easy to maintain these scripts and it\\'s also\\\\\\n    \\\\ not the core business of the companies to do that. So there are some tools that\\\\\\n    \\\\ actually allow you to have this drag-and-drop experience, so you can say, \\\\u201C\\\\\\n    Okay, this data from this table in my BigQuery or Snowflake should go in my Salesforce\\\\\\n    \\\\ or something else.\\\\u201D Right?\"\\n  sec: 2281\\n  time: \\'38:01\\'\\n  who: Alexey\\n- line: \"Exactly. Yeah. I would still consider this reverse-ETL, not reverse-ELT,\\\\\\n    \\\\ because that transformation is not happening in that source where you\\'re pushing\\\\\\n    \\\\ it back to. The transformation is still happening before you move it out of\\\\\\n    \\\\ the database. Really, it\\'s like a porting of the more finalized\\\\u2026 maybe\\\\\\n    \\\\ you could even call it a data mart and bring it back into the source. No transformation\\\\\\n    \\\\ is actually happening in the source system itself.\"\\n  sec: 2316\\n  time: \\'38:36\\'\\n  who: Natalie\\n- header: Is drag-and-drop killing data engineering jobs?\\n- line: \"To make sure I understood the whole picture: we have some of these tools\\\\\\n    \\\\ like Google AdWords \\\\u2013 all these systems, like Google AdWords, or Facebook\\\\\\n    \\\\ Ads, or whatever. We first need to take the data from there and import \\\\u2013\\\\\\n    \\\\ put it into our data warehouse or ingest. We import and then we do something\\\\\\n    \\\\ and then we export back, right? Or using the terminology we just learned, we\\\\\\n    \\\\ first extract, then do something, and then we do this reverse extract, and then\\\\\\n    \\\\ put that back.\"\\n  sec: 2346\\n  time: \\'39:06\\'\\n  who: Alexey\\n- line: \"Speaking of this low-code/no-code, we have a question related to that. \\\\u201C\\\\\\n    Is the data engineering job dying with all these tools that give a drag-and-drop\\\\\\n    \\\\ experience? Since you can do these kinds of drag-and-drop data pipelines with\\\\\\n    \\\\ all these built integrations?\"\\n  who: Alexey\\n- line: \"I would not say dying, I think it is very much evolving. I think in data\\\\\\n    \\\\ engineering, these tools are essentially allowing for the more mundane parts\\\\\\n    \\\\ of data engineers\\\\u2019 job to disappear and allow for them to focus on other\\\\\\n    \\\\ things. For example, in my team at Keep Truckin\\\\u2019, our data engineer was\\\\\\n    \\\\ very much focused on a lot more data infrastructure pieces, instead of being\\\\\\n    \\\\ focused on managing pipelines and waking up in the morning, and feeling like,\\\\\\n    \\\\ \\\\u201COh, these pipelines have broken, and I need to go fix that. This field\\\\\\n    \\\\ was deleted.\\\\u201D It was more around tooling for the analytics team \\\\u2013\\\\\\n    \\\\ ensuring that we have proper data governance pieces in place.\"\\n  sec: 2413\\n  time: \\'40:13\\'\\n  who: Natalie\\n- line: \"There are a lot of things that really are beyond the technical scope of even\\\\\\n    \\\\ maybe any analytics engineer or an analyst \\\\u2013 where a data engineer most\\\\\\n    \\\\ definitely can enable that data team to be operating very efficiently. Something\\\\\\n    \\\\ like common code standards, being able to bring the analytics team to a place\\\\\\n    \\\\ where they can be pushing out in a nearly-continuous delivery process. They\\\\u2019\\\\\\n    re ensuring that there\\'s validation of the code and that pipelines aren\\'t breaking\\\\\\n    \\\\ from the data team and what they\\'re producing. There are a lot of pieces that\\\\\\n    \\\\ I think the data engineer can now actually go and tackle that the analytics\\\\\\n    \\\\ team might not necessarily be very focused on. But without these things, they\\\\\\n    \\\\ actually can\\'t be successful.\"\\n  who: Natalie\\n- line: \"We talked about these scripts that people would write before reverse ETL\\\\\\n    \\\\ tools existed. I imagined that maintaining the scripts was a nightmare because\\\\\\n    \\\\ they break in unpredictable ways. For example the API changes and then all your\\\\\\n    \\\\ scripts are not working. Then you have to deal with all these intricacies \\\\u2013\\\\\\n    \\\\ I\\\\u2019m guessing that this is not fun at all. A data engineer would probably\\\\\\n    \\\\ rather focus on other things. I\\'m not a data engineer, but I don\\'t really want\\\\\\n    \\\\ to even think about maintaining scripts for talking to some third party tools\\\\\\n    \\\\ like Salesforce and trying to maintain them. Yeah, I\\'d rather focus on something\\\\\\n    \\\\ else. I guess this is why these tools are quite useful and why people love them.\\\\\\n    \\\\ Data engineers are still happy \\\\u2013 nobody is going to fire them anytime soon.\"\\n  sec: 2520\\n  time: \\'42:00\\'\\n  who: Alexey\\n- line: Yeah.\\n  sec: 2581\\n  time: \\'43:01\\'\\n  who: Natalie\\n- header: Who is responsible for managing unused data?\\n- line: \"Okay, thanks. We have some more questions. The question is \\\\u201C70-90% of\\\\\\n    \\\\ beta in many organizations is collected but never used. Who is responsible for\\\\\\n    \\\\ taking care of that and for noticing that? Data engineers? How should we actually\\\\\\n    \\\\ go about noticing things like that?\\\\u201D\"\\n  sec: 2582\\n  time: \\'43:02\\'\\n  who: Alexey\\n- line: \"If I can think back to my time when I took on more of that analytics manager\\\\\\n    \\\\ role, I would say it\\'s very much a team effort. It\\'s hard to know what is not\\\\\\n    \\\\ being used if you don\\'t have the business analysts there trying to speak to,\\\\\\n    \\\\ \\\\u201CWhat are the use cases that we\\'re solving for in that business today?\\\\u201D\\\\\\n    \\\\ And then tracing that back to the ingestion layer, \\\\u201CWhat is a dependency\\\\\\n    \\\\ of those use cases? In order to figure out what isn\\'t being used \\\\u2013 I remember\\\\\\n    \\\\ how we would try to do this on a quarterly or monthly cleanup level \\\\u2013 we\\\\\\n    \\\\ really try to take a critical look as a team. It wouldn\\'t be on a single person\\\\\\n    \\\\ to really be responsible to know everything, because that\\'s impossible. We would\\\\\\n    \\\\ really rely a lot on the business analyst and I guess the analytics engineers\\\\\\n    \\\\ to have them understand and be able to trace back to what is actually being\\\\\\n    \\\\ used and what are things that may not be used today, but might be used in the\\\\\\n    \\\\ future. So you always want to have that forward-looking piece too. Of course,\\\\\\n    \\\\ this whole idea of ELT is that you have all the data there, and it maybe might\\\\\\n    \\\\ not be used now, but potentially. If there\\'s a use case for that in the future,\\\\\\n    \\\\ someone should speak to that.  I don\\'t think it should ever be on one person.\\\\\\n    \\\\ I think that would be a pretty difficult role to have if it was, because that\\\\\\n    \\\\ person would be missing the context of the actual business.\"\\n  sec: 2617\\n  time: \\'43:37\\'\\n  who: Natalie\\n- line: \"The person who doesn\\'t miss this context \\\\u2013 who has the context \\\\u2013\\\\\\n    \\\\ would be an analytics engineer, perhaps or an analyst. Right?\"\\n  sec: 2718\\n  time: \\'45:18\\'\\n  who: Alexey\\n- line: \"I think it\\'s both the business analyst and the analytics engineer. Because\\\\\\n    \\\\ the business analyst might be really focused and working with the business,\\\\\\n    \\\\ but they might not know as much about the pipelining. So they need to work together\\\\\\n    \\\\ to ensure that they both have a mutual understanding. Then whoever is in charge\\\\\\n    \\\\ of managing the data governance, the cleanliness of the database, then they\\\\\\n    \\\\ need to communicate with them that, \\\\u201CHey, this is data that\\'s not currently\\\\\\n    \\\\ being used,\\\\u201D and then execute on cleaning it up from there.\"\\n  sec: 2728\\n  time: \\'45:28\\'\\n  who: Natalie\\n- header: \"CDC \\\\u2013 Change Data Capture\"\\n- line: \"Thank you. Another question we have is, \\\\u201CI have no idea what CDC is.\\\\\\n    \\\\ Do you know what CDC is?\\\\u201D\"\\n  sec: 2759\\n  time: \\'45:59\\'\\n  who: Alexey\\n- line: \"Yeah. It\\'s \\\\u201Cchange data capture.\\\\u201D That\\'s a feature that is available\\\\\\n    \\\\ in our connectors. CDC is essentially a way to be able to capture only changed\\\\\\n    \\\\ records. That\\'s where the recording product comes from. Essentially, what it\\\\\\n    \\\\ allows you to do is avoid having to fully replicate your database every time.\\\\\\n    \\\\ Instead, let\\'s say, I sync my database today \\\\u2013 tomorrow, only 10% of those\\\\\\n    \\\\ rows have changed. I only want to sync those 10%. And I only want to capture\\\\\\n    \\\\ those 10% that have changed and then only update those 10% in my destination.\\\\\\n    \\\\ Without Changed Data Capture, you might have to be doing a whole replication\\\\\\n    \\\\ every day. That isn\\'t really the optimal way to manage cloud resources, because\\\\\\n    \\\\ you\\'re consuming more resources to do that replication. By doing CDC, you actually\\\\\\n    \\\\ have the ability to reduce your own cloud costs if you\\'re self-hosting. But\\\\\\n    \\\\ also, it\\'s just much faster because you\\'re moving less data.\"\\n  sec: 2771\\n  time: \\'46:11\\'\\n  who: Natalie\\n- line: \"I\\'m trying to think of an example. I work at OLX, an online marketplace.\\\\\\n    \\\\ This is a place, let\\'s say, if you want to sell your phone, you go create a\\\\\\n    \\\\ listing. Sometimes users \\\\u2013 the sellers \\\\u2013 can go and change the title,\\\\\\n    \\\\ or they can go and change the price. I guess this CDC (Change Data Capture)\\\\\\n    \\\\ will allow us to see\\\\u2026 let\\\\u2019s say if we have 30 million active listings\\\\\\n    \\\\ right now on the website \\\\u2013 we don\\'t want to look at the entire database\\\\\\n    \\\\ of listings. If something changes, if the prices change or titles change, we\\\\\\n    \\\\ just want to see that and keep the delta (difference between the old version\\\\\\n    \\\\ and the changes). Or we keep only the new thing instead of taking all the 30\\\\\\n    \\\\ million records and keeping them over and over again. Is that right?\"\\n  sec: 2846\\n  time: \\'47:26\\'\\n  who: Alexey\\n- line: \"Yeah, exactly. It\\\\u2019s essentially a performance consideration. It also\\\\\\n    \\\\ allows you to capture deleted rows. So that\\'s another benefit as well. I think\\\\\\n    \\\\ that we don\\'t offer it on all of our data warehouse sources yet. But we are\\\\\\n    \\\\ actively working on building out CDC capabilities for all the sources that essentially\\\\\\n    \\\\ allow for that.\"\\n  sec: 2910\\n  time: \\'48:30\\'\\n  who: Natalie\\n- header: Slowly changing dimension\\n- line: \"Do you know what a \\\\u201Cslowly changing dimension\\\\u201D is? I\\\\u2019ve heard\\\\\\n    \\\\ this term a few times. I\\'m curious what this is.\"\\n  sec: 2938\\n  time: \\'48:58\\'\\n  who: Alexey\\n- line: Yeah, I can speak to what I think it means.\\n  sec: 2945\\n  time: \\'49:05\\'\\n  who: Natalie\\n- line: I\\'m also not 100% sure what it actually is, but I hear this term used many\\n    times.\\n  sec: 2950\\n  time: \\'49:10\\'\\n  who: Alexey\\n- line: \"Yeah, I think in the business, you will probably start a pipeline process\\\\\\n    \\\\ with maybe 10 columns that you know you need. Maybe over time, if let\\'s say\\\\\\n    \\\\ a salesperson says, \\\\u201COh, I\\'m actually now going to collect information\\\\\\n    \\\\ on whether or not they\\'d be interested in this new product feature we just launched.\\\\u201D\\\\\\n    \\\\ And they added maybe a checkbox or maybe a picklist in Salesforce. The slowly\\\\\\n    \\\\ changing dimension to me, when I hear that term, means your dimensions may change\\\\\\n    \\\\ over time as your business changes. Now that the sales team is collecting new\\\\\\n    \\\\ information, you also want to ingest that new information into your data warehouse.\\\\\\n    \\\\ That will mean that your dimensions change and that you will actually want to\\\\\\n    \\\\ adjust not just 10 fields, but now 11. Then maybe next week it\\'s 12, because\\\\\\n    \\\\ now they\\'re collecting something else, or there\\'s another piece of data that\\'s\\\\\\n    \\\\ relevant to what you need. That\\'s what I think of when I hear that. I hope that\\\\\\n    \\\\ answers the question.\"\\n  sec: 2957\\n  time: \\'49:17\\'\\n  who: Natalie\\n- line: \"Well, I think the example you gave about a new product feature that a user\\\\\\n    \\\\ is interested in \\\\u2013 this user is currently interested in this feature, but\\\\\\n    \\\\ maybe in one year, the user is no longer interested. I guess this doesn\\'t change\\\\\\n    \\\\ quickly \\\\u2013 it changes slowly, right?\"\\n  sec: 2418\\n  time: \\'40:18\\'\\n  who: Alexey\\n- line: \"Well, when I think about dimensions, to me, it\\'s like adding a new column\\\\\\n    \\\\ in a table structure \\\\u2013 the value of that column, the field might change.\\\\\\n    \\\\ So that\\'s kind of like capturing the history of the field. But ultimately, the\\\\\\n    \\\\ way to think about it is, you\\'re actually capturing an additional dimension\\\\\\n    \\\\ of data that you weren\\'t capturing before. I don\\'t think that that ever happens\\\\\\n    \\\\ all at once in a business. A business is constantly evolving and changing, especially\\\\\\n    \\\\ if you\\'re small and you\\'re in that growth phase. You\\'re constantly trying to\\\\\\n    \\\\ think of new things to track, maybe launching new products or new product features.\\\\\\n    \\\\ There\\'s always going to be this ever-changing and growing set of dimensions\\\\\\n    \\\\ that you\\'ll want to track and that\\'s where the \\\\u201Cslowly changing dimensions\\\\u201D\\\\\\n    \\\\ aspect comes into place.\"\\n  sec: 2439\\n  time: \\'40:39\\'\\n  who: Natalie\\n- header: Are there cases where ETL is preferable over ELT?\\n- line: Do you know of any examples when we still would prefer ETL over ELT?\\n  sec: 2490\\n  time: \\'41:30\\'\\n  who: Alexey\\n- line: \"I would say \\\\u2013 if there\\'s a large enterprise need for it. I personally\\\\\\n    \\\\ can\\'t speak to being in a major enterprise company and having a need for this,\\\\\\n    \\\\ but it might be needed there. It might be something that much larger enterprises\\\\\\n    \\\\ might want to adopt. I think that is kind of the play where ETL has really been\\\\\\n    \\\\ successful \\\\u2013 in these large enterprises. Where you\\'re potentially combining\\\\\\n    \\\\ multiple data warehouses or data sources and bringing them together, and then\\\\\\n    \\\\ pushing them out to multiple data warehouses or lakes. So maybe there\\'s a need\\\\\\n    \\\\ for this kind of intermediary place, maybe a staging area, where you need to\\\\\\n    \\\\ ingest from a lot and then you need to propagate out a lot.\"\\n  sec: 2500\\n  time: \\'41:40\\'\\n  who: Natalie\\n- line: Yeah, I think I worked at an enterprise and we had all these tools like Oracle,\\n    Informatica, and all these kinds of things. I\\'m pretty sure if I come back now\\n    and see what they use it\\'s still Oracle and Informatica. It\\'s been working for\\n    them pretty well at the bank where I worked. We were processing a lot of data\\n    there.\\n  sec: 2558\\n  time: \\'42:38\\'\\n  who: Alexey\\n- line: Yeah. If there\\'s a certain use case for it. The place that I could see a use\\n    case for that kind of staging area and that really complex model, is that intermediary\\n    that essentially allows you to message things from many places to one and then\\n    from one to many again. I think smaller companies don\\'t generally have that need\\n    as strongly but much more complex organizations might be using a different warehouse\\n    for every business unit, or a different data lake to service different teams.\\n    That might be where they need some sort of intermediary solution.\\n  sec: 2584\\n  time: \\'43:04\\'\\n  who: Natalie\\n- header: Why is Airbyte open source?\\n- line: \"Thank you. The last question they prepared for you was about\\\\u2026 we talked\\\\\\n    \\\\ about open source, that Airbyte is open source, and we also talked about DBT\\\\\\n    \\\\ being open source. Do you know why Airbyte is open source? Why make it open\\\\\\n    \\\\ source? Aren\\'t you afraid that somebody will come and just steal your code?\"\\n  sec: 2625\\n  time: \\'43:45\\'\\n  who: Alexey\\n- line: \"Your first question of \\\\u201CWhy open source?\\\\u201D I really think that this\\\\\\n    \\\\ is the way forward for this space. When you look at incumbents in the place\\\\\\n    \\\\ like Fivetran, they\\'re never going to be able to support the long tail of connectors\\\\\\n    \\\\ that really exists out there. This explosion of tools that we\\'re seeing in pretty\\\\\\n    \\\\ much every space means that every tool has an API, they are all housing your\\\\\\n    \\\\ business data, and all of that data is really relevant. But there\\'s kind of\\\\\\n    \\\\ a long tail of connectors that may not be like NetSuite, or like AdWords, like\\\\\\n    \\\\ these really popular ones, but maybe less popular ones that people are still\\\\\\n    \\\\ using and experimenting with and trying out and growing with. Those need to\\\\\\n    \\\\ be supported too. Right now, what we\\'re seeing in this space \\\\u2013 and this\\\\\\n    \\\\ is how I like to think Airbyte actually came to be \\\\u2013 our founders did a\\\\\\n    \\\\ bunch of interviews, what they heard was, \\\\u201CYeah, we\\'re using Fivetran or\\\\\\n    \\\\ Stitch. But we\\'re still writing our own pipelines. We\\'re still building things\\\\\\n    \\\\ on the side. We\\'re still managing these numbers of scripts that tackle that\\\\\\n    \\\\ long tail, because the business still needs that data.\\\\u201D\"\\n  sec: 2652\\n  time: \\'44:12\\'\\n  who: Natalie\\n- line: That\\'s not the future that we see. We want our community and us to enable\\n    that community to really be able to support the many connectors that should exist\\n    out there. We don\\'t see something like a closed source project being able to support\\n    that. Being open source enables us to work like we have many hands, so to say.\\n    When people contribute, we accelerate at such a higher velocity that we can actually\\n    become the standard for data integration.\\n  who: Natalie\\n- line: \"So basically, if I use some proprietary tool and I use something that this\\\\\\n    \\\\ proprietary tool doesn\\'t support \\\\u2013 some very unpopular system that, for\\\\\\n    \\\\ some reasons, we use at work. We need to be able to extract data from there.\\\\\\n    \\\\ If I use something like Fivetran, as you mentioned, or Stitch, they can say,\\\\\\n    \\\\ \\\\u201CYeah, we will consider implementing this in five years\\\\u2026 or never.\\\\u201D\\\\\\n    \\\\ But if you use an open source tool a developer can actually just go ahead and\\\\\\n    \\\\ implement and then plug this thing into existing infrastructure and it just\\\\\\n    \\\\ works. Is that right? Is that the main idea?\"\\n  sec: 2766\\n  time: \\'46:06\\'\\n  who: Alexey\\n- line: We do see a lot of people actually plugging in their custom connectors. We\\n    have a place in the UI where you just add a new source. We have a CDK (connector\\n    development kit) to enable people to build things themselves and it\\'s very flexible.\\n    People can essentially fork our project and bring in custom connectors that they\\n    have. Maybe custom business logic or things that they want to ingrain into their\\n    connector, so they use Airbyte that way.\\n  sec: 2812\\n  time: \\'46:52\\'\\n  who: Natalie\\n- line: \"To your second question, though, I think\\\\u2026 We are open source and we\\\\\\n    \\\\ always want to enable our long-term connectors to be available to anyone to\\\\\\n    \\\\ use. We want to make it super easy for small or medium-sized teams to just get\\\\\\n    \\\\ that basic functionality of being able to be supported by connectors anytime.\\\\\\n    \\\\ We\\'ll always have our connectors be open source. We are coming to the market\\\\\\n    \\\\ with a cloud offering which is more that enterprise set of features like SSO,\\\\\\n    \\\\ certain things around security like RBAC (role based access control), and other\\\\\\n    \\\\ features that generally larger enterprise teams will want. For a small team\\\\\\n    \\\\ or a single developer, they don\\'t necessarily have a need for these, but they\\\\\\n    \\\\ just want to get up and running very quickly with connectors and moving data.\\\\\\n    \\\\ That\\'s the part that will always be a part of our mission and goal.\"\\n  who: Natalie\\n- header: The case of Elasticsearch and AWS\\n- line: Have you heard about this story about Elasticsearch and AWS? I think everyone\\n    whose model has open sourcing in their code probably heard about this story. But\\n    for those who don\\'t know, Elasticsearch had their own cloud offering. So if you\\n    don\\'t want to maintain your own cluster of Elasticsearch servers, you just go\\n    to Elasticsearch and use a managed solution. Then one day, AWS decided that they\\n    also want to provide a managed solution of ElasticSearch. Now Elasticsearch has\\n    a problem, right? Because AWS just took their code and deployed it. Now, people\\n    will go to AWS, for example, instead of going to Elasticsearch for a managed solution.\\n    So, are you not afraid that something like this can happen? That somebody will\\n    basically do the same thing? And because you\\'re open source, they can actually\\n    just do this.\\n  sec: 2906\\n  time: \\'48:26\\'\\n  who: Alexey\\n- line: \"Yeah, it\\'s definitely something that we think very carefully about. The things\\\\\\n    \\\\ that we talk about internally are \\\\u201CAre we under the right license? We\\'re\\\\\\n    \\\\ currently under MIT. Is this the right license for us moving forward, especially\\\\\\n    \\\\ as we launch cloud?\\\\u201D These are definitely things that we consider very\\\\\\n    \\\\ carefully. I think probably there\\\\u2019s more to come soon in the coming month\\\\\\n    \\\\ on that \\\\u2013 on whether we have to make any changes or not. But that\\\\u2019\\\\\\n    s definitely something that we actively discuss internally.\"\\n  sec: 2972\\n  time: \\'49:32\\'\\n  who: Natalie\\n- line: \"Yeah, I guess many open source companies are starting to think about this.\\\\\\n    \\\\ This story of AWS and Elasticsearch \\\\u2013 new things keep appearing. Now, all\\\\\\n    \\\\ of a sudden, Elasticsearch are the bad people because they are starting to hide\\\\\\n    \\\\ things, they are starting to close source some things. I\\\\u2019m curious to see\\\\\\n    \\\\ how it will end and I hope Elasticsearch figures it out.\"\\n  sec: 3605\\n  time: \\'1:00:05\\'\\n  who: Alexey\\n- line: Do you have any last words before we finish?\\n  sec: 3636\\n  time: \\'1:00:36\\'\\n  who: Alexey\\n- header: Conclusion\\n- line: \"It was such a pleasure to be on this, talking about these acronyms. I hope\\\\\\n    \\\\ it helped some of your listeners get more clarity. Airbyte \\\\u2013 check us out.\\\\\\n    \\\\ We are also hiring on a lot of different fronts. Not just on the engineering\\\\\\n    \\\\ front, but also within the go-to-market side as well. So check us out. Our entire\\\\\\n    \\\\ handle gets listed on our company docs page \\\\u2013 very public. If you want\\\\\\n    \\\\ to contribute back or check us out, you can do that very easily. All the information\\\\\\n    \\\\ is on our website.\"\\n  sec: 3642\\n  time: \\'1:00:42\\'\\n  who: Natalie\\n- line: Thank you. How can people find you if they have a question?\\n  sec: 3677\\n  time: \\'1:01:17\\'\\n  who: Alexey\\n- line: For me, just on LinkedIn - Natalie Kwong. That is the best place to find me.\\n  sec: 3680\\n  time: \\'1:01:20\\'\\n  who: Natalie\\n- line: Ok. Thanks a lot. Thanks for joining us today. Thanks for telling us about\\n    acronyms. Now I can make sense of them and hopefully, everyone else can as well.\\n    Thanks, everyone, for joining us and for asking questions and for watching us.\\n  sec: 3689\\n  time: \\'1:01:29\\'\\n  who: Alexey\\n\\n---\\n\\n\\nLinks:\\n\\n* [Natalie\\'s LinkedIn](https://www.linkedin.com/in/nataliekwong/){:target=\"_blank\"}\\n* [Why the Future of ETL Is Not ELT, But EL(T)](https://airbyte.io/blog/why-the-future-of-etl-is-not-elt-but-el){:target=\"_blank\"}')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "github_data[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "442deea1-19b0-4091-bb3b-77b3556cd892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import frontmatter\n",
    "\n",
    "def parse_data(data_raw):\n",
    "    data_parsed = []\n",
    "    errors = []\n",
    "    \n",
    "    for f in data_raw:\n",
    "        try:\n",
    "            post = frontmatter.loads(f.content)\n",
    "            data = post.to_dict()\n",
    "            data['filename'] = f.filename\n",
    "            data_parsed.append(data)\n",
    "        except Exception as e:\n",
    "            # Log the error with filename for debugging\n",
    "            errors.append({\n",
    "                'filename': f.filename,\n",
    "                'error': str(e),\n",
    "                'error_type': type(e).__name__\n",
    "            })\n",
    "            print(f\"Error parsing {f.filename}: {type(e).__name__}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\n{len(errors)} file(s) failed to parse out of {len(data_raw)} total files\")\n",
    "        print(\"\\nFirst few errors:\")\n",
    "        for err in errors[:5]:\n",
    "            print(f\"  - {err['filename']}\")\n",
    "            print(f\"    {err['error_type']}: {err['error']}\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully parsed {len(data_parsed)} files\")\n",
    "    return data_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f923e8c6-8f7a-40dc-bd80-6d64a90c5ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing _podcast/_template.md: ConstructorError: while constructing a mapping\n",
      "  in \"<unicode string>\", line 6, column 8\n",
      "found unhashable key\n",
      "  in \"<unicode string>\", line 6, column 9\n",
      "\n",
      "1 file(s) failed to parse out of 185 total files\n",
      "\n",
      "First few errors:\n",
      "  - _podcast/_template.md\n",
      "    ConstructorError: while constructing a mapping\n",
      "  in \"<unicode string>\", line 6, column 8\n",
      "found unhashable key\n",
      "  in \"<unicode string>\", line 6, column 9\n",
      "\n",
      "Successfully parsed 184 files\n"
     ]
    }
   ],
   "source": [
    "parsed_data = parse_data(github_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1c59c72-59ad-4612-8d66-98e942bd7c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of podcast documents: 184\n"
     ]
    }
   ],
   "source": [
    "num_podcasts = len(parsed_data)\n",
    "print(f\"Number of podcast documents: {num_podcasts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ede7effc-6659-410a-8422-3e304363eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, List\n",
    "\n",
    "\n",
    "def sliding_window(\n",
    "        seq: Iterable[Any],\n",
    "        size: int,\n",
    "        step: int\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create overlapping chunks from a sequence using a sliding window approach.\n",
    "\n",
    "    Args:\n",
    "        seq: The input sequence (string or list) to be chunked.\n",
    "        size (int): The size of each chunk/window.\n",
    "        step (int): The step size between consecutive windows.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing:\n",
    "            - 'start': The starting position of the chunk in the original sequence\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If size or step are not positive integers.\n",
    "\n",
    "    Example:\n",
    "        >>> sliding_window(\"hello world\", size=5, step=3)\n",
    "        [{'start': 0, 'content': 'hello'}, {'start': 3, 'content': 'lo wo'}]\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        batch = seq[i:i+size]\n",
    "        result.append({'start': i, 'content': batch})\n",
    "        if i + size > n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def chunk_documents(\n",
    "        documents: Iterable[Dict[str, str]],\n",
    "        size: int = 2000,\n",
    "        step: int = 1000,\n",
    "        content_field_name: str = 'content'\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Split a collection of documents into smaller chunks using sliding windows.\n",
    "\n",
    "    Takes documents and breaks their content into overlapping chunks while preserving\n",
    "    all other document metadata (filename, etc.) in each chunk.\n",
    "\n",
    "    Args:\n",
    "        documents: An iterable of document dictionaries. Each document must have a content field.\n",
    "        size (int, optional): The maximum size of each chunk. Defaults to 2000.\n",
    "        step (int, optional): The step size between chunks. Defaults to 1000.\n",
    "        content_field_name (str, optional): The name of the field containing document content.\n",
    "                                          Defaults to 'content'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of chunk dictionaries. Each chunk contains:\n",
    "            - All original document fields except the content field\n",
    "            - 'start': Starting position of the chunk in original content\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Example:\n",
    "        >>> documents = [{'content': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, size=100, step=50)\n",
    "        >>> # Or with custom content field:\n",
    "        >>> documents = [{'text': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, content_field_name='text')\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_copy = doc.copy()\n",
    "        doc_content = doc_copy.pop(content_field_name)\n",
    "        chunks = sliding_window(doc_content, size=size, step=step)\n",
    "        for chunk in chunks:\n",
    "            chunk.update(doc_copy)\n",
    "        results.extend(chunks)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cdce516c-5258-47ba-9122-9b98395f9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_documents(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a41992b8-0af5-4f79-8d32-103026024c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6243c92e-16c2-4bf1-83c6-c24cee8d2c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x255a3e070e0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "        text_fields=[\"content\", \"filename\", \"title\", \"description\"],\n",
    ")\n",
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3ee4ad56-1c12-4c68-9f2e-f62e514f6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chunks = len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a93a87c8-bb39-4aa0-86e0-c5d3e0661ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 162\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of chunks: {num_chunks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "220d6f35-84df-426e-89f1-62bdf51ac21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = index.search('how do I use this to find a job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3106b2c0-711f-446b-909f-bd6797d03581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    return index.search(\n",
    "        query=query,\n",
    "        num_results=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37e075d6-f443-45c3-a751-200ae177edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'how do I use this to find a job'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73ce8fb1-82c4-47ba-b932-2d8b802ed8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You're an assistant that helps with the documentation.\n",
    "Answer the QUESTION based on the CONTEXT from the search engine of our documentation.\n",
    "\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "When answering the question, provide the reference to the file with the source.\n",
    "Use the filename field for that. The repo url is: https://github.com/evidentlyai/docs/\n",
    "Include code examples when relevant. \n",
    "If the question is discussed in multiple documents, cite all of them.\n",
    "\n",
    "Don't use markdown or any formatting in the output.\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0850a9d4-1bf0-4788-9d04-930fee2765f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f461ea0d-720f-40c8-92aa-9144d441cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, search_results):\n",
    "    context = json.dumps(search_results)\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    ).strip()\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea33633c-2254-4b1a-befb-fbe869bf4c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm(user_prompt, instructions=None, model=\"gpt-4o-mini\"):\n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ae207a4-8a3f-4f5f-83e6-96517bdfc508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, top_k=3):  \n",
    "    search_results = search(query)\n",
    "    search_results = search_results[:top_k]  \n",
    "    prompt = build_prompt(query, search_results)\n",
    "    response = llm(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd5d6d63-b65d-4419-bf0d-442c62b7f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag('how do I use this to find a job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73fb2142-b645-44ac-a635-f350845f2fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find a job using the insights from the provided content, follow these steps:\n",
      "\n",
      "### 1. **Leverage Professional Networking:**\n",
      "   - **Create or Update Your LinkedIn Profile:** Ensure it's professional and showcases your skills and experiences.\n",
      "   - **Connect with Relevant People:** Reach out to individuals in your desired field. Use personalized messages to initiate conversations.\n",
      "   - **Informational Interviews:** Schedule brief chats with industry professionals to gain insights into their roles and companies.\n",
      "\n",
      "### 2. **Identify Your Career Goals:**\n",
      "   - **Self-Assessment:** Reflect on what you want from your career. Consider your interests, strengths, and values.\n",
      "   - **Career Research:** Investigate potential job roles and the skills required. Online tools or assessments like the O*Net Interest Profiler can help clarify your interests.\n",
      "\n",
      "### 3. **Tailor Your Job Search:**\n",
      "   - **Research Job Descriptions:** Look for roles that align with your skills and preferences. Pay attention to company culture and diversity initiatives.\n",
      "   - **Focus on Job Requirements:** Identify must-have skills versus nice-to-haves. Use this to refine your applications.\n",
      "\n",
      "### 4. **Develop Your Skills:**\n",
      "   - **Upskill as Needed:** Take courses relevant to your target roles (e.g., coding, data analytics). Share your progress on platforms like LinkedIn.\n",
      "   - **Projects:** Engage in relevant projects (personal or freelance) to build your portfolio, showcasing your skills to prospective employers.\n",
      "\n",
      "### 5. **Public Sharing for Visibility:**\n",
      "   - **Document Your Learning Journey:** Write posts or articles about your projects or learnings. This increases your visibility and showcases your expertise.\n",
      "   - **Create Engaging Content:** Share insights or challenges you face, positioning yourself as a knowledgeable candidate in your field.\n",
      "\n",
      "### 6. **Apply Strategically:**\n",
      "   - **Don’t Limit Yourself:** Apply to a range of roles but ensure they align with your career aspirations.\n",
      "   - **Prepare for Interviews:** Practice common interview questions, especially those relating to your specific experiences and how they relate to the roles you’re applying for.\n",
      "\n",
      "### 7. **Follow-Up and Feedback:**\n",
      "   - **Ask for Feedback:** If you don’t get a job, politely request feedback on your interview to identify areas for improvement.\n",
      "   - **Keep Networking:** Continue to build relationships within your industry, even if you are currently employed.\n",
      "\n",
      "### 8. **Utilize Job Platforms and Resources:**\n",
      "   - Use job search engines like Upwork, Indeed, Glassdoor, and company websites to find open positions.\n",
      "   - Consider platforms that focus on your industry (e.g., data science, technology).\n",
      "\n",
      "### 9. **Be Persistent and Resilient:**\n",
      "   - Job searching can be long and challenging; maintain a positive outlook and adjust your strategies as needed.\n",
      "   - Acknowledge setbacks but use them as learning experiences.\n",
      "\n",
      "### Conclusion\n",
      "Use these actionable strategies to enhance your job search. Combine networking, skill-building, and self-awareness to position yourself as a strong candidate in your desired field.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4bcba5fb-fa59-4795-a320-4d4fe830446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how do I use this to find a job'\n",
    "search_results = search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dfe54124-88f8-41ea-ab82-8434413e80d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top result title: Accelerating The Job Hunt for The Perfect Job in Tech\n",
      "Filename: _podcast/s17e06-accelerating-job-hunt-for-perfect-job-in-tech.md\n",
      "Score: N/A\n"
     ]
    }
   ],
   "source": [
    "top_result = search_results[0]\n",
    "print(f\"Top result title: {top_result.get('title', 'N/A')}\")\n",
    "print(f\"Filename: {top_result.get('filename', 'N/A')}\")\n",
    "print(f\"Score: {top_result.get('score', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e956c-58ab-4d19-8131-f1decbf3eff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
