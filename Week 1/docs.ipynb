{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "923fdf38-8a24-4760-a590-e467b8ce55e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b97e5709-4afd-44c7-b4db-0a9135976274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5065d0bf-a3c3-481f-b507-b3fbd4b18060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e413923-1b63-43f6-af87-9847bfc9781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "089d6b8a-292f-4343-bc5c-e16165e26967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import Iterable, Callable\n",
    "import zipfile\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RawRepositoryFile:\n",
    "    filename: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class GithubRepositoryDataReader:\n",
    "    \"\"\"\n",
    "    Downloads and parses markdown and code files from a GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                repo_owner: str,\n",
    "                repo_name: str,\n",
    "                allowed_extensions: Iterable[str] | None = None,\n",
    "                filename_filter: Callable[[str], bool] | None = None\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the GitHub repository data reader.\n",
    "        \n",
    "        Args:\n",
    "            repo_owner: The owner/organization of the GitHub repository\n",
    "            repo_name: The name of the GitHub repository\n",
    "            allowed_extensions: Optional set of file extensions to include\n",
    "                    (e.g., {\"md\", \"py\"}). If not provided, all file types are included\n",
    "            filename_filter: Optional callable to filter files by their path\n",
    "        \"\"\"\n",
    "        prefix = \"https://codeload.github.com\"\n",
    "        self.url = (\n",
    "            f\"{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main\"\n",
    "        )\n",
    "\n",
    "        if allowed_extensions is not None:\n",
    "            self.allowed_extensions = {ext.lower() for ext in allowed_extensions}\n",
    "\n",
    "        if filename_filter is None:\n",
    "            self.filename_filter = lambda filepath: True\n",
    "        else:\n",
    "            self.filename_filter = filename_filter\n",
    "\n",
    "    def read(self) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Download and extract files from the GitHub repository.\n",
    "        \n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If the repository download fails\n",
    "        \"\"\"\n",
    "        resp = requests.get(self.url)\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "        zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "        repository_data = self._extract_files(zf)\n",
    "        zf.close()\n",
    "\n",
    "        return repository_data\n",
    "\n",
    "    def _extract_files(self, zf: zipfile.ZipFile) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Extract and process files from the zip archive.\n",
    "        \n",
    "        Args:\n",
    "            zf: ZipFile object containing the repository data\n",
    "\n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "        \"\"\"\n",
    "        data = []\n",
    "\n",
    "        for file_info in zf.infolist():\n",
    "            filepath = self._normalize_filepath(file_info.filename)\n",
    "\n",
    "            if self._should_skip_file(filepath):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                    if content is not None:\n",
    "                        content = content.strip()\n",
    "\n",
    "                    file = RawRepositoryFile(\n",
    "                        filename=filepath,\n",
    "                        content=content\n",
    "                    )\n",
    "                    data.append(file)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_info.filename}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _should_skip_file(self, filepath: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determine whether a file should be skipped during processing.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to check\n",
    "            \n",
    "        Returns:\n",
    "            True if the file should be skipped, False otherwise\n",
    "        \"\"\"\n",
    "        filepath = filepath.lower()\n",
    "\n",
    "        # directory\n",
    "        if filepath.endswith(\"/\"):\n",
    "            return True\n",
    "\n",
    "        # hidden file\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        if filename.startswith(\".\"):\n",
    "            return True\n",
    "\n",
    "        if self.allowed_extensions:\n",
    "            ext = self._get_extension(filepath)\n",
    "            if ext not in self.allowed_extensions:\n",
    "                return True\n",
    "\n",
    "        if not self.filename_filter(filepath):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _get_extension(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the file extension from a filepath.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to extract extension from\n",
    "            \n",
    "        Returns:\n",
    "            The file extension (without dot) or empty string if no extension\n",
    "        \"\"\"\n",
    "        filename = filepath.lower().split(\"/\")[-1]\n",
    "        if \".\" in filename:\n",
    "            return filename.rsplit(\".\", maxsplit=1)[-1]\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _normalize_filepath(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes the top-level directory from the file path inside the zip archive.\n",
    "        'repo-main/path/to/file.py' -> 'path/to/file.py'\n",
    "        \n",
    "        Args:\n",
    "            filepath: The original filepath from the zip archive\n",
    "            \n",
    "        Returns:\n",
    "            The normalized filepath with top-level directory removed\n",
    "        \"\"\"\n",
    "        parts = filepath.split(\"/\", maxsplit=1)\n",
    "        if len(parts) > 1:\n",
    "            return parts[1]\n",
    "        else:\n",
    "            return parts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b9d588e-4de8-4de6-8b00-5daa00e5e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_github_data():\n",
    "\n",
    "    repo_owner = 'evidentlyai'\n",
    "    repo_name = 'docs'\n",
    "\n",
    "    allowed_extensions = {\"md\", \"mdx\"}\n",
    "\n",
    "\n",
    "    reader = GithubRepositoryDataReader(\n",
    "        repo_owner,\n",
    "        repo_name,\n",
    "        allowed_extensions=allowed_extensions,\n",
    "    )\n",
    "    \n",
    "    return reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f36bf1e-0597-4b3f-adaf-7b1558a1c22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m153 packages\u001b[0m \u001b[2min 1.40s\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 101ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 49ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-frontmatter\u001b[0m\u001b[2m==1.1.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add python-frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "978c53c4-c9e7-47cf-ac11-32dbbbe8fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_data = read_github_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6762f62-26b7-4d37-a23f-94426a2af50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: \"Evidently and GitHub actions\"\n",
      "description: \"Testing LLM outputs as part of the CI/CD flow.\"\n",
      "---\n",
      "\n",
      "You can use Evidently together with GitHub Actions to automatically test the outputs of your LLM agent or application - as part of every code push or pull request.\n",
      "\n",
      "## How the integration work:\n",
      "\n",
      "- You define a test dataset of inputs (e.g. test prompts with or without reference answers). You can store it as a file, or save the dataset at Evidently Cloud callable by Dataset ID.\n",
      "- Run your LLM system or agent against those inputs inside CI.\n",
      "- Evidently automatically evaluates the outputs using the user-specified config (which defines the Evidently descriptors, tests and Report composition), including methods like:\n",
      "  - LLM judges (e.g., tone, helpfulness, correctness)\n",
      "  - Custom Python functions\n",
      "  - Dataset-level metrics like classification quality\n",
      "- If any test fails, the CI job fails.\n",
      "- You get a detailed test report with pass/fail status and metrics.\n",
      "\n",
      "![](/images/examples/github_actions.gif)\n",
      "\n",
      "Results are stored locally or pushed to Evidently Cloud for deeper review and tracking.\n",
      "\n",
      "The final result is CI-native testing for your LLM behavior - so you can safely tweak prompts, models, or logic without breaking things silently.\n",
      "\n",
      "## Code example and tutorial\n",
      "\n",
      "ðŸ‘‰ Check the full tutorial and example repo: https://github.com/evidentlyai/evidently-ci-example\n",
      "\n",
      "Action is also available on GitHub Marketplace: https://github.com/marketplace/actions/run-evidently-report\n"
     ]
    }
   ],
   "source": [
    "print(github_data[40].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a70f667c-33c0-4fee-a14d-e687c8a041dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import frontmatter\n",
    "\n",
    "def parse_data(data_raw):\n",
    "    data_parsed = []\n",
    "    for f in data_raw:\n",
    "        post = frontmatter.loads(f.content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = f.filename\n",
    "        data_parsed.append(data)\n",
    "\n",
    "    return data_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b08765f3-2c5f-4866-b395-3e8fc2c98612",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = parse_data(github_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eba881ee-2b1a-4c10-8bb4-452f3e0eee62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Evidently and GitHub actions',\n",
       " 'description': 'Testing LLM outputs as part of the CI/CD flow.',\n",
       " 'content': 'You can use Evidently together with GitHub Actions to automatically test the outputs of your LLM agent or application - as part of every code push or pull request.\\n\\n## How the integration work:\\n\\n- You define a test dataset of inputs (e.g. test prompts with or without reference answers). You can store it as a file, or save the dataset at Evidently Cloud callable by Dataset ID.\\n- Run your LLM system or agent against those inputs inside CI.\\n- Evidently automatically evaluates the outputs using the user-specified config (which defines the Evidently descriptors, tests and Report composition), including methods like:\\n  - LLM judges (e.g., tone, helpfulness, correctness)\\n  - Custom Python functions\\n  - Dataset-level metrics like classification quality\\n- If any test fails, the CI job fails.\\n- You get a detailed test report with pass/fail status and metrics.\\n\\n![](/images/examples/github_actions.gif)\\n\\nResults are stored locally or pushed to Evidently Cloud for deeper review and tracking.\\n\\nThe final result is CI-native testing for your LLM behavior - so you can safely tweak prompts, models, or logic without breaking things silently.\\n\\n## Code example and tutorial\\n\\nðŸ‘‰ Check the full tutorial and example repo: https://github.com/evidentlyai/evidently-ci-example\\n\\nAction is also available on GitHub Marketplace: https://github.com/marketplace/actions/run-evidently-report',\n",
       " 'filename': 'examples/GitHub_actions.mdx'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_data[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2934d7fd-e395-4e14-9753-06ec705d4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, List\n",
    "\n",
    "\n",
    "def sliding_window(\n",
    "        seq: Iterable[Any],\n",
    "        size: int,\n",
    "        step: int\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create overlapping chunks from a sequence using a sliding window approach.\n",
    "\n",
    "    Args:\n",
    "        seq: The input sequence (string or list) to be chunked.\n",
    "        size (int): The size of each chunk/window.\n",
    "        step (int): The step size between consecutive windows.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing:\n",
    "            - 'start': The starting position of the chunk in the original sequence\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If size or step are not positive integers.\n",
    "\n",
    "    Example:\n",
    "        >>> sliding_window(\"hello world\", size=5, step=3)\n",
    "        [{'start': 0, 'content': 'hello'}, {'start': 3, 'content': 'lo wo'}]\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        batch = seq[i:i+size]\n",
    "        result.append({'start': i, 'content': batch})\n",
    "        if i + size > n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def chunk_documents(\n",
    "        documents: Iterable[Dict[str, str]],\n",
    "        size: int = 2000,\n",
    "        step: int = 1000,\n",
    "        content_field_name: str = 'content'\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Split a collection of documents into smaller chunks using sliding windows.\n",
    "\n",
    "    Takes documents and breaks their content into overlapping chunks while preserving\n",
    "    all other document metadata (filename, etc.) in each chunk.\n",
    "\n",
    "    Args:\n",
    "        documents: An iterable of document dictionaries. Each document must have a content field.\n",
    "        size (int, optional): The maximum size of each chunk. Defaults to 2000.\n",
    "        step (int, optional): The step size between chunks. Defaults to 1000.\n",
    "        content_field_name (str, optional): The name of the field containing document content.\n",
    "                                          Defaults to 'content'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of chunk dictionaries. Each chunk contains:\n",
    "            - All original document fields except the content field\n",
    "            - 'start': Starting position of the chunk in original content\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Example:\n",
    "        >>> documents = [{'content': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, size=100, step=50)\n",
    "        >>> # Or with custom content field:\n",
    "        >>> documents = [{'text': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, content_field_name='text')\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_copy = doc.copy()\n",
    "        doc_content = doc_copy.pop(content_field_name)\n",
    "        chunks = sliding_window(doc_content, size=size, step=step)\n",
    "        for chunk in chunks:\n",
    "            chunk.update(doc_copy)\n",
    "        results.extend(chunks)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6085693-744c-4d20-a29e-f36a8594906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_documents(parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85c8b7a3-2ddd-4c72-b5f2-e4235795ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a212d05e-7caa-4c55-acf3-b0565a919570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x2812c025400>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "        text_fields=[\"content\", \"filename\", \"title\", \"description\"],\n",
    ")\n",
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80e4720b-0220-48b1-b0ba-21230c7edd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = index.search('how do I use llm-as-a-judge for evals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdc8c49c-b6fc-40aa-bc66-10f331b76d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    return index.search(\n",
    "        query=query,\n",
    "        num_results=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01aba13c-a8aa-41cb-bfcf-a61f3400ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'how do I use llm-as-a-judge for evals'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89f94e0e-8c7c-4be0-9cdb-b782f9e3b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You're an assistant that helps with the documentation.\n",
    "Answer the QUESTION based on the CONTEXT from the search engine of our documentation.\n",
    "\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "When answering the question, provide the reference to the file with the source.\n",
    "Use the filename field for that. The repo url is: https://github.com/evidentlyai/docs/\n",
    "Include code examples when relevant. \n",
    "If the question is discussed in multiple documents, cite all of them.\n",
    "\n",
    "Don't use markdown or any formatting in the output.\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73142bea-6b7f-4071-83d3-3101a78deedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a3a7ddd-23ba-4fc7-850e-287b785e4892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, search_results):\n",
    "    context = json.dumps(search_results)\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    ).strip()\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "913510ad-a2da-46df-bbda-a57537e10dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm(user_prompt, instructions=None, model=\"gpt-4o-mini\"):\n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2d042be-ddb5-4117-a31a-089cc1361118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    response = llm(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1986df11-2ae7-41da-b25d-3c8c4a6e4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag('How can I build an eval report with llm as a judge?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "240ae298-2ce9-4f3a-9735-6c6704e90d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building an evaluation report with a Large Language Model (LLM) as a judge involves several steps. Below is a comprehensive guide to help you through the process.\n",
      "\n",
      "### Step 1: Setup and Installation\n",
      "\n",
      "1. **Install the Necessary Libraries:**\n",
      "   Ensure you have the required libraries, especially Evidently, installed in your Python environment.\n",
      "   ```bash\n",
      "   pip install evidently\n",
      "   ```\n",
      "\n",
      "2. **Import Required Modules:**\n",
      "   Import the necessary modules to create your dataset and reports.\n",
      "   ```python\n",
      "   import pandas as pd\n",
      "   import numpy as np\n",
      "   from evidently import Dataset, DataDefinition, Report\n",
      "   from evidently.metrics import *\n",
      "   from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "   ```\n",
      "\n",
      "3. **Set Your OpenAI API Key:**\n",
      "   Ensure that your OpenAI API key is set as an environment variable.\n",
      "   ```python\n",
      "   import os\n",
      "   os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "   ```\n",
      "\n",
      "### Step 2: Create Your Dataset\n",
      "\n",
      "1. **Create a Toy Dataset:**\n",
      "   Generate a DataFrame containing questions, target responses, new responses, and manual labels.\n",
      "   ```python\n",
      "   data = [\n",
      "       [\"Hi there, how do I reset my password?\", \"To reset your password, click on 'Forgot Password'...\", \"To change your password, select 'Forgot Password'...\", \"incorrect\", \"adds new information\"],\n",
      "       # Add more rows as needed\n",
      "   ]\n",
      "   golden_dataset = pd.DataFrame(data, columns=[\"question\", \"target_response\", \"new_response\", \"label\", \"explanation\"])\n",
      "   ```\n",
      "\n",
      "2. **Define DataStructure:**\n",
      "   Create a Dataset object with appropriate definitions.\n",
      "   ```python\n",
      "   definition = DataDefinition(\n",
      "       text_columns=[\"question\", \"target_response\", \"new_response\"],\n",
      "       categorical_columns=[\"label\"]\n",
      "   )\n",
      "   eval_dataset = Dataset.from_pandas(golden_dataset, data_definition=definition)\n",
      "   ```\n",
      "\n",
      "### Step 3: Set Up the LLM Judge\n",
      "\n",
      "1. **Define the Prompt Template:**\n",
      "   Create a binary classification prompt template for evaluating correctness.\n",
      "   ```python\n",
      "   correctness = BinaryClassificationPromptTemplate(\n",
      "       criteria=\"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details...\"\"\",\n",
      "       target_category=\"incorrect\",\n",
      "       non_target_category=\"correct\",\n",
      "       uncertainty=\"unknown\",\n",
      "       include_reasoning=True,\n",
      "       pre_messages=[(\"system\", \"You are an expert evaluator.\")]\n",
      "   )\n",
      "   ```\n",
      "\n",
      "2. **Add the Evaluator to the Dataset:**\n",
      "   Integrate the LLM evaluation into your dataset.\n",
      "   ```python\n",
      "   eval_dataset.add_descriptors(descriptors=[\n",
      "       LLMEval(\"new_response\", template=correctness, provider=\"openai\", model=\"gpt-4o-mini\", alias=\"Correctness\")\n",
      "   ])\n",
      "   ```\n",
      "\n",
      "### Step 4: Generate the Report\n",
      "\n",
      "1. **Run the Evaluation:**\n",
      "   Generate an assessment report based on the criteria defined previously.\n",
      "   ```python\n",
      "   report = Report([TextEvals()])\n",
      "   my_eval = report.run(eval_dataset, None)\n",
      "   ```\n",
      "\n",
      "2. **Preview the Results:**\n",
      "   You can view the results in a DataFrame format.\n",
      "   ```python\n",
      "   eval_dataset.as_dataframe()\n",
      "   ```\n",
      "\n",
      "### Step 5: Analyze the Results\n",
      "\n",
      "1. **Summarize the Evaluation:**\n",
      "   Extract metrics and visualizations from the report.\n",
      "   ```python\n",
      "   print(my_eval.as_dict())  # or my_eval.json() for JSON export\n",
      "   ```\n",
      "\n",
      "2. **Act on the Results:**\n",
      "   Adjust your judging criteria or the prompt based on the results obtained and iterate on your evaluation process.\n",
      "\n",
      "### Additional Notes\n",
      "\n",
      "- **Iterate and Tune:** Use the generated report to refine your LLM prompt or evaluate other metrics.\n",
      "- **Cloud Integration:** Consider integrating with the Evidently Cloud for advanced features and storage of your evaluations.\n",
      "- **Explore Other LLMs:** Experiment with different LLM models to see which one performs better for your use case.\n",
      "\n",
      "This guide provides a structured approach to building an evaluation report utilizing an LLM as a judge, and it can be adapted based on specific requirements or changes in your datasets.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a09beb0-c20d-41b1-aea3-0196f9d45ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
