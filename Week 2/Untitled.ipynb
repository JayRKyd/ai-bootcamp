{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5358f664-3fe6-4c90-9fae-d2e983df3471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "257edce1-81d7-4b4f-beee-cc462c3237a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "164b600d-251e-46a2-8cfd-aa7148c4b4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5841e5f-1e68-4687-9c7b-1d8e6fdad4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb636622-db90-4c54-9416-4b5867590007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4266e5-12cb-4b4c-b317-3ad56c684183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0edab3b7-6182-4320-a76c-f839a75b1ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x2b6030856a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import AppendableIndex\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25cf999a-9de4-4e6c-bcf4-e6e026bfd26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6adcc90a-b7f7-41c7-917a-3864df6ab25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_call(call):\n",
    "    args = json.loads(call.arguments)\n",
    "    f_name = call.name\n",
    "    f = globals()[f_name]\n",
    "    result = f(**args)\n",
    "    result_json = json.dumps(result, indent=2)\n",
    "    return {\n",
    "        \"type\": \"function_call_output\",\n",
    "        \"call_id\": call.call_id,\n",
    "        \"output\": result_json,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b788f49-0fc0-4273-8ca6-fc26022b1f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG - Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b5a8a75-1e8c-43c0-bede-b884a2e2c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "If you want to look up the answer, explain why before making the call. Use as many \n",
    "keywords from the user question as possible when making first requests.\n",
    "\n",
    "Make multiple searches. Try to expand your search by using new keywords based on the results you\n",
    "get from the search.\n",
    "\n",
    "At the end, make a clarifying question based on what you presented and ask if there are \n",
    "other areas that the user wants to explore.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e4b2de1-43af-450f-a00b-753aaf769be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I just discovered the course, can I join it now?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32480eb2-afed-4abd-b05b-d25a333ade62",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32985d90-968f-4e88-a75d-52f7bb7bbde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseFunctionToolCall(arguments='{\"query\":\"join course late enrollment\"}', call_id='call_EmiHCApHVg8EFYqpbOkkBZ4f', name='search', type='function_call', id='fc_04252df827b592eb0068f6d9ed3b7481a19a176ca86dfa3ea1', status='completed')\n",
      "ResponseFunctionToolCall(arguments='{\"query\":\"join course after start date late enrollment eligibility\"}', call_id='call_xlpbztcHHJlMvlaB4dcXYAEu', name='search', type='function_call', id='fc_04252df827b592eb0068f6d9ef0d2881a19a630c9bb1355c36', status='completed')\n",
      "Yes, you can still join the course even if you've discovered it after it has started. You are eligible to submit homework without registering, although it's encouraged to do so for better tracking. There are deadlines for final projects that you'll need to keep in mind, so it's best not to procrastinate.\n",
      "\n",
      "If you have any specific areas you are curious about, or if there’s anything else you'd like to know regarding course materials, project submissions, or deadlines, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=chat_messages,\n",
    "        tools=[search_tool]\n",
    "    )\n",
    "    \n",
    "    chat_messages.extend(response.output)\n",
    "\n",
    "    has_function_calls = False\n",
    "    \n",
    "    for entry in response.output:\n",
    "        if entry.type == 'message':\n",
    "            print(entry.content[0].text)\n",
    "        if entry.type == 'function_call':\n",
    "            print(entry)\n",
    "            result = make_call(entry)\n",
    "            chat_messages.append(result)\n",
    "            has_function_calls = True\n",
    "\n",
    "    if has_function_calls == False:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17d9086c-c6f5-4658-8b1e-4a86df6ebace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.llm import OpenAIClient\n",
    "from toyaikit.tools import Tools\n",
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import OpenAIResponsesRunner\n",
    "from toyaikit.chat.runners import DisplayingRunnerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42e194c6-5b48-434f-b29c-cb1bb40eaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_tools = Tools()\n",
    "agent_tools.add_tool(search, search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d7efaa0-cf06-4399-b831-b17e6d7f6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_interface = IPythonChatInterface()\n",
    "\n",
    "runner = OpenAIResponsesRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=OpenAIClient()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99fb96d0-e7e9-4dff-9b0e-637e0d162426",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = DisplayingRunnerCallback(chat_interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88b1451d-94fd-429f-a089-a36210a3644f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"install Kafka\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"install Kafka\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_Fuw2iwBWXu3cm7tKb5j5RbOH', 'output': '[\\n  {\\n    \"text\": \"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\\\nfastavro: pip install fastavro\\\\nAbhirup Ghosh\\\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\\\nSolution:\\\\nIn build.gradle file, I added the following at the end:\\\\nshadowJar {\\\\narchiveBaseName = \\\\\"java-kafka-rides\\\\\"\\\\narchiveClassifier = \\'\\'\\\\n}\\\\nAnd then in the command line ran \\\\u2018gradle shadowjar\\\\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\\\nKafka Python Videos - Rides.csv\\\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Kafka- python videos have low audio and hard to follow up\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"If you have this error, it most likely that your kafka broker docker container is not working.\\\\nUse docker ps to confirm\\\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"For example, when running JsonConsumer.java, got:\\\\nConsuming form kafka started\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nOr when running JsonProducer.java, got:\\\\nException in thread \\\\\"main\\\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\\\nSolution:\\\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"Kafka installation guide\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"Kafka installation guide\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_cQDcv2KW668gUOFXPeXEyVER', 'output': '[\\n  {\\n    \"text\": \"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\\\nKafka Python Videos - Rides.csv\\\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Kafka- python videos have low audio and hard to follow up\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"If you have this error, it most likely that your kafka broker docker container is not working.\\\\nUse docker ps to confirm\\\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\\\nfastavro: pip install fastavro\\\\nAbhirup Ghosh\\\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\\\nSolution:\\\\nIn build.gradle file, I added the following at the end:\\\\nshadowJar {\\\\narchiveBaseName = \\\\\"java-kafka-rides\\\\\"\\\\narchiveClassifier = \\'\\'\\\\n}\\\\nAnd then in the command line ran \\\\u2018gradle shadowjar\\\\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"For example, when running JsonConsumer.java, got:\\\\nConsuming form kafka started\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nOr when running JsonProducer.java, got:\\\\nException in thread \\\\\"main\\\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\\\nSolution:\\\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To install Kafka, you have several options depending on your environment and programming needs. Here’s a general guide based on common methods:</p>\n",
       "<h3>1. <strong>Using Docker</strong>:</h3>\n",
       "<p>If you're familiar with Docker, the easiest way to set up Kafka is through Docker Compose. Create a <code>docker-compose.yml</code> file that specifies the Kafka and Zookeeper services. You can then run the following command:</p>\n",
       "<pre><code class=\"language-bash\">docker-compose up -d\n",
       "</code></pre>\n",
       "<h3>2. <strong>Manually on Linux/Mac</strong>:</h3>\n",
       "<ol>\n",
       "<li><p><strong>Download Kafka</strong>:</p>\n",
       "<ul>\n",
       "<li>Go to the <a href=\"https://kafka.apache.org/downloads\">Apache Kafka downloads page</a> and download the latest binary version.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Extract the Archive</strong>:</p>\n",
       "<pre><code class=\"language-bash\">tar -xzf kafka_2.12-&lt;version&gt;.tgz\n",
       "cd kafka_2.12-&lt;version&gt;\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Start Zookeeper</strong>:\n",
       "Kafka uses Zookeeper on the backend. Start it using:</p>\n",
       "<pre><code class=\"language-bash\">bin/zookeeper-server-start.sh config/zookeeper.properties\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Start Kafka</strong>:\n",
       "In another terminal window, start Kafka by running:</p>\n",
       "<pre><code class=\"language-bash\">bin/kafka-server-start.sh config/server.properties\n",
       "</code></pre>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>3. <strong>Using Conda or Pip (for Python users)</strong>:</h3>\n",
       "<p>If you are developing in Python, you can install the <code>confluent-kafka</code> package along with its dependencies using:</p>\n",
       "<pre><code class=\"language-bash\">pip install confluent-kafka\n",
       "</code></pre>\n",
       "<p>or</p>\n",
       "<pre><code class=\"language-bash\">conda install -c conda-forge python-confluent-kafka\n",
       "</code></pre>\n",
       "<h3>4. <strong>Using Gradle (for Java users)</strong>:</h3>\n",
       "<p>If you are developing with Java and using Gradle, ensure your <code>build.gradle</code> includes dependencies for Kafka. After making changes, run:</p>\n",
       "<pre><code class=\"language-bash\">gradle shadowjar\n",
       "</code></pre>\n",
       "<p>This will create a JAR file that you can then execute.</p>\n",
       "<h3>Troubleshooting Common Issues:</h3>\n",
       "<ul>\n",
       "<li>If you encounter <code>NoBrokersAvailable</code>, check if your Kafka broker is running with <code>docker ps</code> or equivalent commands.</li>\n",
       "</ul>\n",
       "<h3>Clarifying Question:</h3>\n",
       "<p>Would you like to know more about setting up specific configurations or using Kafka in a certain programming language, or do you have other areas you want to explore?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = runner.loop(prompt='how do I install kafka', callback=callback)\n",
    "messages = result.new_messages  # or result.messages - check the LoopResult attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b976f73-6c1a-4f89-921e-4482edf1d757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Great choice! Using Docker to set up Kafka simplifies the installation and configuration process. Here’s a step-by-step guide to install Kafka using Docker:</p>\n",
       "<h3>Step 1: Install Docker</h3>\n",
       "<p>Ensure you have Docker installed on your machine. You can download it from <a href=\"https://www.docker.com/get-started\">Docker's official website</a>.</p>\n",
       "<h3>Step 2: Create a <code>docker-compose.yml</code> File</h3>\n",
       "<ol>\n",
       "<li><p>Create a new directory for your Kafka project:</p>\n",
       "<pre><code class=\"language-bash\">mkdir kafka-docker\n",
       "cd kafka-docker\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p>Create a <code>docker-compose.yml</code> file in this directory with the following content:</p>\n",
       "<pre><code class=\"language-yaml\">version: '3'\n",
       "\n",
       "services:\n",
       "  zookeeper:\n",
       "    image: wurstmeister/zookeeper:3.4.6\n",
       "    ports:\n",
       "      - &quot;2181:2181&quot;\n",
       "\n",
       "  kafka:\n",
       "    image: wurstmeister/kafka:latest\n",
       "    ports:\n",
       "      - &quot;9092:9092&quot;\n",
       "    environment:\n",
       "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
       "      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9092,OUTSIDE://localhost:9094\n",
       "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT\n",
       "      KAFKA_LISTENERS: INSIDE://0.0.0.0:9092,OUTSIDE://0.0.0.0:9094\n",
       "    depends_on:\n",
       "      - zookeeper\n",
       "</code></pre>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>Step 3: Start the Services</h3>\n",
       "<p>Run the following command in your terminal:</p>\n",
       "<pre><code class=\"language-bash\">docker-compose up -d\n",
       "</code></pre>\n",
       "<p>This will start both Zookeeper and Kafka in the background.</p>\n",
       "<h3>Step 4: Verify the Setup</h3>\n",
       "<p>You can check if the containers are running by executing:</p>\n",
       "<pre><code class=\"language-bash\">docker ps\n",
       "</code></pre>\n",
       "<p>You should see both <code>zookeeper</code> and <code>kafka</code> containers listed.</p>\n",
       "<h3>Step 5: Interacting with Kafka</h3>\n",
       "<p>To create a topic and send/receive messages, you can use the Kafka command-line tools. Here are some commands you might find useful:</p>\n",
       "<p><strong>Create a Topic</strong>:</p>\n",
       "<pre><code class=\"language-bash\">docker exec -it &lt;kafka-container-id&gt; /bin/bash\n",
       "# Then within the bash shell, run:\n",
       "kafka-topics.sh --create --topic my-topic --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1\n",
       "</code></pre>\n",
       "<p><strong>List Topics</strong>:</p>\n",
       "<pre><code class=\"language-bash\">kafka-topics.sh --list --bootstrap-server kafka:9092\n",
       "</code></pre>\n",
       "<p><strong>Send Messages</strong>:</p>\n",
       "<pre><code class=\"language-bash\">kafka-console-producer.sh --topic my-topic --bootstrap-server kafka:9092\n",
       "</code></pre>\n",
       "<h3>Step 6: Stopping the Services</h3>\n",
       "<p>When you're done, you can stop the services with:</p>\n",
       "<pre><code class=\"language-bash\">docker-compose down\n",
       "</code></pre>\n",
       "<h3>Clarifying Question:</h3>\n",
       "<p>Is there anything specific you’d like to configure in your Kafka setup? Or do you have other questions regarding Docker or Kafka?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_messages = runner.loop(\n",
    "    prompt='I want to use docker',\n",
    "    previous_messages=messages,  # Use the messages list directly\n",
    "    callback=callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35ca270b-13c0-43a0-8bf0-21137605625b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: join course late enrollment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"join course late enrollment\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"join course late enrollment\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_RyzamGrNiBaUQim4JyZijDjh', 'output': '[\\n  {\\n    \"text\": \"No, late submissions are not allowed. But if the form is still not closed and it\\\\u2019s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\\\\nOlder news:[source1] [source2]\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Homework - Are late submissions of homework allowed?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Yes, even if you don\\'t register, you\\'re still eligible to submit the homeworks.\\\\nBe aware, however, that there will be deadlines for turning in the final projects. So don\\'t leave everything for the last minute.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Course - Can I still join the course after the start date?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"The purpose of this document is to capture frequently asked technical questions\\\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  \\\\u201cOffice Hours\\'\\' live.1\\\\nSubscribe to course public Google Calendar (it works from Desktop only).\\\\nRegister before the course starts using this link.\\\\nJoin the course Telegram channel with announcements.\\\\nDon\\\\u2019t forget to register in DataTalks.Club\\'s Slack and join the channel.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Course - When will the course start?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Course - Can I follow the course after it finishes?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"No, you can only get a certificate if you finish the course with a \\\\u201clive\\\\u201d cohort. We don\\'t award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Certificate - Can I follow the course in a self-paced mode and get a certificate?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"join course after the start date late...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"join course after the start date late enrollment\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_k8lIJ8JM7V5LmC8Ra95KJr8c', 'output': '[\\n  {\\n    \"text\": \"Yes, even if you don\\'t register, you\\'re still eligible to submit the homeworks.\\\\nBe aware, however, that there will be deadlines for turning in the final projects. So don\\'t leave everything for the last minute.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Course - Can I still join the course after the start date?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"No, late submissions are not allowed. But if the form is still not closed and it\\\\u2019s after the due date, you can still submit the homework. confirm your submission by the date-timestamp on the Course page.y\\\\nOlder news:[source1] [source2]\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Homework - Are late submissions of homework allowed?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"The purpose of this document is to capture frequently asked technical questions\\\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  \\\\u201cOffice Hours\\'\\' live.1\\\\nSubscribe to course public Google Calendar (it works from Desktop only).\\\\nRegister before the course starts using this link.\\\\nJoin the course Telegram channel with announcements.\\\\nDon\\\\u2019t forget to register in DataTalks.Club\\'s Slack and join the channel.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Course - When will the course start?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"You don\\'t need it. You\\'re accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Course - Can I follow the course after it finishes?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>It appears that you can still join the course even after it has officially started. According to the information retrieved:</p>\n",
       "<ol>\n",
       "<li><p><strong>Submitting Homework</strong>: You are eligible to submit homework assignments even if you haven't registered before the course start date. However, keep in mind that there will be deadlines for final projects, so it's important to manage your time effectively.</p>\n",
       "</li>\n",
       "<li><p><strong>Course Materials</strong>: All materials will remain accessible after the course ends, allowing you to learn at your own pace even after the official completion of the course.</p>\n",
       "</li>\n",
       "<li><p><strong>Registration</strong>: Registration is primarily for gauging interest, and you can start learning and submitting homework without being on a registered list.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>With this information, would you like to explore further about deadlines for final projects or anything else related to course participation?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: course deadlines final project assignments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"course deadlines final project assign...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"course deadlines final project assignments\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_RZJJMnwDDdtVhRnwlHgWDIRi', 'output': '[\\n  {\\n    \"text\": \"You can find the latest and up-to-date deadlines here: https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\\\\nAlso, take note of Announcements from @Au-Tomator for any extensions or other news. Or, the form may also show the updated deadline, if Instructor(s) has updated it.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Homework - What are homework and project deadlines?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Yes, you can use any tool you want for your project.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Can I use Airflow instead for my final project?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Each submitted project will be evaluated by 3 (three) randomly assigned students that have also submitted the project.\\\\nYou will also be responsible for grading the projects from 3 fellow students yourself. Please be aware that: not complying to this rule also implies you failing to achieve the Certificate at the end of the course.\\\\nThe final grade you get will be the median score of the grades you get from the peer reviewers.\\\\nAnd of course, the peer review criteria for evaluating or being evaluated must follow the guidelines defined here.\",\\n    \"section\": \"Project\",\\n    \"question\": \"How is my capstone project going to be evaluated?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Yes, even if you don\\'t register, you\\'re still eligible to submit the homeworks.\\\\nBe aware, however, that there will be deadlines for turning in the final projects. So don\\'t leave everything for the last minute.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Course - Can I still join the course after the start date?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Course - Can I follow the course after it finishes?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>For deadlines regarding final project assignments, you can find the latest and most up-to-date deadlines on a specific <a href=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml\">Google Spreadsheet</a>. It's also advisable to keep an eye on announcements from the course team for any extensions or additional updates regarding deadlines.</p>\n",
       "<p>It's crucial to note that:</p>\n",
       "<ul>\n",
       "<li>Each submitted project will be evaluated by three randomly assigned peers who have also submitted their projects.</li>\n",
       "<li>You must also evaluate the projects of three fellow students. Not complying with this requirement may affect your eligibility for the course certificate.</li>\n",
       "<li>The final grade will be the median score from your peer reviews.</li>\n",
       "</ul>\n",
       "<p>Would you like more information about the evaluation process or assistance with anything else related to your final project?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: end\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>If you have any more questions in the future or need assistance, feel free to reach out. Have a great day!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: end\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Understood! If you need anything later, don't hesitate to ask. Have a great day!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "messages = runner.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d83d1523-19b6-4a46-a6d9-c5f0047e9b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def search(query: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Search the FAQ database for entries matching the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): Search query text to look up in the course FAQ.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of search result entries, each containing relevant metadata.\n",
    "    \"\"\"\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "def add_entry(question: str, answer: str) -> None:\n",
    "    \"\"\"\n",
    "    Add a new entry to the FAQ database.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to be added to the FAQ database.\n",
    "        answer (str): The corresponding answer to the question.\n",
    "    \"\"\"\n",
    "    doc = {\n",
    "        'question': question,\n",
    "        'text': answer,\n",
    "        'section': 'user added',\n",
    "        'course': 'data-engineering-zoomcamp'\n",
    "    }\n",
    "    index.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c81bb4a-c2dc-4e5d-b8a7-7029c102981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_tools = Tools()\n",
    "agent_tools.add_tool(search)\n",
    "agent_tools.add_tool(add_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "128ea18a-710f-4774-82df-4a5d47289fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = OpenAIResponsesRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=OpenAIClient()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f449f880-0cd4-4f9a-a259-758d3a575d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: module 1 success tips\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"module 1 success tips\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"module 1 success tips\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_hCh3yALCMhdoHJdIxAhC084b', 'output': '[\\n  {\\n    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\\\nThe solution which worked for me(use following in jupyter notebook) :\\\\n!pip install findspark\\\\nimport findspark\\\\nfindspark.init()\\\\nThereafter , import pyspark and create spark contex<<t as usual\\\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\\\nFilter based on conditions based on multiple columns\\\\nfrom pyspark.sql.functions import col\\\\nnew_final.filter((new_final.a_zone==\\\\\"Murray Hill\\\\\") & (new_final.b_zone==\\\\\"Midwood\\\\\")).show()\\\\nKrishna Anand\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 322\\n  },\\n  {\\n    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\\\nexport PYTHONPATH=\\\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\\\u201d\\\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\\\"` appropriately.\\\\nAdditionally, you can check for the version of \\\\u2018py4j\\\\u2019 of the spark you\\\\u2019re using from here and update as mentioned above.\\\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 323\\n  },\\n  {\\n    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named \\'pytz\\'`\\\\nSolution:\\\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"DBT - Error: No module named \\'pytz\\' while setting up dbt with docker\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 299\\n  },\\n  {\\n    \"text\": \"Issue:\\\\ne\\\\u2026\\\\nSolution:\\\\npip install psycopg2-binary\\\\nIf you already have it, you might need to update it:\\\\npip install psycopg2-binary --upgrade\\\\nOther methods, if the above fails:\\\\nif you are getting the \\\\u201c ModuleNotFoundError: No module named \\'psycopg2\\' \\\\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\\\nFirst uninstall the psycopg package\\\\nThen update conda or pip\\\\nThen install psycopg again using pip.\\\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Postgres - ModuleNotFoundError: No module named \\'psycopg2\\'\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 112\\n  },\\n  {\\n    \"text\": \"create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \\\\\"TypeError: \\'module\\' object is not callable\\\\\"\\\\nSolution:\\\\nconn_string = \\\\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\\\"\\\\nengine = create_engine(conn_string)\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Python - SQLALchemy - TypeError \\'module\\' object is not callable\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 124\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"success tips for module 1\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"success tips for module 1\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_wgZ069INvv8YoFe51WCY7zFh', 'output': '[\\n  {\\n    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\\\nThe solution which worked for me(use following in jupyter notebook) :\\\\n!pip install findspark\\\\nimport findspark\\\\nfindspark.init()\\\\nThereafter , import pyspark and create spark contex<<t as usual\\\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\\\nFilter based on conditions based on multiple columns\\\\nfrom pyspark.sql.functions import col\\\\nnew_final.filter((new_final.a_zone==\\\\\"Murray Hill\\\\\") & (new_final.b_zone==\\\\\"Midwood\\\\\")).show()\\\\nKrishna Anand\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 322\\n  },\\n  {\\n    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\\\nexport PYTHONPATH=\\\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\\\u201d\\\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\\\"` appropriately.\\\\nAdditionally, you can check for the version of \\\\u2018py4j\\\\u2019 of the spark you\\\\u2019re using from here and update as mentioned above.\\\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 323\\n  },\\n  {\\n    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named \\'pytz\\'`\\\\nSolution:\\\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"DBT - Error: No module named \\'pytz\\' while setting up dbt with docker\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 299\\n  },\\n  {\\n    \"text\": \"Issue:\\\\ne\\\\u2026\\\\nSolution:\\\\npip install psycopg2-binary\\\\nIf you already have it, you might need to update it:\\\\npip install psycopg2-binary --upgrade\\\\nOther methods, if the above fails:\\\\nif you are getting the \\\\u201c ModuleNotFoundError: No module named \\'psycopg2\\' \\\\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\\\nFirst uninstall the psycopg package\\\\nThen update conda or pip\\\\nThen install psycopg again using pip.\\\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Postgres - ModuleNotFoundError: No module named \\'psycopg2\\'\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 112\\n  },\\n  {\\n    \"text\": \"create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \\\\\"TypeError: \\'module\\' object is not callable\\\\\"\\\\nSolution:\\\\nconn_string = \\\\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\\\"\\\\nengine = create_engine(conn_string)\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Python - SQLALchemy - TypeError \\'module\\' object is not callable\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 124\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>It seems there's no specific FAQ related to &quot;Module 1 success tips,&quot; but I can infer some useful strategies based on the material typically covered in such a module:</p>\n",
       "<ol>\n",
       "<li><p><strong>Understand the Basics</strong>: Focus on grasping foundational concepts related to Docker and Terraform. This understanding will help you avoid common errors.</p>\n",
       "</li>\n",
       "<li><p><strong>Hands-On Practice</strong>: Try to replicate examples provided in the course. This practical experience will reinforce what you've learned.</p>\n",
       "</li>\n",
       "<li><p><strong>Installation Verification</strong>: Ensure that all necessary tools (like Python packages, Docker, etc.) are correctly installed. Troubleshooting module not found errors often involves checking installations.</p>\n",
       "</li>\n",
       "<li><p><strong>Version Control</strong>: Pay attention to version compatibility issues with libraries and dependencies, particularly with tools like PostgreSQL and psycopg2.</p>\n",
       "</li>\n",
       "<li><p><strong>Use Resources</strong>: If you encounter challenges, don’t hesitate to leverage community forums, documentation, or seek help from your peers or instructors.</p>\n",
       "</li>\n",
       "<li><p><strong>Experiment and Explore</strong>: Beyond following instructions, explore the tools for personal projects. This exploration can deepen your understanding and retention of concepts.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Would you like to explore detailed strategies for troubleshooting specific issues or dive into other topics?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: module 1 strategies for success\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"strategies for success in module 1\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"strategies for success in module 1\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_tswxQeJvRKYm0MaB82ewhjgn', 'output': '[\\n  {\\n    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\\\nThe solution which worked for me(use following in jupyter notebook) :\\\\n!pip install findspark\\\\nimport findspark\\\\nfindspark.init()\\\\nThereafter , import pyspark and create spark contex<<t as usual\\\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\\\nFilter based on conditions based on multiple columns\\\\nfrom pyspark.sql.functions import col\\\\nnew_final.filter((new_final.a_zone==\\\\\"Murray Hill\\\\\") & (new_final.b_zone==\\\\\"Midwood\\\\\")).show()\\\\nKrishna Anand\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 322\\n  },\\n  {\\n    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\\\nexport PYTHONPATH=\\\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\\\u201d\\\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\\\"` appropriately.\\\\nAdditionally, you can check for the version of \\\\u2018py4j\\\\u2019 of the spark you\\\\u2019re using from here and update as mentioned above.\\\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 323\\n  },\\n  {\\n    \"text\": \"Issue:\\\\ne\\\\u2026\\\\nSolution:\\\\npip install psycopg2-binary\\\\nIf you already have it, you might need to update it:\\\\npip install psycopg2-binary --upgrade\\\\nOther methods, if the above fails:\\\\nif you are getting the \\\\u201c ModuleNotFoundError: No module named \\'psycopg2\\' \\\\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\\\nFirst uninstall the psycopg package\\\\nThen update conda or pip\\\\nThen install psycopg again using pip.\\\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Postgres - ModuleNotFoundError: No module named \\'psycopg2\\'\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 112\\n  },\\n  {\\n    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named \\'pytz\\'`\\\\nSolution:\\\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"DBT - Error: No module named \\'pytz\\' while setting up dbt with docker\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 299\\n  },\\n  {\\n    \"text\": \"create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \\\\\"TypeError: \\'module\\' object is not callable\\\\\"\\\\nSolution:\\\\nconn_string = \\\\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\\\"\\\\nengine = create_engine(conn_string)\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Python - SQLALchemy - TypeError \\'module\\' object is not callable\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 124\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Here are some strategies for success in Module 1, based on common challenges and best practices relevant to Docker and Terraform:</p>\n",
       "<ol>\n",
       "<li><p><strong>Master Installation Processes</strong>: Ensure you follow all installation steps precisely to avoid common errors. This includes verifying your setup for tools like Docker and Python packages.</p>\n",
       "</li>\n",
       "<li><p><strong>Troubleshoot Module Errors</strong>: If you encounter issues such as <code>ModuleNotFoundError</code>, verify that all required modules are installed. For example, running <code>pip install psycopg2-binary</code> can resolve issues related to PostgreSQL connections.</p>\n",
       "</li>\n",
       "<li><p><strong>Understand Docker Commands</strong>: Familiarize yourself with Docker commands like <code>docker-compose build</code> and <code>docker-compose run</code> to effectively manage your containerized applications.</p>\n",
       "</li>\n",
       "<li><p><strong>Engage with Documentation</strong>: Regularly refer to official documentation for Docker, SQLAlchemy, and Python libraries. This will help you understand the expected configurations and commands.</p>\n",
       "</li>\n",
       "<li><p><strong>Experiment with Code</strong>: Create small projects or modify existing examples to deepen your understanding. This hands-on practice can clarify how Docker and PostgreSQL integrate.</p>\n",
       "</li>\n",
       "<li><p><strong>Version Management</strong>: Keep track of library versions such as <code>psycopg2</code> and ensure compatibility with PostgreSQL; sometimes an upgrade is necessary to avoid errors.</p>\n",
       "</li>\n",
       "<li><p><strong>Seek Help</strong>: Utilize community forums or course discussion boards if you encounter persistent issues. Collaboration can often lead to quick resolutions.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Would you like more detailed help on a specific aspect or perhaps tips for another module?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: How do I do well in Module 1?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"how to do well in Module 1\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"how to do well in Module 1\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_aYrUKokNXz8WrOTuEtyybg9d', 'output': '[\\n  {\\n    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\\\nThe solution which worked for me(use following in jupyter notebook) :\\\\n!pip install findspark\\\\nimport findspark\\\\nfindspark.init()\\\\nThereafter , import pyspark and create spark contex<<t as usual\\\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\\\nFilter based on conditions based on multiple columns\\\\nfrom pyspark.sql.functions import col\\\\nnew_final.filter((new_final.a_zone==\\\\\"Murray Hill\\\\\") & (new_final.b_zone==\\\\\"Midwood\\\\\")).show()\\\\nKrishna Anand\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 322\\n  },\\n  {\\n    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\\\nexport PYTHONPATH=\\\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\\\u201d\\\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\\\"` appropriately.\\\\nAdditionally, you can check for the version of \\\\u2018py4j\\\\u2019 of the spark you\\\\u2019re using from here and update as mentioned above.\\\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 323\\n  },\\n  {\\n    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named \\'pytz\\'`\\\\nSolution:\\\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"DBT - Error: No module named \\'pytz\\' while setting up dbt with docker\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 299\\n  },\\n  {\\n    \"text\": \"create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \\\\\"TypeError: \\'module\\' object is not callable\\\\\"\\\\nSolution:\\\\nconn_string = \\\\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\\\"\\\\nengine = create_engine(conn_string)\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Python - SQLALchemy - TypeError \\'module\\' object is not callable\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 124\\n  },\\n  {\\n    \"text\": \"Error raised during the jupyter notebook\\\\u2019s cell execution:\\\\nengine = create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\').\\\\nSolution: Need to install Python module \\\\u201cpsycopg2\\\\u201d. Can be installed by Conda or pip.\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Python - SQLAlchemy - ModuleNotFoundError: No module named \\'psycopg2\\'.\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 125\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Here are some strategies to help you do well in Module 1, which typically focuses on Docker and Terraform:</p>\n",
       "<ol>\n",
       "<li><p><strong>Clear Understanding of Concepts</strong>: Make sure you grasp the foundational concepts of Docker and Terraform. Review lecture materials and relevant documentation to reinforce your understanding.</p>\n",
       "</li>\n",
       "<li><p><strong>Follow Installation Instructions Carefully</strong>: Ensure that you correctly install all necessary software and libraries, such as psycopg2 for PostgreSQL. Use <code>pip install psycopg2</code> or similar commands as needed.</p>\n",
       "</li>\n",
       "<li><p><strong>Hands-On Practice</strong>: Engage in practical exercises by setting up your own Docker containers and writing Terraform configurations. This allows you to apply what you’ve learned and solidify your skills.</p>\n",
       "</li>\n",
       "<li><p><strong>Debugging Skills</strong>: If you encounter errors like <code>ModuleNotFoundError</code>, investigate thoroughly. For instance, if you're missing a library, ensure you're using the correct command to install it (e.g., using <code>pip</code> or <code>conda</code>).</p>\n",
       "</li>\n",
       "<li><p><strong>Utilize Version Control</strong>: Keep track of software versions to avoid compatibility issues. This is especially important for libraries related to PostgreSQL and other tools used in the course.</p>\n",
       "</li>\n",
       "<li><p><strong>Engage with Peers</strong>: Collaborate and discuss with classmates or online forums. This interaction can provide different perspectives and insights that enhance your learning experience.</p>\n",
       "</li>\n",
       "<li><p><strong>Seek Help When Needed</strong>: Don’t hesitate to ask for assistance from your instructors or seek answers in community forums when you're stuck. Getting clarification can prevent further confusion.</p>\n",
       "</li>\n",
       "<li><p><strong>Review and Reflect</strong>: After each topic, take time to review what you've learned. Reflecting on your understanding can help reinforce concepts and prepare you for assessments.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Do you need more specific advice on any aspect of Module 1, or is there another area you'd like to explore?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "runner.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94fbfb18-890c-49c0-bc4d-aec8fd512eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Problem description\\nInfrastructure created in AWS with CD-Deploy Action needs to be destroyed\\nSolution description\\nFrom local:\\nterraform init -backend-config=\"key=mlops-zoomcamp-prod.tfstate\" --reconfigure\\nterraform destroy --var-file vars/prod.tfvars\\nAdded by Erick Calderin',\n",
       " 'section': 'Module 6: Best practices',\n",
       " 'question': 'How to destroy infrastructure created via GitHub Actions',\n",
       " 'course': 'mlops-zoomcamp'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.docs[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebf15c2f-852c-4c73-ae18-eca4f4699733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchTools:\n",
    "\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "\n",
    "    def search(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search the FAQ database for entries matching the given query.\n",
    "    \n",
    "        Args:\n",
    "            query (str): Search query text to look up in the course FAQ.\n",
    "    \n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of search result entries, each containing relevant metadata.\n",
    "        \"\"\"\n",
    "        boost = {'question': 3.0, 'section': 0.5}\n",
    "    \n",
    "        results = self.index.search(\n",
    "            query=query,\n",
    "            filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "            boost_dict=boost,\n",
    "            num_results=5,\n",
    "            output_ids=True\n",
    "        )\n",
    "    \n",
    "        return results\n",
    "\n",
    "    def add_entry(self, question: str, answer: str) -> None:\n",
    "        \"\"\"\n",
    "        Add a new entry to the FAQ database.\n",
    "    \n",
    "        Args:\n",
    "            question (str): The question to be added to the FAQ database.\n",
    "            answer (str): The corresponding answer to the question.\n",
    "        \"\"\"\n",
    "        doc = {\n",
    "            'question': question,\n",
    "            'text': answer,\n",
    "            'section': 'user added',\n",
    "            'course': 'data-engineering-zoomcamp'\n",
    "        }\n",
    "        self.index.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80207630-743e-4c3f-b244-dcc70d85b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tools = SearchTools(index)\n",
    "\n",
    "agent_tools = Tools()\n",
    "agent_tools.add_tools(search_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf0d06b7-5ae8-4c80-a57c-3cddd8d22854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'add_entry',\n",
       "  'description': 'Add a new entry to the FAQ database.\\n\\nArgs:\\n    question (str): The question to be added to the FAQ database.\\n    answer (str): The corresponding answer to the question.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'question': {'type': 'string',\n",
       "     'description': 'question parameter'},\n",
       "    'answer': {'type': 'string', 'description': 'answer parameter'}},\n",
       "   'required': ['question', 'answer'],\n",
       "   'additionalProperties': False}},\n",
       " {'type': 'function',\n",
       "  'name': 'search',\n",
       "  'description': 'Search the FAQ database for entries matching the given query.\\n\\nArgs:\\n    query (str): Search query text to look up in the course FAQ.\\n\\nReturns:\\n    List[Dict[str, Any]]: A list of search result entries, each containing relevant metadata.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'query parameter'}},\n",
       "   'required': ['query'],\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_tools.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "100d96f7-b569-43ba-a19c-81603ced531c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: howdy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "howdy\n"
     ]
    }
   ],
   "source": [
    "question = input('You:')\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1150ddf8-8abb-46d9-a148-4c94c7410397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, function_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4068a3c-1051-42c6-8f1e-0c0f54e98180",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    function_tool(search_tools.search),\n",
    "    function_tool(search_tools.add_entry),\n",
    "    # add it here\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e063b7a0-098c-483a-bb32-1f6d1f030d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.tools import wrap_instance_methods\n",
    "tools = wrap_instance_methods(function_tool, search_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90f77f1c-f258-41ea-9dc8-c883c4a87ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=developer_prompt,\n",
    "    tools=tools,\n",
    "    model='gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7fefe87-06d2-4862-9b4e-f9ef3325e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.chat.runners import OpenAIAgentsSDKRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5908b1c9-a9a5-4539-886a-a314683278ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = OpenAIAgentsSDKRunner(\n",
    "    chat_interface=chat_interface,\n",
    "    agent=agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8db9cfc8-0dce-4455-a653-eed32e155ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: how to do well in module 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"how to do well in module 1\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"how to do well in module 1\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'text': 'Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\\nKrishna Anand', 'section': 'Module 5: pyspark', 'question': 'Module Not Found Error in Jupyter Notebook .', 'course': 'data-engineering-zoomcamp', '_id': 322}, {'text': 'You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\\nAdditionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.', 'section': 'Module 5: pyspark', 'question': \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\", 'course': 'data-engineering-zoomcamp', '_id': 323}, {'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\", 'course': 'data-engineering-zoomcamp', '_id': 299}, {'text': 'create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \"TypeError: \\'module\\' object is not callable\"\\nSolution:\\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\\nengine = create_engine(conn_string)', 'section': 'Module 1: Docker and Terraform', 'question': \"Python - SQLALchemy - TypeError 'module' object is not callable\", 'course': 'data-engineering-zoomcamp', '_id': 124}, {'text': \"Error raised during the jupyter notebook’s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\", 'section': 'Module 1: Docker and Terraform', 'question': \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\", 'course': 'data-engineering-zoomcamp', '_id': 125}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"tips for success in Module 1\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"tips for success in Module 1\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'text': 'Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\\nKrishna Anand', 'section': 'Module 5: pyspark', 'question': 'Module Not Found Error in Jupyter Notebook .', 'course': 'data-engineering-zoomcamp', '_id': 322}, {'text': 'You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\\nAdditionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.', 'section': 'Module 5: pyspark', 'question': \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\", 'course': 'data-engineering-zoomcamp', '_id': 323}, {'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\", 'course': 'data-engineering-zoomcamp', '_id': 299}, {'text': \"Issue:\\ne…\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\", 'section': 'Module 1: Docker and Terraform', 'question': \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\", 'course': 'data-engineering-zoomcamp', '_id': 112}, {'text': 'create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \"TypeError: \\'module\\' object is not callable\"\\nSolution:\\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\\nengine = create_engine(conn_string)', 'section': 'Module 1: Docker and Terraform', 'question': \"Python - SQLALchemy - TypeError 'module' object is not callable\", 'course': 'data-engineering-zoomcamp', '_id': 124}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To excel in Module 1 of your course (related to Docker and Terraform), here are some suggestions based on common issues and solutions shared by other students:</p>\n",
       "<ol>\n",
       "<li><p><strong>Familiarize Yourself with Docker</strong>: As Module 1 primarily deals with Docker, ensure you're comfortable with basic Docker commands and concepts. Review the official Docker documentation if needed.</p>\n",
       "</li>\n",
       "<li><p><strong>Install Required Packages</strong>: Make sure you have installed all necessary Python packages listed in the course materials, especially <code>psycopg2</code> if you're working with PostgreSQL. You can install it using:</p>\n",
       "<pre><code class=\"language-bash\">pip install psycopg2-binary\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Troubleshooting Errors</strong>:</p>\n",
       "<ul>\n",
       "<li>If you encounter the &quot;ModuleNotFoundError: No module named 'psycopg2'&quot; error, try updating your package manager or uninstalling and reinstalling the package:<pre><code class=\"language-bash\">pip uninstall psycopg2\n",
       "pip install psycopg2-binary\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Database Connections</strong>: When setting up database connections using SQLAlchemy, ensure your connection string is correct. For example:</p>\n",
       "<pre><code class=\"language-python\">from sqlalchemy import create_engine\n",
       "conn_string = &quot;postgresql+psycopg://username:password@localhost:5432/database_name&quot;\n",
       "engine = create_engine(conn_string)\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Check Error Logs</strong>: If you encounter any issues when executing code, check the error message details for helpful insights. The error message can guide what might be missing or misconfigured.</p>\n",
       "</li>\n",
       "<li><p><strong>Access to Course Materials</strong>: Regularly refer back to the course resources and seek assistance from forums if you face challenges.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Do you have specific topics within Module 1 that you'd like to delve deeper into? Or is there anything else you'd like to explore?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "await runner.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c041ef0-dcff-4606-85ba-878a381d8fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pydantic AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4149826a-cd6f-4207-9c7e-61447913049a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bound method SearchTools.search of <__main__.SearchTools object at 0x000002B603AA0D70>>,\n",
       " <bound method SearchTools.add_entry of <__main__.SearchTools object at 0x000002B603AA0D70>>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "tools = [\n",
    "    search_tools.search,\n",
    "    search_tools.add_entry\n",
    "]\n",
    "\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09b7e0c5-ac0b-4b27-853f-db580e3f0fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=developer_prompt,\n",
    "    tools=tools,\n",
    "     model='gpt-4o-mini'\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "708259e1-b6f6-4e99-a377-ffb09028b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.chat.runners import PydanticAIRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f504482f-294a-4a3b-a7a1-76fa78a6205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = PydanticAIRunner(\n",
    "    chat_interface=chat_interface,\n",
    "    agent=agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4093b219-a6e5-4d94-b2ea-68c17f051ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: module 1 success tips\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search(\"{\\\"query\\\":\\\"module 1 success tips\\\"}\")</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"query\\\":\\\"module 1 success tips\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'text': 'Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\\nKrishna Anand', 'section': 'Module 5: pyspark', 'question': 'Module Not Found Error in Jupyter Notebook .', 'course': 'data-engineering-zoomcamp', '_id': 322}, {'text': 'You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\\nAdditionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.', 'section': 'Module 5: pyspark', 'question': \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\", 'course': 'data-engineering-zoomcamp', '_id': 323}, {'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\", 'course': 'data-engineering-zoomcamp', '_id': 299}, {'text': \"Issue:\\ne…\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\", 'section': 'Module 1: Docker and Terraform', 'question': \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\", 'course': 'data-engineering-zoomcamp', '_id': 112}, {'text': 'create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \"TypeError: \\'module\\' object is not callable\"\\nSolution:\\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\\nengine = create_engine(conn_string)', 'section': 'Module 1: Docker and Terraform', 'question': \"Python - SQLALchemy - TypeError 'module' object is not callable\", 'course': 'data-engineering-zoomcamp', '_id': 124}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search(\"{\\\"query\\\":\\\"success tips module 1 data engine...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"query\\\":\\\"success tips module 1 data engineering\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'text': 'Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv', 'section': 'Module 6: streaming with kafka', 'question': 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing', 'course': 'data-engineering-zoomcamp', '_id': 377}, {'text': 'Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\nMove the contents of the .gitignore file in your main .gitignore.\\nUse the terminal to cd into the Mage folder and:\\nrun “git remote remove origin” to de-couple the Mage repo,\\nrun “rm -rf .git” to delete local git files,\\nrun “git add .” to add the current folder as changes to stage, commit and push.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?', 'course': 'data-engineering-zoomcamp', '_id': 191}, {'text': 'This error appeared when running the command: docker build -t taxi_ingest:v001 .\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\nsudo chown -R $USER dir_path\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\", 'course': 'data-engineering-zoomcamp', '_id': 67}, {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\", 'section': 'General course-related questions', 'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?', 'course': 'data-engineering-zoomcamp', '_id': 3}, {'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\", 'course': 'data-engineering-zoomcamp', '_id': 299}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search(\"{\\\"query\\\":\\\"tips for success in data engineer...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"query\\\":\\\"tips for success in data engineering module 1\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'text': 'Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv', 'section': 'Module 6: streaming with kafka', 'question': 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing', 'course': 'data-engineering-zoomcamp', '_id': 377}, {'text': 'Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\nMove the contents of the .gitignore file in your main .gitignore.\\nUse the terminal to cd into the Mage folder and:\\nrun “git remote remove origin” to de-couple the Mage repo,\\nrun “rm -rf .git” to delete local git files,\\nrun “git add .” to add the current folder as changes to stage, commit and push.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?', 'course': 'data-engineering-zoomcamp', '_id': 191}, {'text': 'This error appeared when running the command: docker build -t taxi_ingest:v001 .\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\nsudo chown -R $USER dir_path\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\", 'course': 'data-engineering-zoomcamp', '_id': 67}, {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\", 'section': 'General course-related questions', 'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?', 'course': 'data-engineering-zoomcamp', '_id': 3}, {'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\", 'course': 'data-engineering-zoomcamp', '_id': 299}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>In searching for &quot;module 1 success tips&quot; specifically related to the Data Engineering Zoomcamp, here's what I found:</p>\n",
       "<ol>\n",
       "<li><p><strong>Preparing Your Development Environment</strong>: Ensure you have Docker set up correctly as many exercises and setups in Module 1 involve working with Docker containers. If you encounter errors, check the permissions of the directories and files you're working with.</p>\n",
       "</li>\n",
       "<li><p><strong>Troubleshooting Common Issues</strong>: Familiarize yourself with common error messages, such as <code>ModuleNotFoundError</code>. For example, if you see this error when trying to work with <code>psycopg2</code>, be sure to install the package using <code>pip install psycopg2-binary</code>.</p>\n",
       "</li>\n",
       "<li><p><strong>Utilizing GitHub</strong>: When working with repositories, make sure your local copies of repositories like Mage can be integrated into your main Data Engineering Zoomcamp repo. Follow Git instructions carefully to avoid issues related to repository tracking.</p>\n",
       "</li>\n",
       "<li><p><strong>Resources and Examples</strong>: Always refer back to example files provided in the course. If a particular resource file is missing, look for where it should be sourced from (e.g., Java examples or data files relevant to the course).</p>\n",
       "</li>\n",
       "<li><p><strong>Participation and Submissions</strong>: You can start learning and submitting homework even without formal registration. The registration mainly helps gauge interest prior to the start of the course.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Based on this, do you have specific areas in Module 1 that you want to explore further, such as Docker, troubleshooting errors, or integrating with Git?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: module 1 docker terraform tips success\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search(\"{\\\"query\\\":\\\"module 1 docker terraform tips fo...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"query\\\":\\\"module 1 docker terraform tips for success\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'text': 'You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command.', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Terraform initialized in an empty directory! The directory has no Terraform configuration files. You may begin working with Terraform immediately by creating Terraform configuration files.g', 'course': 'data-engineering-zoomcamp', '_id': 150}, {'text': \"The error:\\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\\nThe solution:\\nYou have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error creating Bucket: googleapi: Error 403: Permission denied to access ‘storage.buckets.create’', 'course': 'data-engineering-zoomcamp', '_id': 152}, {'text': 'https://github.com/hashicorp/terraform/issues/14513', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error acquiring the state lock', 'course': 'data-engineering-zoomcamp', '_id': 145}, {'text': 'https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Install for WSL', 'course': 'data-engineering-zoomcamp', '_id': 144}, {'text': \"The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error:Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901\": oauth2: cannot fetch token: Post \"https://oauth2.googleapis.com/token\": dial tcp 172.217.163.42:443: i/o timeout', 'course': 'data-engineering-zoomcamp', '_id': 143}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search(\"{\\\"query\\\":\\\"tips for success in module 1 dock...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"query\\\":\\\"tips for success in module 1 docker terraform\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'text': 'You get this error because I run the command terraform init outside the working directory, and this is wrong.You need first to navigate to the working directory that contains terraform configuration files, and and then run the command.', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Terraform initialized in an empty directory! The directory has no Terraform configuration files. You may begin working with Terraform immediately by creating Terraform configuration files.g', 'course': 'data-engineering-zoomcamp', '_id': 150}, {'text': \"The error:\\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\\nThe solution:\\nYou have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error creating Bucket: googleapi: Error 403: Permission denied to access ‘storage.buckets.create’', 'course': 'data-engineering-zoomcamp', '_id': 152}, {'text': 'https://github.com/hashicorp/terraform/issues/14513', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error acquiring the state lock', 'course': 'data-engineering-zoomcamp', '_id': 145}, {'text': 'https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845', 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Install for WSL', 'course': 'data-engineering-zoomcamp', '_id': 144}, {'text': \"The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.\", 'section': 'Module 1: Docker and Terraform', 'question': 'Terraform - Error:Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901\": oauth2: cannot fetch token: Post \"https://oauth2.googleapis.com/token\": dial tcp 172.217.163.42:443: i/o timeout', 'course': 'data-engineering-zoomcamp', '_id': 143}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Here are some success tips for Module 1: Docker and Terraform from the Data Engineering Zoomcamp:</p>\n",
       "<ol>\n",
       "<li><p><strong>Navigate to the Correct Directory</strong>: Ensure that you are executing <code>terraform init</code> in the correct working directory that contains your Terraform configuration files. Running this command in an empty or incorrect directory can lead to errors.</p>\n",
       "</li>\n",
       "<li><p><strong>Understand Permissions in Google Cloud</strong>: If you encounter permissions issues, such as getting a 403 error when creating resources with Terraform, make sure that the Project ID you are using is correct and corresponds to your Google Cloud project. It can sometimes be different from the project name you see in the console.</p>\n",
       "</li>\n",
       "<li><p><strong>Increase Proxy Compatibility</strong>: If you face network issues, such as when Google services are blocked or slow in your area, configuring your VPN properly can help. Make sure your terminal is aware of your system's proxy settings, especially for services like Google Cloud.</p>\n",
       "</li>\n",
       "<li><p><strong>Check the Terraform Documentation</strong>: Following best practices and referring to official Terraform documentation is essential for installations and how to structure your configuration files. If you're new to Terraform, spend time with the documentation to understand how its workflow operates.</p>\n",
       "</li>\n",
       "<li><p><strong>Stay Updated with Community Resources</strong>: Utilize platforms like GitHub and tech forums if you run into specific technical issues. For instance, errors related to state locks or token fetching often have discussions in communities where other users share their solutions.</p>\n",
       "</li>\n",
       "<li><p><strong>Install Terraform on Your OS</strong>: If you're using Windows, consider looking into community guides that detail installing and setting up Terraform on WSL (Windows Subsystem for Linux) for more seamless operation.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>If you have specific challenges in Docker or Terraform that you want to discuss more, or if there's another area where you're seeking guidance, let me know!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: tips for success module 1 data engineering\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search(\"{\\\"query\\\":\\\"success tips module 1 data engine...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"query\\\":\\\"success tips module 1 data engineering\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'text': 'Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv', 'section': 'Module 6: streaming with kafka', 'question': 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing', 'course': 'data-engineering-zoomcamp', '_id': 377}, {'text': 'Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\nMove the contents of the .gitignore file in your main .gitignore.\\nUse the terminal to cd into the Mage folder and:\\nrun “git remote remove origin” to de-couple the Mage repo,\\nrun “rm -rf .git” to delete local git files,\\nrun “git add .” to add the current folder as changes to stage, commit and push.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?', 'course': 'data-engineering-zoomcamp', '_id': 191}, {'text': 'This error appeared when running the command: docker build -t taxi_ingest:v001 .\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\nsudo chown -R $USER dir_path\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\", 'course': 'data-engineering-zoomcamp', '_id': 67}, {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\", 'section': 'General course-related questions', 'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?', 'course': 'data-engineering-zoomcamp', '_id': 3}, {'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\", 'course': 'data-engineering-zoomcamp', '_id': 299}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search(\"{\\\"query\\\":\\\"Module 1 success strategies data ...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"query\\\":\\\"Module 1 success strategies data engineering\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'text': 'Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv', 'section': 'Module 6: streaming with kafka', 'question': 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing', 'course': 'data-engineering-zoomcamp', '_id': 377}, {'text': 'Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\nMove the contents of the .gitignore file in your main .gitignore.\\nUse the terminal to cd into the Mage folder and:\\nrun “git remote remove origin” to de-couple the Mage repo,\\nrun “rm -rf .git” to delete local git files,\\nrun “git add .” to add the current folder as changes to stage, commit and push.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?', 'course': 'data-engineering-zoomcamp', '_id': 191}, {'text': 'This error appeared when running the command: docker build -t taxi_ingest:v001 .\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\nsudo chown -R $USER dir_path\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\", 'course': 'data-engineering-zoomcamp', '_id': 67}, {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\", 'section': 'General course-related questions', 'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?', 'course': 'data-engineering-zoomcamp', '_id': 3}, {'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\", 'section': 'Module 4: analytics engineering with dbt', 'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\", 'course': 'data-engineering-zoomcamp', '_id': 299}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Here are some tips for success in Module 1 of the Data Engineering Zoomcamp:</p>\n",
       "<ol>\n",
       "<li><p><strong>Familiarize Yourself with Docker</strong>: Understand the basics of Docker, as it is central to the module. Make sure you can confidently build and run Docker images. Practice several examples to build your confidence.</p>\n",
       "</li>\n",
       "<li><p><strong>Navigate Correctly</strong>: When using Terraform, ensure you are running commands like <code>terraform init</code> in the correct directory that contains your Terraform configuration files. Running commands in an incorrect location can lead to errors.</p>\n",
       "</li>\n",
       "<li><p><strong>Check Permissions</strong>: If you encounter permission errors while using Docker or Terraform, check the permissions of the directories you are working with. In cases where you need access, consider changing the permissions or running commands with elevated privileges.</p>\n",
       "</li>\n",
       "<li><p><strong>Understand Google Cloud Permissions</strong>: If you are working with Google Cloud resources, ensure you have set the correct permissions for your service accounts. Incorrect permissions can lead to access errors, such as being unable to create storage buckets.</p>\n",
       "</li>\n",
       "<li><p><strong>Utilize Community Resources</strong>: Don't hesitate to leverage community forums, documentation, and GitHub repositories for troubleshooting and tips. Often, others have encountered similar issues, and you can find solutions or workarounds shared by previous learners.</p>\n",
       "</li>\n",
       "<li><p><strong>Follow the Course Structure</strong>: Be sure to follow the module's structure and complete the assignments step-by-step. This will help reinforce your understanding and ensure you don’t miss any critical concepts.</p>\n",
       "</li>\n",
       "<li><p><strong>Ask Questions and Engage</strong>: If you run into complications or uncertainties, don't hesitate to ask questions in the course forums. Engaging with instructors and peers can clarify concepts and provide insights that may not be covered in the materials.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>If you have specific challenges or topics you’d like to focus on further, let me know!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: module 1 homework assignments\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search(\"{\\\"query\\\":\\\"module 1 homework assignments dat...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"query\\\":\\\"module 1 homework assignments data engineering\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'text': 'Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\nMove the contents of the .gitignore file in your main .gitignore.\\nUse the terminal to cd into the Mage folder and:\\nrun “git remote remove origin” to de-couple the Mage repo,\\nrun “rm -rf .git” to delete local git files,\\nrun “git add .” to add the current folder as changes to stage, commit and push.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?', 'course': 'data-engineering-zoomcamp', '_id': 191}, {'text': 'Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv', 'section': 'Module 6: streaming with kafka', 'question': 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing', 'course': 'data-engineering-zoomcamp', '_id': 377}, {'text': 'This error appeared when running the command: docker build -t taxi_ingest:v001 .\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\nsudo chown -R $USER dir_path\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\", 'course': 'data-engineering-zoomcamp', '_id': 67}, {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\", 'section': 'General course-related questions', 'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?', 'course': 'data-engineering-zoomcamp', '_id': 3}, {'text': 'I found out that the easies way to upload datasets form github for the homework is utilising this script git_csv_to_gcs.py. Thank you Lidia!!\\nIt is similar to a script that Alexey provided us in 03-data-warehouse/extras/web_to_gcs.py', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Homework - Ingesting NYC TLC Data', 'course': 'data-engineering-zoomcamp', '_id': 307}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search(\"{\\\"query\\\":\\\"module 1 assignments homework dat...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"query\\\":\\\"module 1 assignments homework data engineering zoomcamp\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'text': 'Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\nMove the contents of the .gitignore file in your main .gitignore.\\nUse the terminal to cd into the Mage folder and:\\nrun “git remote remove origin” to de-couple the Mage repo,\\nrun “rm -rf .git” to delete local git files,\\nrun “git add .” to add the current folder as changes to stage, commit and push.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?', 'course': 'data-engineering-zoomcamp', '_id': 191}, {'text': 'Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv', 'section': 'Module 6: streaming with kafka', 'question': 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing', 'course': 'data-engineering-zoomcamp', '_id': 377}, {'text': 'This error appeared when running the command: docker build -t taxi_ingest:v001 .\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\nsudo chown -R $USER dir_path\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\", 'course': 'data-engineering-zoomcamp', '_id': 67}, {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\", 'section': 'General course-related questions', 'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?', 'course': 'data-engineering-zoomcamp', '_id': 3}, {'text': 'I found out that the easies way to upload datasets form github for the homework is utilising this script git_csv_to_gcs.py. Thank you Lidia!!\\nIt is similar to a script that Alexey provided us in 03-data-warehouse/extras/web_to_gcs.py', 'section': 'Module 4: analytics engineering with dbt', 'question': 'Homework - Ingesting NYC TLC Data', 'course': 'data-engineering-zoomcamp', '_id': 307}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>It seems there are no specific details available directly regarding the homework assignments for Module 1 of the Data Engineering Zoomcamp. However, here are some general practices and suggestions for managing your homework assignments for this module:</p>\n",
       "<ol>\n",
       "<li><p><strong>Understand the Requirements</strong>: Carefully read through the assignment instructions provided in the course materials. Make sure you understand the objectives and deliverables.</p>\n",
       "</li>\n",
       "<li><p><strong>Utilize Docker and Terraform</strong>: Since Module 1 typically involves Docker and Terraform, ensure you have them installed properly. Familiarize yourself with basic commands and workflows, as many assignments will require you to use these tools.</p>\n",
       "</li>\n",
       "<li><p><strong>Version Control</strong>: Take your assignments seriously by using version control (like Git). This will help you manage changes, track your progress, and collaborate if necessary. Make sure to commit your work regularly.</p>\n",
       "</li>\n",
       "<li><p><strong>GitHub Integration</strong>: If you are downloading repo resources (like Mage), ensure that you integrate it properly into your GitHub repository. Follow the provided steps carefully to avoid any issues with repository tracking.</p>\n",
       "</li>\n",
       "<li><p><strong>Ask for Help</strong>: If you encounter challenges or are unsure about assignment expectations, don’t hesitate to ask your peers or instructors for clarification. Engaging with the community can lead to faster problem-solving.</p>\n",
       "</li>\n",
       "<li><p><strong>Learning from Errors</strong>: If you run into errors while working on assignments (e.g., during the Docker build process), use those moments to learn. Understand the error messages and research solutions to prevent them in future tasks.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>If you have any specific questions or need assistance with a particular homework assignment or concept from Module 1, let me know!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: how to succeed in data engineering course\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search(\"{\\\"query\\\":\\\"how to succeed in data engineerin...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"query\\\":\\\"how to succeed in data engineering course\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'text': 'Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\nMove the contents of the .gitignore file in your main .gitignore.\\nUse the terminal to cd into the Mage folder and:\\nrun “git remote remove origin” to de-couple the Mage repo,\\nrun “rm -rf .git” to delete local git files,\\nrun “git add .” to add the current folder as changes to stage, commit and push.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?', 'course': 'data-engineering-zoomcamp', '_id': 191}, {'text': 'Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv', 'section': 'Module 6: streaming with kafka', 'question': 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing', 'course': 'data-engineering-zoomcamp', '_id': 377}, {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\", 'section': 'General course-related questions', 'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?', 'course': 'data-engineering-zoomcamp', '_id': 3}, {'text': 'This error appeared when running the command: docker build -t taxi_ingest:v001 .\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\nsudo chown -R $USER dir_path\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\", 'course': 'data-engineering-zoomcamp', '_id': 67}, {'text': 'All the main videos are stored in the Main “DATA ENGINEERING” playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\\nBelow is the MAIN PLAYLIST’. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\\nh\\nttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-', 'section': 'General course-related questions', 'question': 'Course - Which playlist on YouTube should I refer to?', 'course': 'data-engineering-zoomcamp', '_id': 9}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search(\"{\\\"query\\\":\\\"success strategies data engineeri...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>\"{\\\"query\\\":\\\"success strategies data engineering course\\\"}\"</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[{'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\", 'section': 'General course-related questions', 'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?', 'course': 'data-engineering-zoomcamp', '_id': 3}, {'text': 'Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\nMove the contents of the .gitignore file in your main .gitignore.\\nUse the terminal to cd into the Mage folder and:\\nrun “git remote remove origin” to de-couple the Mage repo,\\nrun “rm -rf .git” to delete local git files,\\nrun “git add .” to add the current folder as changes to stage, commit and push.', 'section': 'Module 2: Workflow Orchestration', 'question': 'Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?', 'course': 'data-engineering-zoomcamp', '_id': 191}, {'text': 'Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv', 'section': 'Module 6: streaming with kafka', 'question': 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing', 'course': 'data-engineering-zoomcamp', '_id': 377}, {'text': 'This error appeared when running the command: docker build -t taxi_ingest:v001 .\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn’t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\nsudo chown -R $USER dir_path\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay', 'section': 'Module 1: Docker and Terraform', 'question': \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\", 'course': 'data-engineering-zoomcamp', '_id': 67}, {'text': 'All the main videos are stored in the Main “DATA ENGINEERING” playlist (no year specified). The Github repository has also been updated to show each video with a thumbnail, that would bring you directly to the same playlist below.\\nBelow is the MAIN PLAYLIST’. And then you refer to the year specific playlist for additional videos for that year like for office hours videos etc. Also find this playlist pinned to the slack channel.\\nh\\nttps://youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&si=NspQhtZhZQs1B9F-', 'section': 'General course-related questions', 'question': 'Course - Which playlist on YouTube should I refer to?', 'course': 'data-engineering-zoomcamp', '_id': 9}]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Here are some strategies for succeeding in a Data Engineering course:</p>\n",
       "<ol>\n",
       "<li><p><strong>Establish a Strong Foundation</strong>: Familiarize yourself with essential technologies like SQL, Python, and data modeling principles. A solid understanding of these fundamentals will help you grasp more complex concepts.</p>\n",
       "</li>\n",
       "<li><p><strong>Engage with the Course Materials</strong>: Watch all video lectures, read accompanying materials, and participate in discussions. Engaging actively with the content will enhance your retention and understanding.</p>\n",
       "</li>\n",
       "<li><p><strong>Hands-On Practice</strong>: Data Engineering relies heavily on practical skills. Work on projects involving databases, ETL processes, data pipeline construction, and cloud services. Hands-on experience is invaluable.</p>\n",
       "</li>\n",
       "<li><p><strong>Leverage Version Control</strong>: Use Git and GitHub to track your work. Not only does this practice improve collaboration, but it also helps you learn version control, which is widely used in the industry.</p>\n",
       "</li>\n",
       "<li><p><strong>Utilize Community Resources</strong>: Don't hesitate to ask questions in forums, study groups, and to your instructors. The community can provide insights and support that enhance your learning experience.</p>\n",
       "</li>\n",
       "<li><p><strong>Work on Assignments Promptly</strong>: Complete homework and projects as soon as possible while the concepts are fresh in your mind. This practice can help solidify understanding and reduce last-minute pressure.</p>\n",
       "</li>\n",
       "<li><p><strong>Stay Updated</strong>: The data engineering landscape is constantly evolving. Follow industry blogs, participate in webinars, and engage with the community to keep abreast of new tools and best practices.</p>\n",
       "</li>\n",
       "<li><p><strong>Document Your Learning</strong>: Keep notes and summaries of what you've learned. This documentation will not only help you during the course but also serve as a valuable resource for future reference.</p>\n",
       "</li>\n",
       "<li><p><strong>Build a Portfolio</strong>: As you complete projects and assignments, assemble them in a portfolio to showcase your skills to potential employers. This can be a valuable asset when seeking employment or internships.</p>\n",
       "</li>\n",
       "<li><p><strong>Set Goals and Stay Organized</strong>: Setting clear goals for what you want to achieve in the course will keep you focused. Use calendars and task lists to manage your time effectively.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>If you'd like to explore any specific topics further, or if you have additional questions, just let me know!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "await runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc49ae3-66a1-4cfe-8f19-67fd15314de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc44dc0-b963-47d3-97c1-6e2b11f6b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent <-> MCP server -> tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d7aefd5-34f7-422f-ad06-b10a585fe137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.mcp import MCPClient, SubprocessMCPTransport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "073ec8ca-1eb1-40f4-a69d-3304d8adcc3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'toyaikit.mcp.transports'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtoyaikit\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmcp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MCPClient\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtoyaikit\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmcp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransports\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SSEMCPTransport\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'toyaikit.mcp.transports'"
     ]
    }
   ],
   "source": [
    "from toyaikit.mcp.client import MCPClient\n",
    "from toyaikit.mcp.transports.sse import SSEMCPTransport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffefde20-056d-4a4e-92e5-e55ef6496719",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"uv run python main.py\".split()\n",
    "workdir = \"mcp_faq\"\n",
    "\n",
    "client = MCPClient(\n",
    "    transport=SubprocessMCPTransport(\n",
    "        server_command=command,\n",
    "        workdir=workdir\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ab8666d-f5ab-4cea-ad69-7d15750bdee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started server with command: uv run python main.py\n"
     ]
    }
   ],
   "source": [
    "client.start_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1c0f307-13c1-4244-b140-3c5d4d5fce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending initialize request...\n",
      "Initialize response: {'protocolVersion': '2024-11-05', 'capabilities': {'experimental': {}, 'prompts': {'listChanged': False}, 'resources': {'subscribe': False, 'listChanged': False}, 'tools': {'listChanged': True}}, 'serverInfo': {'name': 'Demo 🚀', 'version': '1.16.0'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'protocolVersion': '2024-11-05',\n",
       " 'capabilities': {'experimental': {},\n",
       "  'prompts': {'listChanged': False},\n",
       "  'resources': {'subscribe': False, 'listChanged': False},\n",
       "  'tools': {'listChanged': True}},\n",
       " 'serverInfo': {'name': 'Demo 🚀', 'version': '1.16.0'}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c970c394-4b26-41cc-bea4-52b6cb3c5b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending initialized notification...\n",
      "Handshake completed successfully\n"
     ]
    }
   ],
   "source": [
    "client.initialized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e48f10b-0ef9-41ca-bb31-db1c525a60fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving available tools...\n",
      "Available tools: ['add_entry', 'search']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'add_entry',\n",
       "  'description': 'Add a new entry to the FAQ database.\\n\\nArgs:\\n    question (str): The question to be added to the FAQ database.\\n    answer (str): The corresponding answer to the question.',\n",
       "  'inputSchema': {'properties': {'question': {'type': 'string'},\n",
       "    'answer': {'type': 'string'}},\n",
       "   'required': ['question', 'answer'],\n",
       "   'type': 'object'},\n",
       "  '_meta': {'_fastmcp': {'tags': []}}},\n",
       " {'name': 'search',\n",
       "  'description': 'Search the FAQ database for entries matching the given query.\\n\\nArgs:\\n    query (str): Search query text to look up in the course FAQ.\\n\\nReturns:\\n    List[Dict[str, Any]]: A list of search result entries, each containing relevant metadata.',\n",
       "  'inputSchema': {'properties': {'query': {'type': 'string'}},\n",
       "   'required': ['query'],\n",
       "   'type': 'object'},\n",
       "  'outputSchema': {'properties': {'result': {'items': {'additionalProperties': True,\n",
       "      'type': 'object'},\n",
       "     'type': 'array'}},\n",
       "   'required': ['result'],\n",
       "   'type': 'object',\n",
       "   'x-fastmcp-wrap-result': True},\n",
       "  '_meta': {'_fastmcp': {'tags': []}}}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "02ea12f3-c6c4-47c7-8caf-8f43f109f935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool 'search' with arguments: {'query': 'how do I run docker?'}\n"
     ]
    }
   ],
   "source": [
    "result = client.call_tool('search', {'query': 'how do I run docker?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f33cd52f-8e35-428b-866c-a76a36e5d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.mcp import MCPTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b264205-62a8-49b6-8513-65ff062e31cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_tools = MCPTools(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "20521c4c-7850-4175-a11c-52ac36d8c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = OpenAIResponsesRunner(\n",
    "    tools=mcp_tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=OpenAIClient(model='gpt-4o-mini')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "343e643e-735d-444c-ba63-79dd4480597e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: install Kafka\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving available tools...\n",
      "Available tools: ['add_entry', 'search']\n",
      "Calling tool 'search' with arguments: {'query': 'install Kafka'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"install Kafka\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"install Kafka\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_UNhn3jWzCFn3ISnPm3ls60lc', 'output': '[{\"text\":\"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\\\nfastavro: pip install fastavro\\\\nAbhirup Ghosh\\\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\",\"section\":\"Module 6: streaming with kafka\",\"question\":\"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\",\"course\":\"data-engineering-zoomcamp\"},{\"text\":\"In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\\\nSolution:\\\\nIn build.gradle file, I added the following at the end:\\\\nshadowJar {\\\\narchiveBaseName = \\\\\"java-kafka-rides\\\\\"\\\\narchiveClassifier = \\'\\'\\\\n}\\\\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\",\"section\":\"Module 6: streaming with kafka\",\"question\":\"Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\",\"course\":\"data-engineering-zoomcamp\"},{\"text\":\"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\\\nKafka Python Videos - Rides.csv\\\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\",\"section\":\"Module 6: streaming with kafka\",\"question\":\"Kafka- python videos have low audio and hard to follow up\",\"course\":\"data-engineering-zoomcamp\"},{\"text\":\"If you have this error, it most likely that your kafka broker docker container is not working.\\\\nUse docker ps to confirm\\\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\",\"section\":\"Module 6: streaming with kafka\",\"question\":\"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\",\"course\":\"data-engineering-zoomcamp\"},{\"text\":\"For example, when running JsonConsumer.java, got:\\\\nConsuming form kafka started\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nOr when running JsonProducer.java, got:\\\\nException in thread \\\\\"main\\\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\\\nSolution:\\\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\",\"section\":\"Module 6: streaming with kafka\",\"question\":\"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\",\"course\":\"data-engineering-zoomcamp\"}]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool 'search' with arguments: {'query': 'install Apache Kafka'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"install Apache Kafka\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"install Apache Kafka\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_vKnzOsoRT6DXP3sLWngfRiwl', 'output': '[{\"text\":\"Make sure your java version is 11 or 8.\\\\nCheck your version by:\\\\njava --version\\\\nCheck all your versions by:\\\\n/usr/libexec/java_home -V\\\\nIf you already have got java 11 but just not selected as default, select the specific version by:\\\\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\\\\n(or other version of 11)\",\"section\":\"Module 6: streaming with kafka\",\"question\":\"Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\",\"course\":\"data-engineering-zoomcamp\"},{\"text\":\"For example, when running JsonConsumer.java, got:\\\\nConsuming form kafka started\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nOr when running JsonProducer.java, got:\\\\nException in thread \\\\\"main\\\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\\\nSolution:\\\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\",\"section\":\"Module 6: streaming with kafka\",\"question\":\"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\",\"course\":\"data-engineering-zoomcamp\"},{\"text\":\"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\\\nfastavro: pip install fastavro\\\\nAbhirup Ghosh\\\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\",\"section\":\"Module 6: streaming with kafka\",\"question\":\"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\",\"course\":\"data-engineering-zoomcamp\"},{\"text\":\"If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\\\\nFor Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\\\\\\\bin” to the PATH variable.\\\\nAdditional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io\",\"section\":\"Module 5: pyspark\",\"question\":\"Hadoop - Exception in thread \\\\\"main\\\\\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\",\"course\":\"data-engineering-zoomcamp\"},{\"text\":\"While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\\\\n…\\\\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\\\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\\\\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\\\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\\\\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\\\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\\\\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\\\n…\\\\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\\\\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\\\\n…\\\\nSolution:\\\\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\\\\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\\\\nSolution 2:\\\\nCheck what Spark version your local machine has\\\\npyspark –version\\\\nspark-submit –version\\\\nAdd your version to SPARK_VERSION in build.sh\",\"section\":\"Module 6: streaming with kafka\",\"question\":\"Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\",\"course\":\"data-engineering-zoomcamp\"}]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To install Apache Kafka, you can follow these general steps:</p>\n",
       "<ol>\n",
       "<li><p><strong>Prerequisites</strong>:</p>\n",
       "<ul>\n",
       "<li>Ensure you have <code>Java 8</code> or <code>Java 11</code> installed. You can check this using:<pre><code class=\"language-bash\">java --version\n",
       "</code></pre>\n",
       "</li>\n",
       "<li>If necessary, set your Java version using:<pre><code class=\"language-bash\">export JAVA_HOME=$(/usr/libexec/java_home -v 11.0.x)  # Replace with your Java version\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Download Kafka</strong>:\n",
       "You can download the latest version of Kafka from the <a href=\"https://kafka.apache.org/downloads\">Apache Kafka website</a>.</p>\n",
       "</li>\n",
       "<li><p><strong>Extract Kafka</strong>:\n",
       "After downloading, extract the files using:</p>\n",
       "<pre><code class=\"language-bash\">tar -xzf kafka_2.13-2.8.0.tgz  # Replace with your downloaded version\n",
       "cd kafka_2.13-2.8.0\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Start Kafka Server</strong>:\n",
       "Before starting the Kafka server, you need to start the Zookeeper service (Kafka relies on Zookeeper). You can start both services using the following commands:</p>\n",
       "<pre><code class=\"language-bash\">bin/zookeeper-server-start.sh config/zookeeper.properties\n",
       "bin/kafka-server-start.sh config/server.properties\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Create a Topic</strong> (optional):\n",
       "You can create a topic to test your Kafka installation:</p>\n",
       "<pre><code class=\"language-bash\">bin/kafka-topics.sh --create --topic test --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Send Messages</strong>:\n",
       "You can start a simple producer to send messages to your topic:</p>\n",
       "<pre><code class=\"language-bash\">bin/kafka-console-producer.sh --topic test --bootstrap-server localhost:9092\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Consume Messages</strong>:\n",
       "To read messages, start a consumer in a new terminal:</p>\n",
       "<pre><code class=\"language-bash\">bin/kafka-console-consumer.sh --topic test --from-beginning --bootstrap-server localhost:9092\n",
       "</code></pre>\n",
       "</li>\n",
       "</ol>\n",
       "<p>These steps should help you get started with installing and using Kafka. Is there anything specific you would like to delve deeper into, such as configuration or troubleshooting?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: Kafka installation guide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool 'add_entry' with arguments: {'question': 'Kafka installation guide', 'answer': '### Apache Kafka Installation Guide\\n\\n#### Prerequisites:\\n1. **Java Installation**:\\n   - Kafka requires Java 8 or 11. Check your Java version with:\\n     ```bash\\n     java --version\\n     ```\\n   - If you need to set the default Java version, you can run:\\n     ```bash\\n     export JAVA_HOME=$(/usr/libexec/java_home -v 11.0.x)  # Use the Java version you have installed\\n     ```\\n\\n#### Download Kafka:\\n2. **Download the Kafka Distribution**:\\n   - Visit the [Apache Kafka downloads page](https://kafka.apache.org/downloads) to get the latest version.\\n   - Use the following command to download (update the URL according to the latest version):\\n     ```bash\\n     wget https://downloads.apache.org/kafka/2.8.0/kafka_2.13-2.8.0.tgz\\n     ```\\n\\n#### Extract Kafka:\\n3. **Unzip the Downloaded Files**:\\n   - After downloading, extract the contents:\\n     ```bash\\n     tar -xzf kafka_2.13-2.8.0.tgz\\n     cd kafka_2.13-2.8.0\\n     ```\\n\\n#### Start Services:\\n4. **Start Zookeeper**:\\n   - Kafka uses Zookeeper for managing distributed brokers:\\n     ```bash\\n     bin/zookeeper-server-start.sh config/zookeeper.properties\\n     ```\\n\\n5. **Start Kafka Server**:\\n   - After Zookeeper is running, start Kafka:\\n     ```bash\\n     bin/kafka-server-start.sh config/server.properties\\n     ```\\n\\n#### Create a Topic:\\n6. **Creating a Topic** (optional):\\n   - You can create a topic to test:\\n     ```bash\\n     bin/kafka-topics.sh --create --topic test --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\\n     ```\\n\\n#### Send and Receive Messages:\\n7. **Running a Producer**:\\n   - Start a producer to send messages to the topic:\\n     ```bash\\n     bin/kafka-console-producer.sh --topic test --bootstrap-server localhost:9092\\n     ```\\n\\n8. **Running a Consumer**:\\n   - In a new terminal, start a consumer to read messages:\\n     ```bash\\n     bin/kafka-console-consumer.sh --topic test --from-beginning --bootstrap-server localhost:9092\\n     ```\\n\\n#### Notes:\\n- Make sure to replace version numbers and file names according to the latest release you download.\\n- For production setups, additional configurations are recommended to ensure reliability and performance.'}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\toyaikit\\chat\\runners.py:210\u001b[39m, in \u001b[36mOpenAIResponsesRunner.run\u001b[39m\u001b[34m(self, previous_messages, stop_criteria)\u001b[39m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m.chat_interface.display(\u001b[33m\"\u001b[39m\u001b[33mChat ended.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m loop_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprevious_messages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisplaying_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m chat_messages.extend(loop_result.new_messages)\n\u001b[32m    217\u001b[39m total_input_tokens += loop_result.tokens.input_tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\toyaikit\\chat\\runners.py:148\u001b[39m, in \u001b[36mOpenAIResponsesRunner.loop\u001b[39m\u001b[34m(self, prompt, previous_messages, callback)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m response.output:\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m entry.type == \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m         chat_messages.append(result)\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m callback:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\toyaikit\\mcp\\mcp_tools.py:93\u001b[39m, in \u001b[36mMCPTools.function_call\u001b[39m\u001b[34m(self, tool_call_response)\u001b[39m\n\u001b[32m     90\u001b[39m arguments = json.loads(tool_call_response.arguments)\n\u001b[32m     92\u001b[39m result = \u001b[38;5;28mself\u001b[39m.mcp_client.call_tool(function_name, arguments)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m output = \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     96\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfunction_call_output\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     97\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcall_id\u001b[39m\u001b[33m\"\u001b[39m: tool_call_response.call_id,\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m: output,\n\u001b[32m     99\u001b[39m }\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "runner.run();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3bbcea79-55d5-4158-bf84-07b948834d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.mcp import MCPServerSSE\n",
    "\n",
    "faq_mcp_client = MCPServerSSE(\n",
    "    url='http://localhost:8000/sse'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd4d0432-3ccb-4458-8562-998edf2cdf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=developer_prompt,\n",
    "    model='gpt-4o-mini',\n",
    "    toolsets=[faq_mcp_client]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dafbe801-d19f-4167-bdbc-28edeeabc83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = PydanticAIRunner(\n",
    "    chat_interface=chat_interface,\n",
    "    agent=agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "69099e6e-72ff-4a5f-a884-decbcef9fc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: how do i install kafka python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3697, in run_code\n",
      "  |     await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |   File \"C:\\Users\\ryahj\\AppData\\Local\\Temp\\ipykernel_28068\\411691516.py\", line 1, in <module>\n",
      "  |     await runner.run();\n",
      "  |     ^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\toyaikit\\chat\\runners.py\", line 324, in run\n",
      "  |     result = await self.agent.run(\n",
      "  |              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |         user_prompt=user_input, message_history=message_history\n",
      "  |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |     )\n",
      "  |     ^\n",
      "  |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\pydantic_ai\\agent\\abstract.py\", line 222, in run\n",
      "  |     async with self.iter(\n",
      "  |                ~~~~~~~~~^\n",
      "  |         user_prompt=user_prompt,\n",
      "  |         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |     ...<9 lines>...\n",
      "  |         builtin_tools=builtin_tools,\n",
      "  |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |     ) as agent_run:\n",
      "  |     ^\n",
      "  |   File \"C:\\Python313\\Lib\\contextlib.py\", line 214, in __aenter__\n",
      "  |     return await anext(self.gen)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\pydantic_ai\\agent\\__init__.py\", line 665, in iter\n",
      "  |     async with toolset:\n",
      "  |                ^^^^^^^\n",
      "  |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\pydantic_ai\\toolsets\\combined.py\", line 51, in __aenter__\n",
      "  |     await exit_stack.enter_async_context(toolset)\n",
      "  |   File \"C:\\Python313\\Lib\\contextlib.py\", line 668, in enter_async_context\n",
      "  |     result = await _enter(cm)\n",
      "  |              ^^^^^^^^^^^^^^^^\n",
      "  |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\pydantic_ai\\mcp.py\", line 317, in __aenter__\n",
      "  |     self._read_stream, self._write_stream = await exit_stack.enter_async_context(self.client_streams())\n",
      "  |                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"C:\\Python313\\Lib\\contextlib.py\", line 668, in enter_async_context\n",
      "  |     result = await _enter(cm)\n",
      "  |              ^^^^^^^^^^^^^^^^\n",
      "  |   File \"C:\\Python313\\Lib\\contextlib.py\", line 214, in __aenter__\n",
      "  |     return await anext(self.gen)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\pydantic_ai\\mcp.py\", line 762, in client_streams\n",
      "  |     async with transport_client_partial(headers=self.headers) as (read_stream, write_stream, *_):\n",
      "  |                ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"C:\\Python313\\Lib\\contextlib.py\", line 214, in __aenter__\n",
      "  |     return await anext(self.gen)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\mcp\\client\\sse.py\", line 55, in sse_client\n",
      "  |     async with anyio.create_task_group() as tg:\n",
      "  |                ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 781, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  |         \"unhandled errors in a TaskGroup\", self._exceptions\n",
      "  |     ) from None\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    |     yield\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 394, in handle_async_request\n",
      "    |     resp = await self._pool.handle_async_request(req)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 256, in handle_async_request\n",
      "    |     raise exc from None\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 236, in handle_async_request\n",
      "    |     response = await connection.handle_async_request(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |         pool_request.request\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n",
      "    |     raise exc\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 78, in handle_async_request\n",
      "    |     stream = await self._connect(request)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 124, in _connect\n",
      "    |     stream = await self._network_backend.connect_tcp(**kwargs)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpcore\\_backends\\auto.py\", line 31, in connect_tcp\n",
      "    |     return await self._backend.connect_tcp(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<5 lines>...\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 113, in connect_tcp\n",
      "    |     with map_exceptions(exc_map):\n",
      "    |          ~~~~~~~~~~~~~~^^^^^^^^^\n",
      "    |   File \"C:\\Python313\\Lib\\contextlib.py\", line 162, in __exit__\n",
      "    |     self.gen.throw(value)\n",
      "    |     ~~~~~~~~~~~~~~^^^^^^^\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    |     raise to_exc(exc) from exc\n",
      "    | httpcore.ConnectError: All connection attempts failed\n",
      "    | \n",
      "    | The above exception was the direct cause of the following exception:\n",
      "    | \n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\mcp\\client\\sse.py\", line 61, in sse_client\n",
      "    |     async with aconnect_sse(\n",
      "    |                ~~~~~~~~~~~~^\n",
      "    |         client,\n",
      "    |         ^^^^^^^\n",
      "    |         \"GET\",\n",
      "    |         ^^^^^^\n",
      "    |         url,\n",
      "    |         ^^^^\n",
      "    |     ) as event_source:\n",
      "    |     ^\n",
      "    |   File \"C:\\Python313\\Lib\\contextlib.py\", line 214, in __aenter__\n",
      "    |     return await anext(self.gen)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpx_sse\\_api.py\", line 69, in aconnect_sse\n",
      "    |     async with client.stream(method, url, headers=headers, **kwargs) as response:\n",
      "    |                ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Python313\\Lib\\contextlib.py\", line 214, in __aenter__\n",
      "    |     return await anext(self.gen)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1583, in stream\n",
      "    |     response = await self.send(\n",
      "    |                ^^^^^^^^^^^^^^^^\n",
      "    |     ...<4 lines>...\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1629, in send\n",
      "    |     response = await self._send_handling_auth(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<4 lines>...\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1657, in _send_handling_auth\n",
      "    |     response = await self._send_handling_redirects(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<3 lines>...\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1694, in _send_handling_redirects\n",
      "    |     response = await self._send_single_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1730, in _send_single_request\n",
      "    |     response = await transport.handle_async_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 393, in handle_async_request\n",
      "    |     with map_httpcore_exceptions():\n",
      "    |          ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "    |   File \"C:\\Python313\\Lib\\contextlib.py\", line 162, in __exit__\n",
      "    |     self.gen.throw(value)\n",
      "    |     ~~~~~~~~~~~~~~^^^^^^^\n",
      "    |   File \"C:\\Users\\ryahj\\CascadeProjects\\ai-bootcamp\\ai-bootcamp\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    |     raise mapped_exc(message) from exc\n",
      "    | httpx.ConnectError: All connection attempts failed\n",
      "    +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "await runner.run();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360a48f-1dc6-4c44-ade6-21837011f7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
