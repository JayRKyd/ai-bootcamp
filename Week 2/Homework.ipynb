{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f470c4b0-d4ca-4803-97c2-e30b814c61bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e648604-36d9-4d04-bf12-1b6992baf2e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (842801469.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython.exe -m pip install --upgrade pip\u001b[39m\n                  ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "901ee5c3-56e4-4898-b94b-906ef439643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095358df-9a3d-4a14-bd24-96bc9533dd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed7d9f46-1ad4-44bb-9364-88e4842ece54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b05f2934-397d-49b3-a291-53f72523f603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m221 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m201 packages\u001b[0m \u001b[2min 913ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add openai-agents==0.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8934b1e0-f0cf-4256-8854-3ded5d60dd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (1.109.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (4.14.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from openai) (2.12.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai requests beautifulsoup4 --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccb28408-6559-4ad8-8813-d87503ffe63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31merror\u001b[39m\u001b[0m: The dependency `openai` could not be found in `project.dependencies`\n"
     ]
    }
   ],
   "source": [
    "!uv remove openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d93dbaa-b3aa-46d0-8385-6ac1209e6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdf7fa69-1b88-4b22-9696-72e072aecf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b436f15-9e94-4da1-8656-795aaa672abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, function_tool, Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "876307b0-3d53-4975-93ea-26d1e8be8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23d2656c-56e8-451d-b83d-ee2a33ff39f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7c60dc8-ef14-4f6a-9759-70ec49813fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import OpenAIAgentsSDKRunner\n",
    "\n",
    "chat_interface = IPythonChatInterface()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "134e166b-23b6-4576-aa03-6e6b2fcae9f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'web_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m runner = OpenAIAgentsSDKRunner(\n\u001b[32m      2\u001b[39m     chat_interface=chat_interface,\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     agent=\u001b[43mweb_agent\u001b[49m\n\u001b[32m      4\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'web_agent' is not defined"
     ]
    }
   ],
   "source": [
    "runner = OpenAIAgentsSDKRunner(\n",
    "    chat_interface=chat_interface,\n",
    "    agent=web_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dc52141-3667-48d6-8290-9f210edd8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_web_page(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the content of a web page given its URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add headers to avoid being blocked\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # Send GET request with headers\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text content\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error fetching web page: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9749376e-fb07-4eac-8c54-271af3fee93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Define the tool for the agent\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"fetch_web_page\",\n",
    "            \"description\": \"Fetch the content of a web page given its URL\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"url\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The URL of the web page to fetch\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"url\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "def run_agent(user_message: str):\n",
    "    \"\"\"Run the agent with web fetching capability.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "    \n",
    "    # Initial API call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    \n",
    "    response_message = response.choices[0].message\n",
    "    messages.append(response_message)\n",
    "    \n",
    "    # Handle tool calls\n",
    "    if response_message.tool_calls:\n",
    "        for tool_call in response_message.tool_calls:\n",
    "            if tool_call.function.name == \"fetch_web_page\":\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                url = function_args.get(\"url\")\n",
    "                \n",
    "                print(f\"ðŸ” Fetching: {url}\")\n",
    "                \n",
    "                # Call our function\n",
    "                function_response = fetch_web_page(url)\n",
    "                \n",
    "                # Add function response to messages\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"content\": function_response\n",
    "                })\n",
    "        \n",
    "        # Get final response\n",
    "        final_response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return final_response.choices[0].message.content\n",
    "    \n",
    "    return response_message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b2d7707-c9fb-4632-8efd-4352ff96f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Fetching: https://example.com\n"
     ]
    }
   ],
   "source": [
    "result = run_agent(\"Can you fetch the content from https://example.com and tell me what it says?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aefebbf4-a5dd-40c0-a9ff-f7965b09d321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The website at https://example.com is an 'Example Domain' and is intended to be used in documentation examples without needing permission. It is suggested to avoid use in operations. You can learn more on their website.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a06634-01bd-4ae2-81da-56557b54eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Summary Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1ad9c69-c5a0-43bb-b344-0f8723bb8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17f678e2-ccab-490b-903a-1bd0e5c76bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summary(content: str, filename: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Saves a summary to a text file in the summaries directory.\n",
    "    \n",
    "    Args:\n",
    "        content (str): The content to save.\n",
    "        filename (str, optional): The filename. If not provided, uses timestamp.\n",
    "    \n",
    "    Returns:\n",
    "        str: The saved content and confirmation message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create summaries directory if it doesn't exist\n",
    "        os.makedirs(\"summaries\", exist_ok=True)\n",
    "        \n",
    "        # Generate filename if not provided\n",
    "        if filename is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"summary_{timestamp}.txt\"\n",
    "        \n",
    "        # Ensure .txt extension\n",
    "        if not filename.endswith('.txt'):\n",
    "            filename += '.txt'\n",
    "        \n",
    "        filepath = os.path.join(\"summaries\", filename)\n",
    "        \n",
    "        # Write content to file\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        # Return both the content and confirmation\n",
    "        return f\"Summary saved successfully to {filepath}\\n\\n--- SAVED CONTENT ---\\n{content}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error saving summary: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f665ff78-f1cd-4a72-8d96-8b5ef4d007ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add save_summary tool to your existing tools list\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"fetch_web_page\",\n",
    "            \"description\": \"Fetch the content of a web page given its URL\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"url\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The URL of the web page to fetch\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"url\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"save_summary\",\n",
    "            \"description\": \"Save a summary to a text file\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"content\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The content to save\"\n",
    "                    },\n",
    "                    \"filename\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Optional filename for the summary. If not provided, a timestamp will be used.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"content\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d5b6c70-8091-4b52-b5d6-7ee6a38e1689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(user_message: str):\n",
    "    \"\"\"Run the agent with web fetching and save summary capabilities.\"\"\"\n",
    "    \n",
    "    # Add system message with instructions\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"\"\"You are a helpful web assistant. When a user asks \"What is this page about?\" or similar questions about a webpage:\n",
    "1. First, use the fetch_web_page tool to get the content\n",
    "2. Then, create a clear summary of the page\n",
    "3. Finally, use the save_summary tool to save your summary\n",
    "4. Show the user what you found and confirm the summary was saved\"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # Initial API call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    \n",
    "    response_message = response.choices[0].message\n",
    "    messages.append(response_message)\n",
    "    \n",
    "    # Handle tool calls\n",
    "    while response_message.tool_calls:\n",
    "        for tool_call in response_message.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            if function_name == \"fetch_web_page\":\n",
    "                url = function_args.get(\"url\")\n",
    "                print(f\"ðŸ” Fetching: {url}\")\n",
    "                function_response = fetch_web_page(url)\n",
    "                \n",
    "            elif function_name == \"save_summary\":\n",
    "                content = function_args.get(\"content\")\n",
    "                filename = function_args.get(\"filename\")\n",
    "                print(f\"ðŸ’¾ Saving summary...\")\n",
    "                function_response = save_summary(content, filename)\n",
    "            \n",
    "            # Add function response to messages\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"content\": function_response\n",
    "            })\n",
    "        \n",
    "        # Get next response\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "        response_message = response.choices[0].message\n",
    "        messages.append(response_message)\n",
    "    \n",
    "    return response_message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "558ce130-22ae-4970-9bb6-e44cd681a353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Fetching: https://en.wikipedia.org/wiki/Capybara\n",
      "ðŸ’¾ Saving summary...\n"
     ]
    }
   ],
   "source": [
    "# Test it: Fetch a page, summarize it, and save\n",
    "result = run_agent(\"What is this page about? https://en.wikipedia.org/wiki/Capybara\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06dbf628-d5b4-4bfc-b107-b082ed9db96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found detailed information on the Wikipedia page about capybaras, which are the largest rodents in the world, scientifically known as *Hydrochoerus hydrochaeris*. They are native to South America and typically inhabit areas near water bodies such as savannas and forests. \n",
      "\n",
      "The page covers various aspects of capybaras, including their classification, physical description, ecology, social behavior, reproduction, and their interactions with humans. Capybaras are social animals, often living in groups, and primarily feed on grasses and aquatic plants. They are not considered endangered and have stable populations. Additionally, the article highlights their cultural significance and current popularity in media and social media.\n",
      "\n",
      "I have saved this summary successfully. If you need further information or details, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd5d17-0470-4253-8f90-df42399847c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Search Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df1708f9-2309-44df-9cc8-5ae654a4b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import AppendableIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "855f8075-b266-4071-8dec-431eae290c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = AppendableIndex(text_fields=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d7411b1-2391-4987-92f4-5e716540ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07144baa-3650-4b4f-ac4e-4e3ae8c2f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve the search function to handle synonyms and related terms\n",
    "class SearchTools:\n",
    "    \"\"\"Tools for searching web content.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.indexed_pages = {}  # Store indexed content: {url: content}\n",
    "    \n",
    "    def index_page(self, url: str, content: str = None) -> bool:\n",
    "        \"\"\"\n",
    "        Index a web page for searching.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL of the page to index\n",
    "            content (str): Optional pre-fetched content. If None, will fetch it.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if indexing succeeds, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not content:\n",
    "                # Fetch the page content\n",
    "                content = fetch_web_page(url)\n",
    "                if content.startswith(\"Error\"):\n",
    "                    print(f\"Failed to fetch page: {url}\")\n",
    "                    return False\n",
    "            \n",
    "            # Store the indexed content\n",
    "            self.indexed_pages[url] = content\n",
    "            print(f\"âœ… Successfully indexed: {url}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error indexing page {url}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def search(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Search through indexed pages for relevant content.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The search query\n",
    "            \n",
    "        Returns:\n",
    "            str: Search results\n",
    "        \"\"\"\n",
    "        if not isinstance(query, str) or not query.strip():\n",
    "            return \"Error: Query must be a non-empty string.\"\n",
    "        \n",
    "        if not self.indexed_pages:\n",
    "            return \"No pages have been indexed yet. Please index a page first using index_page.\"\n",
    "        \n",
    "        try:\n",
    "            results = []\n",
    "            query_lower = query.lower()\n",
    "            \n",
    "            # Extract keywords from the query\n",
    "            keywords = [word.strip('?,!.') for word in query_lower.split() if len(word) > 3]\n",
    "            \n",
    "            # Search for pages containing the keywords\n",
    "            for url, content in self.indexed_pages.items():\n",
    "                content_lower = content.lower()\n",
    "                matches = []\n",
    "                \n",
    "                # Find all occurrences of keywords\n",
    "                for keyword in keywords:\n",
    "                    if keyword in content_lower:\n",
    "                        # Find context around the keyword\n",
    "                        start = 0\n",
    "                        while True:\n",
    "                            index = content_lower.find(keyword, start)\n",
    "                            if index == -1:\n",
    "                                break\n",
    "                            \n",
    "                            # Get snippet around the keyword\n",
    "                            snippet_start = max(0, index - 150)\n",
    "                            snippet_end = min(len(content), index + 150)\n",
    "                            snippet = content[snippet_start:snippet_end].strip()\n",
    "                            \n",
    "                            matches.append(snippet)\n",
    "                            start = index + 1\n",
    "                            \n",
    "                            # Limit to 2 matches per keyword per page\n",
    "                            if len(matches) >= 2:\n",
    "                                break\n",
    "                        \n",
    "                        if len(matches) >= 3:\n",
    "                            break\n",
    "                \n",
    "                if matches:\n",
    "                    results.append({\n",
    "                        'url': url,\n",
    "                        'snippets': matches[:3]  # Limit to 3 snippets per page\n",
    "                    })\n",
    "            \n",
    "            if not results:\n",
    "                return f\"No results found for query: '{query}'. Try different keywords or index more pages.\"\n",
    "            \n",
    "            # Format results\n",
    "            output = f\"Found {len(results)} page(s) with relevant information for '{query}':\\n\\n\"\n",
    "            for i, result in enumerate(results, 1):\n",
    "                output += f\"{i}. URL: {result['url']}\\n\"\n",
    "                for j, snippet in enumerate(result['snippets'], 1):\n",
    "                    output += f\"   Match {j}: ...{snippet}...\\n\"\n",
    "                output += \"\\n\"\n",
    "            \n",
    "            return output\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error performing search: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d278022f-c610-4fae-886d-cb812a134ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tools = SearchTools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b9b8523-dc4c-4a7c-9c79-1e3f65afe8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_agent_tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"index_page\",\n",
    "            \"description\": \"Index a web page so it can be searched later\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"url\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The URL of the page to index\"\n",
    "                    },\n",
    "                    \"content\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Optional pre-fetched content\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"url\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search\",\n",
    "            \"description\": \"Search through indexed pages for relevant information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query (natural language, NOT a URL)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "46414136-6c12-47cb-944d-bf5faff039e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the search agent runner with better instructions\n",
    "def run_search_agent(user_message: str):\n",
    "    \"\"\"Run the search agent.\"\"\"\n",
    "    \n",
    "    search_instructions = \"\"\"\n",
    "Your task is to search through indexed web pages.\n",
    "\n",
    "Before performing a search:\n",
    "1. Check if relevant pages have been indexed\n",
    "2. If pages are already indexed, proceed directly to searching\n",
    "3. If no pages are indexed, ask the user to provide a URL to index first\n",
    "\n",
    "Important rules:\n",
    "- Do NOT call `search` using a URL or link as the query\n",
    "- After successfully indexing a page, do NOT perform a search automatically unless the user explicitly asks\n",
    "- When the user asks a question (not about indexing), use the `search` tool with their question\n",
    "- Only use natural-language queries for search (e.g., \"threats to capybara populations\")\n",
    "\n",
    "If pages are already indexed and the user asks a question, search immediately without asking for more URLs.\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": search_instructions},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # Add context about indexed pages\n",
    "    if search_tools.indexed_pages:\n",
    "        indexed_urls = \"\\n\".join([f\"- {url}\" for url in search_tools.indexed_pages.keys()])\n",
    "        messages.insert(1, {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"Currently indexed pages:\\n{indexed_urls}\\n\\nYou can search these pages directly.\"\n",
    "        })\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages,\n",
    "        tools=search_agent_tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    \n",
    "    response_message = response.choices[0].message\n",
    "    messages.append(response_message)\n",
    "    \n",
    "    # Handle tool calls\n",
    "    while response_message.tool_calls:\n",
    "        for tool_call in response_message.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            if function_name == \"index_page\":\n",
    "                url = function_args.get(\"url\")\n",
    "                content = function_args.get(\"content\")\n",
    "                print(f\"ðŸ“š Indexing page: {url}\")\n",
    "                success = search_tools.index_page(url, content)\n",
    "                function_response = f\"Page indexed successfully: {success}\"\n",
    "                \n",
    "            elif function_name == \"search\":\n",
    "                query = function_args.get(\"query\")\n",
    "                print(f\"ðŸ”Ž Searching for: {query}\")\n",
    "                function_response = search_tools.search(query)\n",
    "            \n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"content\": function_response\n",
    "            })\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=messages,\n",
    "            tools=search_agent_tools,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "        response_message = response.choices[0].message\n",
    "        messages.append(response_message)\n",
    "    \n",
    "    return response_message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8697e23d-8821-477b-82bf-776227f04ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Indexing Main Capybara Page ===\n",
      "ðŸ“š Indexing page: https://en.wikipedia.org/wiki/Capybara\n",
      "âœ… Successfully indexed: https://en.wikipedia.org/wiki/Capybara\n",
      "The page \"https://en.wikipedia.org/wiki/Capybara\" has been successfully indexed. If you have any questions or need information about capybaras, you can now ask, and I will search the indexed page for it.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Indexing Main Capybara Page ===\")\n",
    "result = run_search_agent(\"Index this page: https://en.wikipedia.org/wiki/Capybara\")\n",
    "print(result)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5eacfbe4-cdde-43d5-b71c-7208b4369e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing: https://en.wikipedia.org/wiki/Lesser_capybara\n",
      "ðŸ“š Indexing page: https://en.wikipedia.org/wiki/Lesser_capybara\n",
      "âœ… Successfully indexed: https://en.wikipedia.org/wiki/Lesser_capybara\n",
      "The page \"https://en.wikipedia.org/wiki/Lesser_capybara\" has been indexed successfully. You can now search this page along with others that are already indexed.\n",
      "\n",
      "Indexing: https://en.wikipedia.org/wiki/Hydrochoerus\n",
      "ðŸ“š Indexing page: https://en.wikipedia.org/wiki/Hydrochoerus\n",
      "âœ… Successfully indexed: https://en.wikipedia.org/wiki/Hydrochoerus\n",
      "The page https://en.wikipedia.org/wiki/Hydrochoerus has been successfully indexed. What do you want to do next?\n",
      "\n",
      "Indexing: https://en.wikipedia.org/wiki/Neochoerus\n",
      "ðŸ“š Indexing page: https://en.wikipedia.org/wiki/Neochoerus\n",
      "âœ… Successfully indexed: https://en.wikipedia.org/wiki/Neochoerus\n",
      "The page https://en.wikipedia.org/wiki/Neochoerus has been indexed successfully. You may now search across all indexed pages.\n",
      "\n",
      "Indexing: https://en.wikipedia.org/wiki/Caviodon\n",
      "ðŸ“š Indexing page: https://en.wikipedia.org/wiki/Caviodon\n",
      "âœ… Successfully indexed: https://en.wikipedia.org/wiki/Caviodon\n",
      "I have successfully indexed the page: https://en.wikipedia.org/wiki/Caviodon\n",
      "\n",
      "You may now perform searches including this page.\n",
      "\n",
      "Indexing: https://en.wikipedia.org/wiki/Neochoerus_aesopi\n",
      "ðŸ“š Indexing page: https://en.wikipedia.org/wiki/Neochoerus_aesopi\n",
      "âœ… Successfully indexed: https://en.wikipedia.org/wiki/Neochoerus_aesopi\n",
      "The page https://en.wikipedia.org/wiki/Neochoerus_aesopi has been indexed successfully. You can now search for information within this page.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pages = [\n",
    "    \"https://en.wikipedia.org/wiki/Lesser_capybara\",\n",
    "    \"https://en.wikipedia.org/wiki/Hydrochoerus\",\n",
    "    \"https://en.wikipedia.org/wiki/Neochoerus\",\n",
    "    \"https://en.wikipedia.org/wiki/Caviodon\",\n",
    "    \"https://en.wikipedia.org/wiki/Neochoerus_aesopi\"\n",
    "]\n",
    "\n",
    "for page in pages:\n",
    "    print(f\"Indexing: {page}\")\n",
    "    result = run_search_agent(f\"Index this page: {page}\")\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d9bc7f4c-2daf-434a-9319-9a6622c09314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages indexed: 6\n",
      "Indexed URLs:\n",
      "  - https://en.wikipedia.org/wiki/Capybara\n",
      "  - https://en.wikipedia.org/wiki/Lesser_capybara\n",
      "  - https://en.wikipedia.org/wiki/Hydrochoerus\n",
      "  - https://en.wikipedia.org/wiki/Neochoerus\n",
      "  - https://en.wikipedia.org/wiki/Caviodon\n",
      "  - https://en.wikipedia.org/wiki/Neochoerus_aesopi\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total pages indexed: {len(search_tools.indexed_pages)}\")\n",
    "print(\"Indexed URLs:\")\n",
    "for url in search_tools.indexed_pages.keys():\n",
    "    print(f\"  - {url}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b3d09a8-98a4-4483-a302-c1b639417c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Searching for Threats ===\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Searching for Threats ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b1d9d86-0121-429e-b727-881fcbea90d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Searching for: threats to capybara populations\n"
     ]
    }
   ],
   "source": [
    "result = run_search_agent(\"What are threats to capybara populations?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "065a29a3-1807-4f7c-b9da-5e6b5f33beae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capybaras have flourished in cattle ranches and they roam in home ranges averaging 10 hectares (25 acres) in high-density populations. There may be potential risks due to human interactions, however, the exact threats to capybara populations aren't explicitly outlined in the indexed pages. For comprehensive information, you might want to look at more specialized sources or research studies. [Wikipedia - Capybara](https://en.wikipedia.org/wiki/Capybara)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074d517-805f-4d34-abc1-31eea110306d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
