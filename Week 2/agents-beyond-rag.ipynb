{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f199f621-47e6-4ad2-831b-f0e3e7f13150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\ryahj\\cascadeprojects\\ai-bootcamp\\ai-bootcamp\\.venv\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75f8d253-4bf0-40fd-a7f8-004d98f8613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eefffc28-b308-4114-9a46-35d9fddedc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2db70a04-5729-414c-a4a1-723809c0fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158fac03-7e8e-4ef5-801a-bfe1905f79ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded docs.py successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://github.com/alexeygrigorev/ai-bootcamp-codespace/raw/refs/heads/main/week1/docs.py\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"docs.py\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(\"Downloaded docs.py successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e39a1b-e416-46c2-82be-0218ab38dab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs\n",
    "\n",
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)\n",
    "chunks = docs.chunk_documents(parsed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b6e1328-ee67-47e2-808c-72c793fb78f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x1f4c5e9c2f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"content\", \"filename\", \"title\", \"description\"],\n",
    ")\n",
    "\n",
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a449d9cd-79fb-496a-b217-be1d60442657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, TypedDict\n",
    "\n",
    "class SearchResult(TypedDict):\n",
    "    \"\"\"Represents a single search result entry.\"\"\"\n",
    "    start: int\n",
    "    content: str\n",
    "    title: str\n",
    "    description: str\n",
    "    filename: str\n",
    "\n",
    "\n",
    "def search(query: str) -> List[SearchResult]:\n",
    "    \"\"\"\n",
    "    Search the index for documents matching the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[SearchResult]: A list of search results. Each result dictionary contains:\n",
    "            - start (int): The starting position or offset within the source file.\n",
    "            - content (str): A text excerpt or snippet containing the match.\n",
    "            - title (str): The title of the matched document.\n",
    "            - description (str): A short description of the document.\n",
    "            - filename (str): The path or name of the source file.\n",
    "    \"\"\"\n",
    "    return index.search(\n",
    "        query=query,\n",
    "        num_results=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06f2c3a-1142-48c7-ae42-2ec8dfba5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_index = {}\n",
    "\n",
    "for item in parsed_data:\n",
    "    filename = item['filename']\n",
    "    content = item['content']\n",
    "    file_index[filename] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c27df0e9-8891-4d18-9acd-cd35b154600c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f7ef18b-a96f-4892-a665-2a7088540a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve the contents of a file from the file index if it exists.\n",
    "\n",
    "    Args:\n",
    "        fielname (str): The name of the file to read.\n",
    "\n",
    "     Returns:\n",
    "         str: The file's contents if found, otherwise an error message \n",
    "         indicating that the file does not exist.\n",
    "    \"\"\"\n",
    "    if filename in file_index:\n",
    "        return file_index[filename]\n",
    "    return \"File doesn't exist\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d03a8d9-5262-46bd-8389-f217d7b5b7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.llm import OpenAIClient\n",
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import OpenAIResponsesRunner\n",
    "from toyaikit.chat.runners import DisplayingRunnerCallback\n",
    "from toyaikit.tools import Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b85b70e4-87ee-4868-9a10-aca820996f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You are an assistant that helps improve and generate high-quality documentation for the project.\n",
    "\n",
    "You have access to the following tools:\n",
    "- search — Use this to explore topics in depth. Make multiple search calls if needed to gather comprehensive information.\n",
    "- read_file — Use this when code snippets are missing or when you need to retrieve the full content of a file for context.\n",
    "\n",
    "## Critical Rule\n",
    "Before generating or finalizing any code example or technical explanation, you must always call `read_file` to cross-check the correctness of the code.\n",
    "Do not rely soley on search  results or assumptions - always verify by reading the actual file content.\n",
    "\n",
    "If `read_file` cannot be used or the file content is unavailable, clearly state:\n",
    "> \"Unable to verify with read_file.\"\n",
    "\n",
    "When answering a question:\n",
    "1. Provide file references for all source materials.  \n",
    "   Use this format:  \n",
    "   [{filename}](https://github.com/evidentlyai/docs/blob/main/{filename})\n",
    "2. If the topic is covered in multiple documents, cite all relevant sources.\n",
    "3. Include code examples whenever they clarify or demonstrate the concept.\n",
    "4. Be concise, accurate, and helpful — focus on clarity and usability for developers.\n",
    "5. If documentation is missing or unclear, infer from context and note that explicitly.\n",
    "\n",
    "Example Citation:\n",
    "See the full implementation in [metrics/api_reference.md](https://github.com/evidentlyai/docs/blob/main/metrics/api_reference.md).\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c877152b-2e30-412f-91d7-b61d0e0f7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_tools = Tools()\n",
    "\n",
    "agent_tools.add_tool(search)\n",
    "agent_tools.add_tool(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b23b1a1c-a7e6-48fc-abf9-44aebcc08e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_interface = IPythonChatInterface()\n",
    "\n",
    "runner = OpenAIResponsesRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=instructions,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=OpenAIClient()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd6fb3a-3116-4064-b57c-2773107036a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: How do I run llm as a judge evals?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"llm as a judge evals\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"llm as a judge evals\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_tbQXhyb6yXG0bNp95JL5dKxx', 'output': '[\\n  {\\n    \"start\": 0,\\n    \"content\": \"import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\\\n\\\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\\\n\\\\n<Info>\\\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we\\\\u2019ll optionally show how to upload results to the Evidently Platform for easy exploration.\\\\n</Info>\\\\n\\\\nWe\\'ll explore two ways to use an LLM as a judge:\\\\n\\\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \\\\\"ground truth\\\\\" (approved responses) to compare against.\\\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\\\n\\\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\\\n\\\\n<Info>\\\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\\\n</Info>\\\\n\\\\n## Tutorial scope\\\\n\\\\nHere\\'s what we\\'ll do:\\\\n\\\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\\\n\\\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\\\n\\\\nTo complete the tutorial, you will need:\\\\n\\\\n- Basic Python knowledge.\\\\n- An OpenAI API key to use for the LLM evaluator.\\\\n\\\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\\\n\\\\n<Info>\\\\n  Run a sample notebook: [Jupyter \",\\n    \"title\": \"LLM as a judge\",\\n    \"description\": \"How to create and evaluate an LLM judge.\",\\n    \"filename\": \"examples/LLM_judge.mdx\"\\n  },\\n  {\\n    \"start\": 1000,\\n    \"content\": \"n.\\\\n\\\\n<Info>\\\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\\\n</Info>\\\\n\\\\n## Tutorial scope\\\\n\\\\nHere\\'s what we\\'ll do:\\\\n\\\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\\\n\\\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\\\n\\\\nTo complete the tutorial, you will need:\\\\n\\\\n- Basic Python knowledge.\\\\n- An OpenAI API key to use for the LLM evaluator.\\\\n\\\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\\\n\\\\n<Info>\\\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\\\n</Info>\\\\n\\\\n## 1.  Installation and Imports\\\\n\\\\nInstall Evidently:\\\\n\\\\n```python\\\\npip install evidently\\\\n```\\\\n\\\\nImport the required modules:\\\\n\\\\n```python\\\\nimport pandas as pd\\\\nimport numpy as np\\\\n\\\\nfrom evidently import Dataset\\\\nfrom evidently import DataDefinition\\\\nfrom evidently import Report\\\\nfrom evidently import BinaryClassification\\\\nfrom evidently.descriptors import *\\\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\\\nfrom evidently.metrics import *\\\\n\\\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\\\n```\\\\n\\\\nPass your OpenAI key as an environment variable:\\\\n\\\\n```python\\\\nimport os\\\\nos.environ[\\\\\"OPENAI_API_KEY\\\\\"] = \\\\\"YOUR_KEY\\\\\"\\\\n```\\\\n\\\\n<Info>\\\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metric\",\\n    \"title\": \"LLM as a judge\",\\n    \"description\": \"How to create and evaluate an LLM judge.\",\\n    \"filename\": \"examples/LLM_judge.mdx\"\\n  },\\n  {\\n    \"start\": 2000,\\n    \"content\": \"notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\\\n</Info>\\\\n\\\\n## 1.  Installation and Imports\\\\n\\\\nInstall Evidently:\\\\n\\\\n```python\\\\npip install evidently\\\\n```\\\\n\\\\nImport the required modules:\\\\n\\\\n```python\\\\nimport pandas as pd\\\\nimport numpy as np\\\\n\\\\nfrom evidently import Dataset\\\\nfrom evidently import DataDefinition\\\\nfrom evidently import Report\\\\nfrom evidently import BinaryClassification\\\\nfrom evidently.descriptors import *\\\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\\\nfrom evidently.metrics import *\\\\n\\\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\\\n```\\\\n\\\\nPass your OpenAI key as an environment variable:\\\\n\\\\n```python\\\\nimport os\\\\nos.environ[\\\\\"OPENAI_API_KEY\\\\\"] = \\\\\"YOUR_KEY\\\\\"\\\\n```\\\\n\\\\n<Info>\\\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\\\n</Info>\\\\n\\\\n## 2.  Create the Dataset\\\\n\\\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\\\n\\\\n- **Questions**. The inputs sent to the LLM app.\\\\n- **Target responses**. The approved responses you consider accurate.\\\\n- **New responses**. Imitated new responses from the system.\\\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\\\n\\\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\\\n\\\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\\\n- Get the \\\\\"ground truth\\\\\". You can use it to evaluate the quality of the LLM judge.\\\\n\\\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\\\\\!\\\\n\\\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\\\n\\\\n<Accordion title=\\\\\"Toy data to run the example\\\\\" defaultOpen={false}>\\\\n  ```python\\\\n  \",\\n    \"title\": \"LLM as a judge\",\\n    \"description\": \"How to create and evaluate an LLM judge.\",\\n    \"filename\": \"examples/LLM_judge.mdx\"\\n  },\\n  {\\n    \"start\": 20000,\\n    \"content\": \"openai\\\\\",\\\\n            model = \\\\\"gpt-4o-mini\\\\\",\\\\n            alias=\\\\\"Verbosity\\\\\")\\\\n    ])\\\\n```\\\\n\\\\nRun the Report and view the summary results:\\\\u00a0\\\\n\\\\n```python\\\\nreport = Report([\\\\n    TextEvals()\\\\n])\\\\n\\\\nmy_eval = report.run(eval_dataset, None)\\\\nmy_eval\\\\n```\\\\n\\\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\\\n\\\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\\\n\\\\n<Info>\\\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\\\n</Info>\\\\n\\\\n## What\\'s next?\\\\n\\\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\\\n\\\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\\\n\\\\n### Set up Evidently Cloud\\\\n\\\\n<CloudSignup />\\\\n\\\\nImport the components to connect with Evidently Cloud:\\\\n\\\\n```python\\\\nfrom evidently.ui.workspace import CloudWorkspace\\\\n```\\\\n\\\\n### Create a Project\\\\n\\\\n<CreateProject />\\\\n\\\\n### Send your eval\\\\n\\\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\\\n\\\\n```python\\\\nws.add_run(project.id, my_eval, include_data=True)\\\\n```\\\\n\\\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\\\n\\\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\\\n\\\\n<Info>\\\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\\\n</Info>\\\\n\\\\n# Reference documentation\\\\n\\\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).\",\\n    \"title\": \"LLM as a judge\",\\n    \"description\": \"How to create and evaluate an LLM judge.\",\\n    \"filename\": \"examples/LLM_judge.mdx\"\\n  },\\n  {\\n    \"start\": 17000,\\n    \"content\": \"\\\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \\\\\"incorrect\\\\\" answers .\\\\n\\\\n## 4. Evaluate the LLM Eval quality\\\\n\\\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\\\\\! We can treat it as a simple **binary classification** problem.\\\\n\\\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \\\\\"target\\\\\", and the LLM-judge response is the \\\\\"prediction\\\\\":\\\\n\\\\n```python\\\\ndf=eval_dataset.as_dataframe()\\\\n\\\\ndefinition_2 = DataDefinition(\\\\n    classification=[BinaryClassification(\\\\n        target=\\\\\"label\\\\\",\\\\n        prediction_labels=\\\\\"Correctness\\\\\",\\\\n        pos_label = \\\\\"incorrect\\\\\")],\\\\n    categorical_columns=[\\\\\"label\\\\\", \\\\\"Correctness\\\\\"])\\\\n\\\\nclass_dataset = Dataset.from_pandas(\\\\n    pd.DataFrame(df),\\\\n    data_definition=definition_2)\\\\n```\\\\n\\\\n<Info>\\\\n  `Pos_label` refers to the class that is treated as the target (\\\\\"what we want to predict better\\\\\") for metrics like precision, recall, F1-score.\\\\n</Info>\\\\n\\\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\\\n\\\\n```python\\\\nreport = Report([\\\\n    ClassificationPreset()\\\\n])\\\\n\\\\nmy_eval = report.run(class_dataset, None)\\\\nmy_eval\\\\n\\\\n# or my_eval.as_dict()\\\\n```\\\\n\\\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\\\\\! You can also refine the prompt to try to improve them.\\\\n\\\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\\\n\\\\n## 5. Verbosity evaluator\\\\n\\\\nNext, let\\\\u2019s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don\\\\u2019t have a reference answer.\\\\n\\\\nHere\\'s ho\",\\n    \"title\": \"LLM as a judge\",\\n    \"description\": \"How to create and evaluate an LLM judge.\",\\n    \"filename\": \"examples/LLM_judge.mdx\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To run an LLM (Large Language Model) as a judge for evaluations, you'll follow a structured approach that involves creating an evaluation dataset, configuring the LLM evaluator, and then analyzing the results. Here's a concise guide based on the available documentation.</p>\n",
       "<h3>Steps to Run LLM as a Judge</h3>\n",
       "<ol>\n",
       "<li><p><strong>Install Necessary Libraries</strong><br />\n",
       "You need to install the Evidently library:</p>\n",
       "<pre><code class=\"language-bash\">pip install evidently\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Import Required Modules</strong><br />\n",
       "Start by importing the necessary libraries:</p>\n",
       "<pre><code class=\"language-python\">import pandas as pd\n",
       "import numpy as np\n",
       "from evidently import Dataset, DataDefinition, Report, BinaryClassification\n",
       "from evidently.presets import TextEvals, ClassificationPreset\n",
       "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Set Up OpenAI API Key</strong><br />\n",
       "To use the LLM, you need to pass your OpenAI API key as an environment variable:</p>\n",
       "<pre><code class=\"language-python\">import os\n",
       "os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;YOUR_KEY&quot;\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Create the Evaluation Dataset</strong><br />\n",
       "You'll need to create a dataset that includes:</p>\n",
       "<ul>\n",
       "<li>Input questions</li>\n",
       "<li>Target responses (correct answers)</li>\n",
       "<li>New responses from the model</li>\n",
       "<li>Manual labels indicating correctness</li>\n",
       "</ul>\n",
       "<p>Example snippet to create a toy dataset:</p>\n",
       "<pre><code class=\"language-python\"># Generate your dataset\n",
       "data = {\n",
       "    'questions': [...],\n",
       "    'target_responses': [...],\n",
       "    'new_responses': [...],\n",
       "    'manual_labels': [...]\n",
       "}\n",
       "eval_dataset = pd.DataFrame(data)\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Design the LLM Judge Prompt</strong><br />\n",
       "You can design a prompt for the LLM that specifies how to evaluate the new responses. It can be tailored according to your specific needs, such as checking for correctness or verbosity.</p>\n",
       "</li>\n",
       "<li><p><strong>Run the Evaluation</strong><br />\n",
       "Using the Report class, you can evaluate the dataset:</p>\n",
       "<pre><code class=\"language-python\">report = Report([TextEvals()])\n",
       "my_eval = report.run(eval_dataset, None)\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Analyze the Results</strong><br />\n",
       "You can view the results directly in a DataFrame:</p>\n",
       "<pre><code class=\"language-python\">results_df = eval_dataset\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Iterate and Refine</strong><br />\n",
       "Based on the results, you may need to refine your LLM prompt or dataset labels to improve future evaluations.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<h3>Resources</h3>\n",
       "<p>For a complete tutorial with interactive Jupyter Notebook examples, check the linked resources:</p>\n",
       "<ul>\n",
       "<li><a href=\"https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb\">Run the example in Jupyter</a></li>\n",
       "<li><a href=\"https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb\">Run the example in Google Colab</a></li>\n",
       "</ul>\n",
       "<p>For detailed documentation on LLM judges, refer to: <a href=\"https://github.com/evidentlyai/docs/blob/main/metrics/customize_llm_judge\">LLM Judges Documentation</a>.</p>\n",
       "<p>This structured approach ensures that you can effectively utilize LLMs to evaluate outputs based on custom or reference-based criteria.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a441192c-58d5-4972-841a-becc924c1031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
